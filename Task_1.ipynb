{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380f9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, l1=0.0, l2=0.0):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # Initialise (weights) velocity terms for momentum optimization\n",
    "        self.velocity_biases = np.zeros_like(self.biases) # Initialise (biases) velocity terms for momentum optimization\n",
    "        self.l1 = l1 # L1 regularization coefficient (default 0.0).\n",
    "        self.l2 = l2 # L2 regularization coefficient (default 0.0).\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        '''\n",
    "        # Regularization terms\n",
    "        l1_reg = self.l1 * np.sign(self.weights)\n",
    "        l2_reg = self.l2 * self.weights\n",
    "\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient) + l1_reg + l2_reg\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if optimizer == 'GD':\n",
    "            # Update weights and biases\n",
    "            self.weights += learning_rate * weights_gradient\n",
    "            self.biases += learning_rate * biases_gradient\n",
    "        elif optimizer == 'Momentum':\n",
    "            # Momentum update for weights and biases\n",
    "            self.velocity_weights = momentum * self.velocity_weights + learning_rate * weights_gradient\n",
    "            self.velocity_biases = momentum * self.velocity_biases + learning_rate * biases_gradient\n",
    "\n",
    "            # Update weights and biases using velocity\n",
    "            self.weights += self.velocity_weights\n",
    "            self.biases += self.velocity_biases\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for printing/plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for printing/plotting\n",
    "        self.accuracy_history = [] #placeholder to store the (train) accuracy for printing/plotting\n",
    "        self.val_accuracy_history = [] #placeholder to store the accuracy calculated on the validation set for printing/plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Computes the accuracy of predictions by comparing them with the true labels. \n",
    "        Accuracy is computed as the proportion of correct predictions to the total number of predictions.\n",
    "        '''\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate, optimizer, momentum)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, optimizer='GD', momentum=0.9, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate, optimizer, momentum) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_predictions = self.predict(X_train)\n",
    "            train_accuracy = self.compute_accuracy(train_predictions, np.argmax(y_train, axis=1))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_predictions = self.predict(X_val)\n",
    "            val_accuracy = self.compute_accuracy(val_predictions, np.argmax(y_val, axis=1))\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.plot(self.accuracy_history, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracy_history, label='Val Accuracy')\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 2.302356789917726 --- Val Loss: 2.3030626335665714 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/1000 --- Train Loss: 2.3015546697331475 --- Val Loss: 2.30481692923134 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: 2.301546929308975 --- Val Loss: 2.305424323508293 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 30/1000 --- Train Loss: 2.301547701748579 --- Val Loss: 2.305904673126399 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/1000 --- Train Loss: 2.3015448387861417 --- Val Loss: 2.3056093337588055 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.301543182229616 --- Val Loss: 2.305386636037665 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: 2.3015400500602885 --- Val Loss: 2.305506964970304 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.301538943521579 --- Val Loss: 2.305403722661314 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: 2.301537585140498 --- Val Loss: 2.3056023931103287 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: 2.301536702818009 --- Val Loss: 2.3054550596727497 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: 2.3015300454707375 --- Val Loss: 2.3049256533644016 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: 2.301500815838859 --- Val Loss: 2.305293016883942 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 120/1000 --- Train Loss: 2.301421575468958 --- Val Loss: 2.3054974024198724 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.300949523726798 --- Val Loss: 2.30534957909564 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.280722795618599 --- Val Loss: 2.2849282117767706 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: 1.7324776785074736 --- Val Loss: 1.722695006580204 --- Train Acc: 0.22 --- Val Acc: 0.17\n",
      "Epoch 160/1000 --- Train Loss: 1.4034969542349074 --- Val Loss: 1.3528349579525638 --- Train Acc: 0.39 --- Val Acc: 0.45\n",
      "Epoch 170/1000 --- Train Loss: 0.9899853801118286 --- Val Loss: 0.9272678464693561 --- Train Acc: 0.63 --- Val Acc: 0.66\n",
      "Epoch 180/1000 --- Train Loss: 0.553095782550537 --- Val Loss: 0.5042225102117264 --- Train Acc: 0.84 --- Val Acc: 0.86\n",
      "Epoch 190/1000 --- Train Loss: 0.22172539375370087 --- Val Loss: 0.18661181479609912 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 200/1000 --- Train Loss: 0.09969235728766364 --- Val Loss: 0.08041118952129303 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 210/1000 --- Train Loss: 0.06783669950894512 --- Val Loss: 0.045012039149903364 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/1000 --- Train Loss: 0.03871483257211618 --- Val Loss: 0.02822491526566302 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 230/1000 --- Train Loss: 0.026162935593576145 --- Val Loss: 0.01792051409748029 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.0242229818939071 --- Val Loss: 0.01251708405900228 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.019439589168431477 --- Val Loss: 0.01117575869537181 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.01219480616676006 --- Val Loss: 0.006884608368049146 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.015328853038733428 --- Val Loss: 0.0054689126116652 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.014122756433136147 --- Val Loss: 0.004574034532210348 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.008425846235401544 --- Val Loss: 0.0037746497542826734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.0073959206236514545 --- Val Loss: 0.0030728385877774596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.007148098688519604 --- Val Loss: 0.0027991784951494074 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.005041958753974565 --- Val Loss: 0.002223121051152566 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.00427818696316838 --- Val Loss: 0.002362246874743891 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.005700404077166819 --- Val Loss: 0.0019793487116419733 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.004117177801307024 --- Val Loss: 0.001875360851975013 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.003404584846800598 --- Val Loss: 0.0015054740243684794 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.00750153412091846 --- Val Loss: 0.0013681528863190265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.004900327180073538 --- Val Loss: 0.0011217439874369693 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.002478677574025675 --- Val Loss: 0.0011604415584896607 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.005181552439081183 --- Val Loss: 0.001112844646963107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.0030060344231055067 --- Val Loss: 0.0009976813638938025 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.003257692725594857 --- Val Loss: 0.0009015471290758989 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.002989100781547583 --- Val Loss: 0.0008064031307479407 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.004705266474253968 --- Val Loss: 0.0007353573855329655 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.0037917525446124903 --- Val Loss: 0.0008154231319784656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.0018817097202069246 --- Val Loss: 0.0006896236236265254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.003273891234296671 --- Val Loss: 0.0006231060851387359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.002841324631376345 --- Val Loss: 0.0005826234265036463 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.005529645241276387 --- Val Loss: 0.0006025376748158585 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.004834511829244008 --- Val Loss: 0.0005841302590456079 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.0022551512676501147 --- Val Loss: 0.0005929074988992282 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.0014258305202364972 --- Val Loss: 0.0005568942918209986 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.0033415880469512297 --- Val Loss: 0.0005428013721999217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.0016716506092732447 --- Val Loss: 0.0005140428000073823 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.0022087375274295155 --- Val Loss: 0.0005806397741862596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.0023205312700000213 --- Val Loss: 0.00044902562972978663 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.0013265668251402868 --- Val Loss: 0.0004182700302532922 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.0029719674853805884 --- Val Loss: 0.00043381965071879265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.0018615445910901175 --- Val Loss: 0.00038591409212441476 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.002673591095321845 --- Val Loss: 0.00038047702107641795 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.0015927166734335384 --- Val Loss: 0.0003390874075411512 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.001403086630539543 --- Val Loss: 0.0003395927081047969 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.0016298534285919969 --- Val Loss: 0.0003043140790131179 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.003188343838256794 --- Val Loss: 0.0003037961554159207 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.001178879428725212 --- Val Loss: 0.0002998157748673638 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.0013581980257668803 --- Val Loss: 0.0003163654897775714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.0032236103888950932 --- Val Loss: 0.0002784704854040845 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.0009284679583418629 --- Val Loss: 0.0003285367483168252 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.001204313131240686 --- Val Loss: 0.00028002351251090753 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.002579338606128365 --- Val Loss: 0.00026544339880157984 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.0011147655446382071 --- Val Loss: 0.00026953880353518803 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.0013907567637035344 --- Val Loss: 0.00025357650085694217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.0017643192046140138 --- Val Loss: 0.0002833933806050634 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.0024717046315138494 --- Val Loss: 0.00023481440871528158 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.0009597840813464966 --- Val Loss: 0.00023771964402890644 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.0012800187834710287 --- Val Loss: 0.00022990928746920869 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.0006377793507080582 --- Val Loss: 0.00023173974704575266 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.0006142630087686835 --- Val Loss: 0.00019827361253252528 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.006025131591577447 --- Val Loss: 0.00018225709909021223 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.0019101733602403523 --- Val Loss: 0.00017738431519360998 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.0005648173150589995 --- Val Loss: 0.0002014665609429143 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.00595106788479372 --- Val Loss: 0.014795724022187469 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.0008271741996441107 --- Val Loss: 0.0002189994529216031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.0027625114994478286 --- Val Loss: 0.00020335966110294188 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.002559200368868788 --- Val Loss: 0.00019554454017426436 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.0009409726239936969 --- Val Loss: 0.00017363034437281849 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.0010633287691086287 --- Val Loss: 0.000162436666650023 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.000742021167402009 --- Val Loss: 0.00016039636398033078 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.0009433387329315694 --- Val Loss: 0.00014595128575259982 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.0005527412346128654 --- Val Loss: 0.00017904441613359568 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.0006894446656948526 --- Val Loss: 0.00016361388608799403 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.0006268102119439098 --- Val Loss: 0.00013366206697259205 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.0009014159312917142 --- Val Loss: 0.00012425043769076892 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.000971177890405463 --- Val Loss: 0.00015154412213958562 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.0008286063959864463 --- Val Loss: 0.00012741082209208763 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.0007453002922626374 --- Val Loss: 0.00011777075926920235 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.0006321569835548771 --- Val Loss: 0.00011387439406768384 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.0006641458102533699 --- Val Loss: 0.00010477575096066011 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.0004588805706631513 --- Val Loss: 0.00011414369091215687 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32)) \n",
    "network.add_layer(ReLU())\n",
    "#network.add_layer(Layer(64, 32, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=1000, learning_rate=0.01, optimizer='Momentum', momentum=0.9, batch_size=64)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2500 --- Train Loss: 0.6924793029070984 --- Val Loss: 0.6924295946989082 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 10/2500 --- Train Loss: 0.6860562181310774 --- Val Loss: 0.6838158223168646 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 20/2500 --- Train Loss: 0.6807321392337471 --- Val Loss: 0.6755345001695018 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 30/2500 --- Train Loss: 0.6792673407244719 --- Val Loss: 0.6719831010574683 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 40/2500 --- Train Loss: 0.6792277596356675 --- Val Loss: 0.6709468648120274 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 50/2500 --- Train Loss: 0.6792827724641768 --- Val Loss: 0.6707524645234354 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 60/2500 --- Train Loss: 0.6792549184692865 --- Val Loss: 0.670826509705079 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 70/2500 --- Train Loss: 0.6792170181769884 --- Val Loss: 0.6709760350710035 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 80/2500 --- Train Loss: 0.6791978086903345 --- Val Loss: 0.6711059172251427 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 90/2500 --- Train Loss: 0.6791913812710705 --- Val Loss: 0.6711808300397636 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 100/2500 --- Train Loss: 0.6791897564759164 --- Val Loss: 0.6712071749319828 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 110/2500 --- Train Loss: 0.6791895861558322 --- Val Loss: 0.6712061084106702 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 120/2500 --- Train Loss: 0.6791898329353667 --- Val Loss: 0.6711956977897502 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 130/2500 --- Train Loss: 0.6791900036541096 --- Val Loss: 0.6711856785307206 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 140/2500 --- Train Loss: 0.6791898834208515 --- Val Loss: 0.6711792251438415 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 150/2500 --- Train Loss: 0.6791894473251981 --- Val Loss: 0.6711760756591999 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 160/2500 --- Train Loss: 0.6791887510839182 --- Val Loss: 0.6711748693187548 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 170/2500 --- Train Loss: 0.6791878452023807 --- Val Loss: 0.6711743445944282 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 180/2500 --- Train Loss: 0.6791867269456177 --- Val Loss: 0.6711736996736403 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 190/2500 --- Train Loss: 0.6791853867285834 --- Val Loss: 0.6711725773877371 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 200/2500 --- Train Loss: 0.6791837198083952 --- Val Loss: 0.6711708192702587 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 210/2500 --- Train Loss: 0.679181619056553 --- Val Loss: 0.6711683704103107 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 220/2500 --- Train Loss: 0.67917892631927 --- Val Loss: 0.671165155545312 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 230/2500 --- Train Loss: 0.6791754272684392 --- Val Loss: 0.6711609409661933 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 240/2500 --- Train Loss: 0.6791707254657421 --- Val Loss: 0.6711553209802977 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 250/2500 --- Train Loss: 0.6791641112203937 --- Val Loss: 0.671147536572631 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 260/2500 --- Train Loss: 0.6791547071418835 --- Val Loss: 0.6711365845909485 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 270/2500 --- Train Loss: 0.6791413565507924 --- Val Loss: 0.6711210962409224 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 280/2500 --- Train Loss: 0.6791217718501182 --- Val Loss: 0.6710980359709219 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 290/2500 --- Train Loss: 0.6790915030976775 --- Val Loss: 0.6710625540901692 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 300/2500 --- Train Loss: 0.6790427400704303 --- Val Loss: 0.6710064262537954 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 310/2500 --- Train Loss: 0.6789608217683604 --- Val Loss: 0.6709131129539829 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 320/2500 --- Train Loss: 0.678817518191593 --- Val Loss: 0.6707498210155484 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 330/2500 --- Train Loss: 0.678549121141344 --- Val Loss: 0.6704428789633944 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 340/2500 --- Train Loss: 0.6779927656447874 --- Val Loss: 0.6698137888595389 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 350/2500 --- Train Loss: 0.6766980222463601 --- Val Loss: 0.6683568238287784 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 360/2500 --- Train Loss: 0.6731887626373375 --- Val Loss: 0.6644025917812443 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 370/2500 --- Train Loss: 0.6609499835619199 --- Val Loss: 0.65068326307468 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 380/2500 --- Train Loss: 0.5992455426256829 --- Val Loss: 0.582028134211053 --- Train Acc: 0.72 --- Val Acc: 0.73\n",
      "Epoch 390/2500 --- Train Loss: 0.37993856571672474 --- Val Loss: 0.35818268003054626 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 400/2500 --- Train Loss: 0.22254852011060572 --- Val Loss: 0.2128340465591823 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 410/2500 --- Train Loss: 0.12744264206722683 --- Val Loss: 0.13387353661273116 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 420/2500 --- Train Loss: 0.09447319035213395 --- Val Loss: 0.11454442449465337 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 430/2500 --- Train Loss: 0.08363752962637633 --- Val Loss: 0.11110790766944066 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 440/2500 --- Train Loss: 0.07901379452965722 --- Val Loss: 0.10924836032035001 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 450/2500 --- Train Loss: 0.07608250440784366 --- Val Loss: 0.10779691276694568 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 460/2500 --- Train Loss: 0.07404836586167247 --- Val Loss: 0.10680749730569794 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 470/2500 --- Train Loss: 0.07246714726638491 --- Val Loss: 0.10612364066991824 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 480/2500 --- Train Loss: 0.07108810845218151 --- Val Loss: 0.10535903356924761 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 490/2500 --- Train Loss: 0.0698792096310493 --- Val Loss: 0.10459723401456175 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 500/2500 --- Train Loss: 0.06881806164863198 --- Val Loss: 0.10391561068529682 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 510/2500 --- Train Loss: 0.06783782240966467 --- Val Loss: 0.10330837498642412 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 520/2500 --- Train Loss: 0.06691131397943524 --- Val Loss: 0.10278021847922965 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 530/2500 --- Train Loss: 0.06604459946338737 --- Val Loss: 0.10231534205751167 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 540/2500 --- Train Loss: 0.06523660385365551 --- Val Loss: 0.10191745495785963 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 550/2500 --- Train Loss: 0.06449086639273147 --- Val Loss: 0.101579736603808 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 560/2500 --- Train Loss: 0.06377828690334612 --- Val Loss: 0.10129488008264963 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 570/2500 --- Train Loss: 0.06297834698832944 --- Val Loss: 0.10107092881120817 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 580/2500 --- Train Loss: 0.062292676411011765 --- Val Loss: 0.10086074587181026 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 590/2500 --- Train Loss: 0.061706952852767526 --- Val Loss: 0.1006067772842592 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 600/2500 --- Train Loss: 0.06112092247454727 --- Val Loss: 0.10027933931608934 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 610/2500 --- Train Loss: 0.06055678100047988 --- Val Loss: 0.09990770722319633 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 620/2500 --- Train Loss: 0.06001720302237866 --- Val Loss: 0.09952082138725325 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 630/2500 --- Train Loss: 0.0594990845594849 --- Val Loss: 0.09914075238158872 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 640/2500 --- Train Loss: 0.05899539438864389 --- Val Loss: 0.09875213924296052 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 650/2500 --- Train Loss: 0.0585079904369871 --- Val Loss: 0.09836030033837861 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 660/2500 --- Train Loss: 0.058036342702891826 --- Val Loss: 0.09797139854756123 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 670/2500 --- Train Loss: 0.05757944900957001 --- Val Loss: 0.09757351459105652 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 680/2500 --- Train Loss: 0.05713116896228223 --- Val Loss: 0.09717658799200832 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 690/2500 --- Train Loss: 0.05668957492746009 --- Val Loss: 0.09677702509769134 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 700/2500 --- Train Loss: 0.05625477984205946 --- Val Loss: 0.09637200330966196 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 710/2500 --- Train Loss: 0.05582908084800359 --- Val Loss: 0.09597451022554582 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 720/2500 --- Train Loss: 0.055416723322170625 --- Val Loss: 0.09558981828234324 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 730/2500 --- Train Loss: 0.055012440927152795 --- Val Loss: 0.09521584598961007 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 740/2500 --- Train Loss: 0.05460547180207558 --- Val Loss: 0.0948443754103224 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 750/2500 --- Train Loss: 0.054193473876844284 --- Val Loss: 0.09446510039083846 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 760/2500 --- Train Loss: 0.05363955805864673 --- Val Loss: 0.09403782524149075 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 770/2500 --- Train Loss: 0.052997363960495286 --- Val Loss: 0.09356999855946736 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 780/2500 --- Train Loss: 0.05232883283492529 --- Val Loss: 0.09309308936595552 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 790/2500 --- Train Loss: 0.05164021446409263 --- Val Loss: 0.09261177607988688 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 800/2500 --- Train Loss: 0.050959410229279696 --- Val Loss: 0.09214696471027466 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 810/2500 --- Train Loss: 0.05030838756950194 --- Val Loss: 0.0916992140950258 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 820/2500 --- Train Loss: 0.049715058300084464 --- Val Loss: 0.09133684320460975 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 830/2500 --- Train Loss: 0.04915303727846 --- Val Loss: 0.091061914475941 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 840/2500 --- Train Loss: 0.048607296976333605 --- Val Loss: 0.09086887708441993 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 850/2500 --- Train Loss: 0.04805524389465677 --- Val Loss: 0.09077735332695966 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 860/2500 --- Train Loss: 0.04748943916300954 --- Val Loss: 0.09079139477375668 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 870/2500 --- Train Loss: 0.04700633783647362 --- Val Loss: 0.09094274170901857 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 880/2500 --- Train Loss: 0.0466026080461611 --- Val Loss: 0.09135698348497844 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 890/2500 --- Train Loss: 0.046157966956987075 --- Val Loss: 0.09183184501432731 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 900/2500 --- Train Loss: 0.04569850548652556 --- Val Loss: 0.09240226372859546 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 910/2500 --- Train Loss: 0.045215292437860134 --- Val Loss: 0.09305760947763839 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 920/2500 --- Train Loss: 0.04471110784558375 --- Val Loss: 0.0937797160058221 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 930/2500 --- Train Loss: 0.0441934923082055 --- Val Loss: 0.09457824494978805 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 940/2500 --- Train Loss: 0.04369658608598823 --- Val Loss: 0.09543784233026605 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 950/2500 --- Train Loss: 0.043211015794558806 --- Val Loss: 0.09640004224937167 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 960/2500 --- Train Loss: 0.04275706864769297 --- Val Loss: 0.09744705163920282 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 970/2500 --- Train Loss: 0.04232766734780746 --- Val Loss: 0.0986130940205192 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 980/2500 --- Train Loss: 0.04193916994623412 --- Val Loss: 0.09988822037839543 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 990/2500 --- Train Loss: 0.04159524178353691 --- Val Loss: 0.10127828114425717 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 1000/2500 --- Train Loss: 0.04127871526865035 --- Val Loss: 0.10275796141852425 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 1010/2500 --- Train Loss: 0.040994689769965564 --- Val Loss: 0.1043233205382862 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1020/2500 --- Train Loss: 0.04075617836901247 --- Val Loss: 0.10595124334009738 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1030/2500 --- Train Loss: 0.040556573987425903 --- Val Loss: 0.1076373232656664 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1040/2500 --- Train Loss: 0.04038247707376802 --- Val Loss: 0.10940128961069776 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1050/2500 --- Train Loss: 0.04025887513235468 --- Val Loss: 0.11118587961753766 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1060/2500 --- Train Loss: 0.040162475105081724 --- Val Loss: 0.11298954074851487 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1070/2500 --- Train Loss: 0.04009723177775737 --- Val Loss: 0.11480773769343759 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1080/2500 --- Train Loss: 0.040057777022080104 --- Val Loss: 0.11660596113625739 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1090/2500 --- Train Loss: 0.040039077111845074 --- Val Loss: 0.11838245922991307 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1100/2500 --- Train Loss: 0.04004635167547525 --- Val Loss: 0.1201332409686117 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1110/2500 --- Train Loss: 0.040075381542121744 --- Val Loss: 0.12184378608097322 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1120/2500 --- Train Loss: 0.040118024989477544 --- Val Loss: 0.12354725580436163 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1130/2500 --- Train Loss: 0.04017979395768296 --- Val Loss: 0.12521848833476726 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1140/2500 --- Train Loss: 0.04025977619401267 --- Val Loss: 0.12686278770009743 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1150/2500 --- Train Loss: 0.04035308856309104 --- Val Loss: 0.12847130493262468 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1160/2500 --- Train Loss: 0.040452985146916025 --- Val Loss: 0.13002577728005543 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1170/2500 --- Train Loss: 0.040560669900468596 --- Val Loss: 0.1315467249873203 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1180/2500 --- Train Loss: 0.04067503213345099 --- Val Loss: 0.1330242107760386 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1190/2500 --- Train Loss: 0.0407916992531997 --- Val Loss: 0.13445741119226312 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1200/2500 --- Train Loss: 0.0409209111479254 --- Val Loss: 0.13586473256763135 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1210/2500 --- Train Loss: 0.04105431471760021 --- Val Loss: 0.13723584184884322 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1220/2500 --- Train Loss: 0.041184438735090034 --- Val Loss: 0.1385777908080538 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1230/2500 --- Train Loss: 0.04132296595227166 --- Val Loss: 0.13988668218658656 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1240/2500 --- Train Loss: 0.04146377922438865 --- Val Loss: 0.14116832667694476 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1250/2500 --- Train Loss: 0.04160659290396998 --- Val Loss: 0.14241789262650237 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1260/2500 --- Train Loss: 0.041751316807609726 --- Val Loss: 0.1436437653542351 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1270/2500 --- Train Loss: 0.04189472655718256 --- Val Loss: 0.14484780458306443 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1280/2500 --- Train Loss: 0.042039430809805836 --- Val Loss: 0.14601153156187502 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1290/2500 --- Train Loss: 0.042182516725093835 --- Val Loss: 0.14714282552973143 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1300/2500 --- Train Loss: 0.04232556355494501 --- Val Loss: 0.14825298329066577 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1310/2500 --- Train Loss: 0.04246563599089903 --- Val Loss: 0.14934529285967948 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1320/2500 --- Train Loss: 0.042603578077480254 --- Val Loss: 0.15039894924591982 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1330/2500 --- Train Loss: 0.042740815632185986 --- Val Loss: 0.15142494813213844 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1340/2500 --- Train Loss: 0.0428757875462658 --- Val Loss: 0.15241306177937647 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1350/2500 --- Train Loss: 0.04300778300875094 --- Val Loss: 0.15337221997086314 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1360/2500 --- Train Loss: 0.04313928683772398 --- Val Loss: 0.15430711158369564 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1370/2500 --- Train Loss: 0.04326899586467752 --- Val Loss: 0.1552234381051787 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1380/2500 --- Train Loss: 0.04339908142306805 --- Val Loss: 0.15611204617711266 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1390/2500 --- Train Loss: 0.04352997559785013 --- Val Loss: 0.1569951206163043 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1400/2500 --- Train Loss: 0.04365979803990749 --- Val Loss: 0.15785305938680763 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1410/2500 --- Train Loss: 0.04378818282212572 --- Val Loss: 0.15870043885464763 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1420/2500 --- Train Loss: 0.04391500025974994 --- Val Loss: 0.1595232807494011 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1430/2500 --- Train Loss: 0.04404187786637682 --- Val Loss: 0.160339648573067 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1440/2500 --- Train Loss: 0.04416651521965812 --- Val Loss: 0.1611352602922109 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1450/2500 --- Train Loss: 0.0442905071059422 --- Val Loss: 0.16192193708384828 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1460/2500 --- Train Loss: 0.04441232860117571 --- Val Loss: 0.16268628585881922 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1470/2500 --- Train Loss: 0.044533300270253734 --- Val Loss: 0.16343791992957857 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1480/2500 --- Train Loss: 0.044652040659078296 --- Val Loss: 0.16416827784374477 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1490/2500 --- Train Loss: 0.04477021059054362 --- Val Loss: 0.16489704039149858 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1500/2500 --- Train Loss: 0.04488574045698095 --- Val Loss: 0.1656008493267652 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1510/2500 --- Train Loss: 0.04500063038121416 --- Val Loss: 0.16629885782517964 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1520/2500 --- Train Loss: 0.04511479206095981 --- Val Loss: 0.1669838403833437 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1530/2500 --- Train Loss: 0.045226926245534226 --- Val Loss: 0.16765388846145698 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1540/2500 --- Train Loss: 0.045338656812074676 --- Val Loss: 0.168317871978973 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1550/2500 --- Train Loss: 0.045448284931229654 --- Val Loss: 0.16896496232855943 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1560/2500 --- Train Loss: 0.045557097930974934 --- Val Loss: 0.16960544280480847 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1570/2500 --- Train Loss: 0.04566421522310771 --- Val Loss: 0.17023058974146066 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1580/2500 --- Train Loss: 0.04577131840450817 --- Val Loss: 0.17085034369511157 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1590/2500 --- Train Loss: 0.04587607535441428 --- Val Loss: 0.1714585189031559 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1600/2500 --- Train Loss: 0.0459803737901826 --- Val Loss: 0.17205826505718447 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1610/2500 --- Train Loss: 0.046083050102844865 --- Val Loss: 0.17264768769516078 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1620/2500 --- Train Loss: 0.04618447161754105 --- Val Loss: 0.17322368538492033 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1630/2500 --- Train Loss: 0.04628431624563278 --- Val Loss: 0.17379014574391122 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1640/2500 --- Train Loss: 0.04638376358634472 --- Val Loss: 0.174351776436855 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1650/2500 --- Train Loss: 0.04648176638078301 --- Val Loss: 0.17490296632385527 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1660/2500 --- Train Loss: 0.04657744486687328 --- Val Loss: 0.1754369942111482 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1670/2500 --- Train Loss: 0.04667214717480953 --- Val Loss: 0.17596761071499517 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1680/2500 --- Train Loss: 0.046766068858790276 --- Val Loss: 0.17648802327359472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1690/2500 --- Train Loss: 0.04685955996857369 --- Val Loss: 0.17700552572454217 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1700/2500 --- Train Loss: 0.046951941679563756 --- Val Loss: 0.17751533361830352 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1710/2500 --- Train Loss: 0.04704278455458912 --- Val Loss: 0.17801403524001422 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1720/2500 --- Train Loss: 0.04713395737842025 --- Val Loss: 0.17851375787194843 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1730/2500 --- Train Loss: 0.04722385108934176 --- Val Loss: 0.17900492283250394 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1740/2500 --- Train Loss: 0.04731305669181227 --- Val Loss: 0.17949017302506864 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1750/2500 --- Train Loss: 0.047400881561361866 --- Val Loss: 0.17996742449151432 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1760/2500 --- Train Loss: 0.04748825455731468 --- Val Loss: 0.1804395660786955 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1770/2500 --- Train Loss: 0.04757503117285454 --- Val Loss: 0.18090754950959712 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1780/2500 --- Train Loss: 0.047660388100469 --- Val Loss: 0.18136760377480293 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1790/2500 --- Train Loss: 0.04774551452338416 --- Val Loss: 0.18182460187713745 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1800/2500 --- Train Loss: 0.04782955300602349 --- Val Loss: 0.18227403329240494 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1810/2500 --- Train Loss: 0.04791267019447825 --- Val Loss: 0.18271771952926924 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1820/2500 --- Train Loss: 0.04799513196160239 --- Val Loss: 0.18315602805003972 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1830/2500 --- Train Loss: 0.04807651626670899 --- Val Loss: 0.18358832649981502 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1840/2500 --- Train Loss: 0.04815725104429281 --- Val Loss: 0.1840153043385026 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1850/2500 --- Train Loss: 0.04823772691422009 --- Val Loss: 0.18444095514811285 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1860/2500 --- Train Loss: 0.048317130323348764 --- Val Loss: 0.18485921383195544 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1870/2500 --- Train Loss: 0.04839585449572762 --- Val Loss: 0.18527397486435507 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1880/2500 --- Train Loss: 0.04847297115246804 --- Val Loss: 0.18567852901885964 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1890/2500 --- Train Loss: 0.04855012752469913 --- Val Loss: 0.18608314162064601 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1900/2500 --- Train Loss: 0.04862646760027315 --- Val Loss: 0.18648172765214258 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1910/2500 --- Train Loss: 0.048701808605584776 --- Val Loss: 0.186874285072957 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1920/2500 --- Train Loss: 0.048776795259115005 --- Val Loss: 0.18726490836774082 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1930/2500 --- Train Loss: 0.04885069505346972 --- Val Loss: 0.1876490555742903 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1940/2500 --- Train Loss: 0.04892445528414401 --- Val Loss: 0.18803113650914588 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1950/2500 --- Train Loss: 0.04899739629872687 --- Val Loss: 0.18840863576850145 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1960/2500 --- Train Loss: 0.04906919130604742 --- Val Loss: 0.18877956870318283 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1970/2500 --- Train Loss: 0.04914107915767654 --- Val Loss: 0.18915031744197547 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1980/2500 --- Train Loss: 0.04921189735438775 --- Val Loss: 0.18951504785611414 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1990/2500 --- Train Loss: 0.04928229362910934 --- Val Loss: 0.18987589852317877 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2000/2500 --- Train Loss: 0.049352114024629655 --- Val Loss: 0.19023427357717876 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2010/2500 --- Train Loss: 0.0494206085196289 --- Val Loss: 0.19058462973123427 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2020/2500 --- Train Loss: 0.0494897905024447 --- Val Loss: 0.19093885867437307 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2030/2500 --- Train Loss: 0.049557617410629654 --- Val Loss: 0.19128413454346546 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2040/2500 --- Train Loss: 0.04962471022945205 --- Val Loss: 0.19162592531311212 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2050/2500 --- Train Loss: 0.049691713160171916 --- Val Loss: 0.1919672036827845 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2060/2500 --- Train Loss: 0.04975764094190231 --- Val Loss: 0.19230131202215564 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2070/2500 --- Train Loss: 0.049823731456189034 --- Val Loss: 0.19263696222094323 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2080/2500 --- Train Loss: 0.04988871566363439 --- Val Loss: 0.19296542394323132 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2090/2500 --- Train Loss: 0.04995332720520785 --- Val Loss: 0.193292157354709 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2100/2500 --- Train Loss: 0.0500171655312416 --- Val Loss: 0.1936137740330032 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2110/2500 --- Train Loss: 0.05008107396458284 --- Val Loss: 0.19393614659222672 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2120/2500 --- Train Loss: 0.05014415192907212 --- Val Loss: 0.19425361431171048 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2130/2500 --- Train Loss: 0.05020660540571682 --- Val Loss: 0.19456771899988554 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2140/2500 --- Train Loss: 0.050269327628101705 --- Val Loss: 0.19488267189894642 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2150/2500 --- Train Loss: 0.050330552292639455 --- Val Loss: 0.19518914189516073 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2160/2500 --- Train Loss: 0.05039174994186828 --- Val Loss: 0.19549555217273198 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2170/2500 --- Train Loss: 0.05045204931581054 --- Val Loss: 0.19579653549820833 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2180/2500 --- Train Loss: 0.05051253233489119 --- Val Loss: 0.1960985047471052 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2190/2500 --- Train Loss: 0.05057235777345013 --- Val Loss: 0.19639674873423227 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2200/2500 --- Train Loss: 0.050631576420880585 --- Val Loss: 0.19669142607407913 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2210/2500 --- Train Loss: 0.05069058456827316 --- Val Loss: 0.19698492100810724 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2220/2500 --- Train Loss: 0.05074889711092526 --- Val Loss: 0.19727426581549867 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2230/2500 --- Train Loss: 0.05080715365129209 --- Val Loss: 0.19756372176537457 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2240/2500 --- Train Loss: 0.05086495105183297 --- Val Loss: 0.19784953436915279 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2250/2500 --- Train Loss: 0.05092170095454628 --- Val Loss: 0.1981306687334096 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2260/2500 --- Train Loss: 0.05097895610268643 --- Val Loss: 0.19841401397815117 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2270/2500 --- Train Loss: 0.05103491065624275 --- Val Loss: 0.1986901061709317 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2280/2500 --- Train Loss: 0.05109124504544744 --- Val Loss: 0.1989679321488131 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2290/2500 --- Train Loss: 0.051146840332877315 --- Val Loss: 0.19924214025124512 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2300/2500 --- Train Loss: 0.051201949895040906 --- Val Loss: 0.19951317965786153 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2310/2500 --- Train Loss: 0.051256946100548643 --- Val Loss: 0.1997833902439144 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2320/2500 --- Train Loss: 0.05131126524957898 --- Val Loss: 0.20004997221322887 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2330/2500 --- Train Loss: 0.05136539770637834 --- Val Loss: 0.20031443165876092 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2340/2500 --- Train Loss: 0.051419899401554135 --- Val Loss: 0.20058052115378525 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2350/2500 --- Train Loss: 0.05147393445104807 --- Val Loss: 0.2008435085692472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2360/2500 --- Train Loss: 0.05152825810664451 --- Val Loss: 0.20110712750438295 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2370/2500 --- Train Loss: 0.051582033852184336 --- Val Loss: 0.20136799630922445 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2380/2500 --- Train Loss: 0.05163564050763575 --- Val Loss: 0.2016276880898069 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2390/2500 --- Train Loss: 0.05168858267309819 --- Val Loss: 0.20188348295936917 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2400/2500 --- Train Loss: 0.051741773807213634 --- Val Loss: 0.20214115419109002 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2410/2500 --- Train Loss: 0.051794367455122156 --- Val Loss: 0.20239499603761324 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2420/2500 --- Train Loss: 0.051846199530267256 --- Val Loss: 0.20264513413559923 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2430/2500 --- Train Loss: 0.05189777547616921 --- Val Loss: 0.2028935751141166 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2440/2500 --- Train Loss: 0.05194888845070302 --- Val Loss: 0.20313989490601472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2450/2500 --- Train Loss: 0.05199946804449014 --- Val Loss: 0.20338300302256326 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2460/2500 --- Train Loss: 0.05205018412564615 --- Val Loss: 0.2036270957279281 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2470/2500 --- Train Loss: 0.05210034164073493 --- Val Loss: 0.2038677660822 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2480/2500 --- Train Loss: 0.05215058632592951 --- Val Loss: 0.20410926356051587 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2490/2500 --- Train Loss: 0.05220006857998703 --- Val Loss: 0.20434664904373093 --- Train Acc: 1.00 --- Val Acc: 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABclUlEQVR4nO3deXgUVdo28Lt6TTp7CFkIIWHfIcgSkUUcooDLsGpkGEFU+JRlZKKOMoyAOE5cGd5XGVFHQGdGQRGXd0QEoqholH2HIGvCkkASsq/dfb4/qruTJgsQuqqSzv27rrq6u+p099OVkL4551SVJIQQICIiIvISOq0LICIiIvIkhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV2G4ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsioibu9OnTkCQJr776qtalEDULDDdEzdDq1ashSRJ27typdSlewRke6ltefPFFrUskoutg0LoAIqKmYvLkybjzzjtrre/Xr58G1RBRYzHcEFGLUFJSAj8/vwbb3HTTTfj973+vUkVEpBQOSxF5sT179mDMmDEIDAyEv78/Ro4ciZ9//tmtTVVVFZ577jl07twZPj4+aNWqFYYOHYrNmze72mRlZWH69Olo27YtzGYzoqKiMHbsWJw+ffqqNXzzzTcYNmwY/Pz8EBwcjLFjx+LIkSOu7evWrYMkSfjuu+9qPfett96CJEk4ePCga93Ro0cxadIkhIaGwsfHBwMGDMAXX3zh9jznsN13332HWbNmITw8HG3btr3W3daguLg43H333di0aRPi4+Ph4+ODHj16YP369bXanjx5Evfeey9CQ0NhsVhw880348svv6zVrry8HIsXL0aXLl3g4+ODqKgoTJgwASdOnKjV9u2330bHjh1hNpsxcOBA7Nixw237jfysiLwFe26IvNShQ4cwbNgwBAYG4k9/+hOMRiPeeustjBgxAt999x0SEhIAAIsXL0ZKSgoeeeQRDBo0CIWFhdi5cyd2796N22+/HQAwceJEHDp0CHPnzkVcXBwuXryIzZs3IyMjA3FxcfXWsGXLFowZMwYdOnTA4sWLUVZWhtdffx1DhgzB7t27ERcXh7vuugv+/v746KOPcOutt7o9f+3atejZsyd69erl+kxDhgxBdHQ0nnnmGfj5+eGjjz7CuHHj8Mknn2D8+PFuz581axZat26NhQsXoqSk5Kr7rLS0FDk5ObXWBwcHw2Co/nP566+/IikpCY8++iimTZuGVatW4d5778XGjRtd+yw7Oxu33HILSktL8Yc//AGtWrXCe++9h9/+9rdYt26dq1abzYa7774bqampuP/++/H444+jqKgImzdvxsGDB9GxY0fX+37wwQcoKirC//t//w+SJOHll1/GhAkTcPLkSRiNxhv6WRF5FUFEzc6qVasEALFjx45624wbN06YTCZx4sQJ17rz58+LgIAAMXz4cNe6vn37irvuuqve17l8+bIAIF555ZXrrjM+Pl6Eh4eL3Nxc17p9+/YJnU4npk6d6lo3efJkER4eLqxWq2vdhQsXhE6nE0uWLHGtGzlypOjdu7coLy93rbPb7eKWW24RnTt3dq1z7p+hQ4e6vWZ9Tp06JQDUu6SlpbnaxsbGCgDik08+ca0rKCgQUVFRol+/fq518+bNEwDEDz/84FpXVFQk2rdvL+Li4oTNZhNCCLFy5UoBQCxdurRWXXa73a2+Vq1aiby8PNf2zz//XAAQ//d//yeEuLGfFZE34bAUkRey2WzYtGkTxo0bhw4dOrjWR0VF4Xe/+x22bduGwsJCAHKvxKFDh/Drr7/W+Vq+vr4wmUzYunUrLl++fM01XLhwAXv37sWDDz6I0NBQ1/o+ffrg9ttvx4YNG1zrkpKScPHiRWzdutW1bt26dbDb7UhKSgIA5OXl4ZtvvsF9992HoqIi5OTkICcnB7m5uRg1ahR+/fVXnDt3zq2GGTNmQK/XX3PNM2fOxObNm2stPXr0cGvXpk0bt16iwMBATJ06FXv27EFWVhYAYMOGDRg0aBCGDh3qaufv74+ZM2fi9OnTOHz4MADgk08+QVhYGObOnVurHkmS3B4nJSUhJCTE9XjYsGEA5OEvoPE/KyJvw3BD5IUuXbqE0tJSdO3atda27t27w263IzMzEwCwZMkS5Ofno0uXLujduzeeeuop7N+/39XebDbjpZdewldffYWIiAgMHz4cL7/8sutLvD5nzpwBgHpryMnJcQ0VjR49GkFBQVi7dq2rzdq1axEfH48uXboAAI4fPw4hBJ599lm0bt3abVm0aBEA4OLFi27v0759+6vuq5o6d+6MxMTEWktgYKBbu06dOtUKHs46nXNbzpw5U+9nd24HgBMnTqBr165uw171adeundtjZ9BxBpnG/qyIvA3DDVELN3z4cJw4cQIrV65Er1698M9//hM33XQT/vnPf7razJs3D8eOHUNKSgp8fHzw7LPPonv37tizZ49HajCbzRg3bhw+/fRTWK1WnDt3Dj/++KOr1wYA7HY7AODJJ5+ss3dl8+bN6NSpk9vr+vr6eqS+pqK+XighhOu+0j8rouaA4YbIC7Vu3RoWiwXp6em1th09ehQ6nQ4xMTGudaGhoZg+fTo+/PBDZGZmok+fPli8eLHb8zp27IgnnngCmzZtwsGDB1FZWYnXXnut3hpiY2MBoN4awsLC3A7NTkpKQk5ODlJTU/Hxxx9DCOEWbpzDa0ajsc7elcTERAQEBFzbDrpBzl6kmo4dOwYArkm7sbGx9X5253ZA3q/p6emoqqryWH3X+7Mi8jYMN0ReSK/X44477sDnn3/udghwdnY2PvjgAwwdOtQ11JKbm+v2XH9/f3Tq1AkVFRUA5COIysvL3dp07NgRAQEBrjZ1iYqKQnx8PN577z3k5+e71h88eBCbNm2qdbK8xMREhIaGYu3atVi7di0GDRrkNqwUHh6OESNG4K233sKFCxdqvd+lS5ca3ikedP78eXz66aeux4WFhXj//fcRHx+PyMhIAMCdd96J7du3Iy0tzdWupKQEb7/9NuLi4lzzeCZOnIicnBy88cYbtd7nygB1NY39WRF5Gx4KTtSMrVy5Ehs3bqy1/vHHH8df//pXbN68GUOHDsWsWbNgMBjw1ltvoaKiAi+//LKrbY8ePTBixAj0798foaGh2LlzJ9atW4c5c+YAkHskRo4cifvuuw89evSAwWDAp59+iuzsbNx///0N1vfKK69gzJgxGDx4MB5++GHXoeBBQUG1eoaMRiMmTJiANWvWoKSkpM7rKC1fvhxDhw5F7969MWPGDHTo0AHZ2dlIS0vD2bNnsW/fvkbsxWq7d+/Gv//971rrO3bsiMGDB7sed+nSBQ8//DB27NiBiIgIrFy5EtnZ2Vi1apWrzTPPPIMPP/wQY8aMwR/+8AeEhobivffew6lTp/DJJ59Ap5P/bzl16lS8//77SE5Oxvbt2zFs2DCUlJRgy5YtmDVrFsaOHXvN9d/Iz4rIq2h6rBYRNYrzUOf6lszMTCGEELt37xajRo0S/v7+wmKxiNtuu0389NNPbq/117/+VQwaNEgEBwcLX19f0a1bN/HCCy+IyspKIYQQOTk5Yvbs2aJbt27Cz89PBAUFiYSEBPHRRx9dU61btmwRQ4YMEb6+viIwMFDcc8894vDhw3W23bx5swAgJElyfYYrnThxQkydOlVERkYKo9EooqOjxd133y3WrVtXa/80dKh8TVc7FHzatGmutrGxseKuu+4SX3/9tejTp48wm82iW7du4uOPP66z1kmTJong4GDh4+MjBg0aJP773//WaldaWioWLFgg2rdvL4xGo4iMjBSTJk1yHcbvrK+uQ7wBiEWLFgkhbvxnReQtJCGus9+TiKgFi4uLQ69evfDf//5X61KIqB6cc0NEREReheGGiIiIvArDDREREXkVzrkhIiIir8KeGyIiIvIqDDdERETkVVrcSfzsdjvOnz+PgICAWhe+IyIioqZJCIGioiK0adPGdRLM+rS4cHP+/Hm3a+oQERFR85GZmYm2bds22KbFhRvnhfUyMzNd19YhIiKipq2wsBAxMTHXdIHcFhdunENRgYGBDDdERETNzLVMKeGEYiIiIvIqDDdERETkVRhuiIiIyKu0uDk3RETkXWw2G6qqqrQugzzAZDJd9TDva8FwQ0REzZIQAllZWcjPz9e6FPIQnU6H9u3bw2Qy3dDrMNwQEVGz5Aw24eHhsFgsPDFrM+c8ye6FCxfQrl27G/p5Nolws3z5crzyyivIyspC37598frrr2PQoEF1th0xYgS+++67WuvvvPNOfPnll0qXSkRETYDNZnMFm1atWmldDnlI69atcf78eVitVhiNxka/juYTiteuXYvk5GQsWrQIu3fvRt++fTFq1ChcvHixzvbr16/HhQsXXMvBgweh1+tx7733qlw5ERFpxTnHxmKxaFwJeZJzOMpms93Q62gebpYuXYoZM2Zg+vTp6NGjB1asWAGLxYKVK1fW2T40NBSRkZGuZfPmzbBYLAw3REQtEIeivIunfp6ahpvKykrs2rULiYmJrnU6nQ6JiYlIS0u7ptd49913cf/998PPz6/O7RUVFSgsLHRbiIiIyHtpGm5ycnJgs9kQERHhtj4iIgJZWVlXff727dtx8OBBPPLII/W2SUlJQVBQkGvhRTOJiMjbxMXFYdmyZVqX0WRoPix1I95991307t273snHADB//nwUFBS4lszMTBUrJCIiqiZJUoPL4sWLG/W6O3bswMyZM2+othEjRmDevHk39BpNhaZHS4WFhUGv1yM7O9ttfXZ2NiIjIxt8bklJCdasWYMlS5Y02M5sNsNsNt9wrdfi4P5daBMagNC2XVR5PyIial4uXLjgur927VosXLgQ6enprnX+/v6u+0II2Gw2GAxX/6pu3bq1Zwtt5jTtuTGZTOjfvz9SU1Nd6+x2O1JTUzF48OAGn/vxxx+joqICv//975Uu85oc/GYNOnwyBtmrH0B5RaXW5RARURNU84CYoKAgSJLkenz06FEEBATgq6++Qv/+/WE2m7Ft2zacOHECY8eORUREBPz9/TFw4EBs2bLF7XWvHJaSJAn//Oc/MX78eFgsFnTu3BlffPHFDdX+ySefoGfPnjCbzYiLi8Nrr73mtv0f//gHOnfuDB8fH0RERGDSpEmubevWrUPv3r3h6+uLVq1aITExESUlJTdUT0M0H5ZKTk7GO++8g/feew9HjhzBY489hpKSEkyfPh0AMHXqVMyfP7/W8959912MGzeuyZzfIDC2L+ySHt2tR/HtZ+9qXQ4RUYsjhEBppVWTRQjhsc/xzDPP4MUXX8SRI0fQp08fFBcX484770Rqair27NmD0aNH45577kFGRkaDr/Pcc8/hvvvuw/79+3HnnXdiypQpyMvLa1RNu3btwn333Yf7778fBw4cwOLFi/Hss89i9erVAICdO3fiD3/4A5YsWYL09HRs3LgRw4cPByD3Vk2ePBkPPfQQjhw5gq1bt2LChAke3WdX0vwkfklJSbh06RIWLlyIrKwsxMfHY+PGja5JxhkZGbWuM5Geno5t27Zh06ZNWpRcp3YduyO96zR0TX8TYUfeR0HZIwjybfwJiIiI6PqUVdnQY+HXmrz34SWjYDF55it1yZIluP32212PQ0ND0bdvX9fj559/Hp9++im++OILzJkzp97XefDBBzF58mQAwN/+9jf87//+L7Zv347Ro0dfd01Lly7FyJEj8eyzzwIAunTpgsOHD+OVV17Bgw8+iIyMDPj5+eHuu+9GQEAAYmNj0a9fPwByuLFarZgwYQJiY2MBAL17977uGq6H5j03ADBnzhycOXMGFRUV+OWXX5CQkODatnXrVlcydOratSuEEG4//Kag85g5sEPCQBzGJ6nbtC6HiIiaoQEDBrg9Li4uxpNPPonu3bsjODgY/v7+OHLkyFV7bvr06eO67+fnh8DAwHpPkHs1R44cwZAhQ9zWDRkyBL/++itsNhtuv/12xMbGokOHDnjggQfwn//8B6WlpQCAvn37YuTIkejduzfuvfdevPPOO7h8+XKj6rhWmvfceBNdcFtcCh+M1hd/QtnO/6Bg5FD23hARqcTXqMfhJaM0e29PufK8bU8++SQ2b96MV199FZ06dYKvry8mTZqEysqG53deefkCSZJgt9s9VmdNAQEB2L17N7Zu3YpNmzZh4cKFWLx4MXbs2IHg4GBs3rwZP/30EzZt2oTXX38dCxYswC+//IL27dsrUk+T6LnxJq2GPAgAuMe+Fcs2H9W2GCKiFkSSJFhMBk0WJc+U/OOPP+LBBx/E+PHj0bt3b0RGRuL06dOKvV9dunfvjh9//LFWXV26dIFeLwc7g8GAxMREvPzyy9i/fz9Onz6Nb775BoD8sxkyZAiee+457NmzByaTCZ9++qli9bLnxsN03e+B9f/80M56CYfTvsYzVQK3dmkNH6MeNruATYgrJlFJcP6bkOA4B4Jzi4Qa2ySgjnaSJG+TnJuveOz8B1f9uPo963sdXPm6jvs6CbCYDQjwMcDfZIBOx9OeExEprXPnzli/fj3uueceSJKEZ599VrEemEuXLmHv3r1u66KiovDEE09g4MCBeP7555GUlIS0tDS88cYb+Mc//gEA+O9//4uTJ09i+PDhCAkJwYYNG2C329G1a1f88ssvSE1NxR133IHw8HD88ssvuHTpErp3767IZwAYbjzPZIGh9wRgz78wUf89/rSjO9bs8M4TBwb5GtGxtR+6RQXijh4RGNa5NfQMPEREHrV06VI89NBDuOWWWxAWFoann35asUsJffDBB/jggw/c1j3//PP4y1/+go8++ggLFy7E888/j6ioKCxZsgQPPvggACA4OBjr16/H4sWLUV5ejs6dO+PDDz9Ez549ceTIEXz//fdYtmwZCgsLERsbi9deew1jxoxR5DMAgCSUPBarCSosLERQUBAKCgoQGBiozJucSQNWjYbNYMGiDmtxOF8Pq11AkiTopereEgG4enHk+/KtvEK47svrRfV9ccVzHdtrrheOF61+DVHjPYTrderdBuf26sc2u3yYZZWt7l+ZHlGBWHZ/PLpEBHhkNxIR1ae8vBynTp1C+/bt4ePjo3U55CEN/Vyv5/ubPTdKaHczEN4T+ouH8FffD4HJ/6geX2rmhBCosNpRVG5FTnEFjl8sxvZTefh87zkcvlCI+95Kw2ezhiAurO4LmRIRESmN4UYJkgTc9Sqw6k5g3wcABDB4NtC6G6Bv4OgpIQC7DRA2QNir79sdj2uts1V35dScLAOpjltdA9sc2xt8vgTozZD0BvgY9fAx6tE6wIzuUYG4p28bzEvsjIdW78C+swX40yf7sXbmzYpOsCMiIqoPw41SYm+RA86XTwL7PpQXSIDBRw44NYOK8z7qHu5pUowWwBwImAOAgEggrDMQ3R+tut6Jf/y+P257dSu2n8rDL6fycHOHpnH2aCIialkYbpQ08BEgvCfw4zLg9DagshiwlslLozh6WHR6QNJX3wKQJ9iIq9zaa6+7XlWl8lKcBeT+Cpz+Adi5EtCbED14NibF/xYf7LyAz/eeY7ghIiJNMNwoLXawvNjtQGkOUFUG2K2OoaAaAcUVWnR1rHO2U2iYxzVL2Y4Gw5G1AqgoBMoL5duCs8DFI8DxLUD2QWDb3/FE9H58iOnYfPgi/jZecGiKiIhUx3CjFp0O8A/Xuoq6uU6oc5VzOpr8AEto7fWJi4EjXwDr/x9anUvFTGMk3iq+C2cvlyEm1KJExURERPXiGYrpxkkS0GMsMOZFAMAcw+fwQxl2Zyh77RAiIqK6MNyQ5/R7AAhpjwBRjNt1u/BrdrHWFRERUQvEcEOeo9MDfe4DAIzRb8fp3BKNCyIiopaI4YY8q/MdAIABunRkMNwQESlixIgRmDdvntZlNFkMN+RZkX1g15vRSioCck9oXQ0RUZNyzz33YPTo0XVu++GHHyBJEvbv33/D77N69WoEBwff8Os0Vww35FkGE0R4TwBAdOVJlFfZNC6IiKjpePjhh7F582acPXu21rZVq1ZhwIAB6NOnjwaVeReGG/I4XetOAID2UhZySyo1roaIqOm4++670bp1a6xevdptfXFxMT7++GM8/PDDyM3NxeTJkxEdHQ2LxYLevXvjww8/9GgdGRkZGDt2LPz9/REYGIj77rsP2dnZru379u3DbbfdhoCAAAQGBqJ///7YuXMnAODMmTO45557EBISAj8/P/Ts2RMbNmzwaH03iue5IY+TQuVwEydlIbe4AtHBvhpXREQtghDyGdS1YLRc04lWDQYDpk6ditWrV2PBggWuE51+/PHHsNlsmDx5MoqLi9G/f388/fTTCAwMxJdffokHHngAHTt2xKBBg264VLvd7go23333HaxWK2bPno2kpCRs3boVADBlyhT069cPb775JvR6Pfbu3QujUb424uzZs1FZWYnvv/8efn5+OHz4MPz9/W+4Lk9iuCHPa9URABCnY88NEamoqhT4Wxtt3vvP5+UTnV6Dhx56CK+88gq+++47jBgxAoA8JDVx4kQEBQUhKCgITz75pKv93Llz8fXXX+Ojjz7ySLhJTU3FgQMHcOrUKcTExAAA3n//ffTs2RM7duzAwIEDkZGRgaeeegrdunUDAHTu3Nn1/IyMDEycOBG9e/cGAHTo0OGGa/I0DkuR5wW1BQBE4DJyixluiIhq6tatG2655RasXLkSAHD8+HH88MMPePjhhwEANpsNzz//PHr37o3Q0FD4+/vj66+/RkZGhkfe/8iRI4iJiXEFGwDo0aMHgoODceTIEQBAcnIyHnnkESQmJuLFF1/EiRPVB4j84Q9/wF//+lcMGTIEixYt8sgEaE9jzw15nl9rAECYVIC8kgqNiyGiFsNokXtQtHrv6/Dwww9j7ty5WL58OVatWoWOHTvi1ltvBQC88sor+J//+R8sW7YMvXv3hp+fH+bNm4fKSvX+s7h48WL87ne/w5dffomvvvoKixYtwpo1azB+/Hg88sgjGDVqFL788kts2rQJKSkpeO211zB37lzV6rsa9tyQ5znCjZ9UgcLCAo2LIaIWQ5LkoSEtluu8SPB9990HnU6HDz74AO+//z4eeugh1/ybH3/8EWPHjsXvf/979O3bFx06dMCxY8c8tpu6d++OzMxMZGZmutYdPnwY+fn56NGjh2tdly5d8Mc//hGbNm3ChAkTsGrVKte2mJgYPProo1i/fj2eeOIJvPPOOx6rzxPYc0OeZw6AVWeGwV4BqeSS1tUQETU5/v7+SEpKwvz581FYWIgHH3zQta1z585Yt24dfvrpJ4SEhGDp0qXIzs52Cx7XwmazYe/evW7rzGYzEhMT0bt3b0yZMgXLli2D1WrFrFmzcOutt2LAgAEoKyvDU089hUmTJqF9+/Y4e/YsduzYgYkTJwIA5s2bhzFjxqBLly64fPkyvv32W3Tv3v1Gd4lHMdyQ50kSykyhCCi/AF0pww0RUV0efvhhvPvuu7jzzjvRpk31ROi//OUvOHnyJEaNGgWLxYKZM2di3LhxKCi4vp7w4uJi9OvXz21dx44dcfz4cXz++eeYO3cuhg8fDp1Oh9GjR+P1118HAOj1euTm5mLq1KnIzs5GWFgYJkyYgOeeew6AHJpmz56Ns2fPIjAwEKNHj8bf//73G9wbniUJIYTWRaipsLAQQUFBKCgoQGBgoNbleK3cvw9Bq4KDeCP8OcyZNU/rcojIy5SXl+PUqVNo3749fHx8tC6HPKShn+v1fH9zzg0pwuYTAgDQV3DODRERqYvhhpRhllO1vqpI40KIiKilYbghRUg+crgxVBVrXAkREbU0DDekCJ1vEADAZGW4ISIidTHckCIkR7jxtZdoXAkRebMWdkyM1/PUz5PhhhRhcIQbi2C4ISLPc17EsbRUowtlkiKcZ2HW6/U39Do8zw0pwhlu/EQpqmx2GPXM0UTkOXq9HsHBwbh48SIAwGKxuM7wS82T3W7HpUuXYLFYYDDcWDxhuCFFGHzk66z4SJUor7Ix3BCRx0VGRgKAK+BQ86fT6dCuXbsbDqoMN6QIo48/AMAXFSivsiOA59giIg+TJAlRUVEIDw9HVVWV1uWQB5hMJuh0N/6fYYYbUoRkkntuLKhAeZVN42qIyJvp9fobnqNB3oVjBaQMY/WwVBnDDRERqYjhhpThCDe+qEBZJcMNERGph+GGlFFjWIo9N0REpCaGG1KGa1iqClVWq8bFEBFRS6J5uFm+fDni4uLg4+ODhIQEbN++vcH2+fn5mD17NqKiomA2m9GlSxds2LBBpWrpmhl9XXftFTyRHxERqUfTo6XWrl2L5ORkrFixAgkJCVi2bBlGjRqF9PR0hIeH12pfWVmJ22+/HeHh4Vi3bh2io6Nx5swZBAcHq188NcxQHW6sleUaFkJERC2NpuFm6dKlmDFjBqZPnw4AWLFiBb788kusXLkSzzzzTK32K1euRF5eHn766SfXqbfj4uLULJmulU4HK/QwwAZ7VYXW1RARUQui2bBUZWUldu3ahcTExOpidDokJiYiLS2tzud88cUXGDx4MGbPno2IiAj06tULf/vb32CzccJqU2SV5ABqY88NERGpSLOem5ycHNhsNkRERLitj4iIwNGjR+t8zsmTJ/HNN99gypQp2LBhA44fP45Zs2ahqqoKixYtqvM5FRUVqKio7jkoLCz03IegBtkkIyDKYbOy54aIiNSj+YTi62G32xEeHo63334b/fv3R1JSEhYsWIAVK1bU+5yUlBQEBQW5lpiYGBUrbtmcPTeCw1JERKQizcJNWFgY9Ho9srOz3dZnZ2e7LoZ2paioKHTp0sXtNNvdu3dHVlaW6zLpV5o/fz4KCgpcS2Zmpuc+BDXINSzFnhsiIlKRZuHGZDKhf//+SE1Nda2z2+1ITU3F4MGD63zOkCFDcPz4cdjtdte6Y8eOISoqCiaTqc7nmM1mBAYGui2kDrvO0XNjrTt4EhERKUHTYank5GS88847eO+993DkyBE89thjKCkpcR09NXXqVMyfP9/V/rHHHkNeXh4ef/xxHDt2DF9++SX+9re/Yfbs2Vp9BGqATScHTh4tRUREatL0UPCkpCRcunQJCxcuRFZWFuLj47Fx40bXJOOMjAy3S5/HxMTg66+/xh//+Ef06dMH0dHRePzxx/H0009r9RGoATbHsBRsDDdERKQeSQghtC5CTYWFhQgKCkJBQQGHqBSW+coQxJQcxLrOL2HSlEe1LoeIiJqx6/n+blZHS1HzYtc75kFxzg0REamI4YYUIxxzbmBjuCEiIvUw3JBihKvnhnNuiIhIPQw3pBhnz43EnhsiIlIRww0pRugdR0vZGW6IiEg9DDekGKE3A2DPDRERqYvhhpTjmHOjY88NERGpiOGGlGNwzrmp0rgQIiJqSRhuSDESe26IiEgDDDekGMnZc2Nnzw0REamH4YaU45hQrGfPDRERqYjhhpTj6LnRC6vGhRARUUvCcEOK0enki85LwqZxJURE1JIw3JBy9HK4Yc8NERGpieGGFKNznKGYPTdERKQmhhtSjOQINzqGGyIiUhHDDSlG4rAUERFpgOGGFKMzOHpuwJ4bIiJSD8MNKUbn6LnhsBQREamJ4YYU45xzY+CwFBERqYjhhhSjd/bccFiKiIhUxHBDinH23Og5LEVERCpiuCHF6J3hhj03RESkIoYbUozzaCk9bLDbhcbVEBFRS8FwQ4pxhhsD7LAJhhsiIlIHww0pRu8KN1ZYbQw3RESkDoYbUozzPDcG2GG12zWuhoiIWgqGG1KM3mCSbyUbbJxzQ0REKmG4IcXoDTV7bhhuiIhIHQw3pBhJVz3nhj03RESkFoYbUo6++mgp9twQEZFaGG5IOTo9APk8NzYeLUVERCphuCHl6Jxzbmw8WoqIiFTDcEPKcc254dFSRESkHoYbUo6j50YvCVhtvL4UERGpg+GGlOOYcwMAdmuVhoUQEVFLwnBDynH03ABAFcMNERGphOGGlOM4FBxgzw0REamH4YaUU6PnhuGGiIjUwnBDypGqf73sNquGhRARUUvCcEPKkSRUQe69sdnYc0NEROpoEuFm+fLliIuLg4+PDxISErB9+/Z6265evRqSJLktPj4+KlZL18MO+YgpYa3UuBIiImopNA83a9euRXJyMhYtWoTdu3ejb9++GDVqFC5evFjvcwIDA3HhwgXXcubMGRUrputhcwxN2TgsRUREKtE83CxduhQzZszA9OnT0aNHD6xYsQIWiwUrV66s9zmSJCEyMtK1REREqFgxXQ+bY1iKE4qJiEgtmoabyspK7Nq1C4mJia51Op0OiYmJSEtLq/d5xcXFiI2NRUxMDMaOHYtDhw7V27aiogKFhYVuC6nH7ui5EXb23BARkTo0DTc5OTmw2Wy1el4iIiKQlZVV53O6du2KlStX4vPPP8e///1v2O123HLLLTh79myd7VNSUhAUFORaYmJiPP45qH6uOTccliIiIpVoPix1vQYPHoypU6ciPj4et956K9avX4/WrVvjrbfeqrP9/PnzUVBQ4FoyMzNVrrhls0tyuLHx2lJERKQSw9WbKCcsLAx6vR7Z2dlu67OzsxEZGXlNr2E0GtGvXz8cP368zu1msxlms/mGa6XGcYYbsOeGiIhUomnPjclkQv/+/ZGamupaZ7fbkZqaisGDB1/Ta9hsNhw4cABRUVFKlUk3wBlu7JxzQ0REKtG05wYAkpOTMW3aNAwYMACDBg3CsmXLUFJSgunTpwMApk6diujoaKSkpAAAlixZgptvvhmdOnVCfn4+XnnlFZw5cwaPPPKIlh+D6iFcc254tBQREalD83CTlJSES5cuYeHChcjKykJ8fDw2btzommSckZEBna66g+ny5cuYMWMGsrKyEBISgv79++Onn35Cjx49tPoI1ABnzw0nFBMRkVokIYTQugg1FRYWIigoCAUFBQgMDNS6HK93NqU/2lYcx1fx/8CYcVO0LoeIiJqp6/n+bnZHS1HzYpfkzkGe54aIiNTCcEOKEhyWIiIilTHckKKE4wzFYM8NERGphOGGFCU4LEVERCpjuCFFCZ3jJH52nqGYiIjUwXBDiuKcGyIiUhvDDSnKOSzFOTdERKQWhhtSVPWwFMMNERGpg+GGlOUYlpI454aIiFTCcEOKEjrH0VKC4YaIiNTBcEPK4rAUERGpjOGGFMUJxUREpDaGG1KWjnNuiIhIXQw3pCznsJRgzw0REamD4YaU5ZhQLNntGhdCREQtBcMNKco154Y9N0REpBKGG1KWa84Nww0REamD4YaU5RyW4nluiIhIJQw3pChJz54bIiJSF8MNKUtizw0REamL4YYUJemdE4p5tBQREamD4YaU5Zhzo+PRUkREpBKGG1KUxAnFRESkMoYbUpRzQrGOl18gIiKVMNyQothzQ0REamO4IUU5JxTrwHBDRETqYLghRbHnhoiI1MZwQ4qSDM6jpRhuiIhIHQw3pChJckwoZrghIiKVMNyQonR6o3zLcENERCphuCFFcUIxERGpjeGGFKUzsOeGiIjUxXBDinIeLaVnzw0REamE4YYUpXOeoZgXziQiIpUw3JCinBOK2XNDRERqYbghRemc57kBe26IiEgdDDekKJ3jaCk9JxQTEZFKGG5IUTpd9bCUEELjaoiIqCVguCFF6Q3V4cZmZ7ghIiLlMdyQopxHSxlgh5XhhoiIVNAkws3y5csRFxcHHx8fJCQkYPv27df0vDVr1kCSJIwbN07ZAqnRnD03OsnOnhsiIlKF5uFm7dq1SE5OxqJFi7B792707dsXo0aNwsWLFxt83unTp/Hkk09i2LBhKlVKjeGcUGyAjT03RESkCs3DzdKlSzFjxgxMnz4dPXr0wIoVK2CxWLBy5cp6n2Oz2TBlyhQ899xz6NChg4rV0vXSu85zw54bIiJSh6bhprKyErt27UJiYqJrnU6nQ2JiItLS0up93pIlSxAeHo6HH35YjTLpBjivLWXghGIiIlKJQcs3z8nJgc1mQ0REhNv6iIgIHD16tM7nbNu2De+++y727t17Te9RUVGBiooK1+PCwsJG10uNIMn5WQ87yhluiIhIBZoPS12PoqIiPPDAA3jnnXcQFhZ2Tc9JSUlBUFCQa4mJiVG4SnKjqznnhmcpJiIi5WnacxMWFga9Xo/s7Gy39dnZ2YiMjKzV/sSJEzh9+jTuuece1zq74wvTYDAgPT0dHTt2dHvO/PnzkZyc7HpcWFjIgKMmXfXlFzgsRUREatA03JhMJvTv3x+pqamuw7ntdjtSU1MxZ86cWu27deuGAwcOuK37y1/+gqKiIvzP//xPnaHFbDbDbDYrUj9dgxo9Nww3RESkBk3DDQAkJydj2rRpGDBgAAYNGoRly5ahpKQE06dPBwBMnToV0dHRSElJgY+PD3r16uX2/ODgYACotZ6aCEe40UsCNhuvL0VERMrTPNwkJSXh0qVLWLhwIbKyshAfH4+NGze6JhlnZGRAp2tWU4Oopho/O6vVqmEhRETUUkiiEVczzMzMhCRJaNu2LQBg+/bt+OCDD9CjRw/MnDnT40V6UmFhIYKCglBQUIDAwECty/F+FUVAivx7cmj6MfSMjbjKE4iIiGq7nu/vRnWJ/O53v8O3334LAMjKysLtt9+O7du3Y8GCBViyZEljXpK8la66c9BuY88NEREpr1Hh5uDBgxg0aBAA4KOPPkKvXr3w008/4T//+Q9Wr17tyfqouasRbmzWKg0LISKilqJR4aaqqsp1BNKWLVvw29/+FoB8NNOFCxc8Vx01f5LedZc9N0REpIZGhZuePXtixYoV+OGHH7B582aMHj0aAHD+/Hm0atXKowVSM6fTwQ4JAGCzseeGiIiU16hw89JLL+Gtt97CiBEjMHnyZPTt2xcA8MUXX7iGq4icbJB7b+wMN0REpIJGHQo+YsQI5OTkoLCwECEhIa71M2fOhMVi8Vhx5B1s0MMIK+w8zw0REamgUT03ZWVlqKiocAWbM2fOYNmyZUhPT0d4eLhHC6Tmz+64eKbgnBsiIlJBo8LN2LFj8f777wMA8vPzkZCQgNdeew3jxo3Dm2++6dECqflzDktxzg0REamhUeFm9+7dGDZsGABg3bp1iIiIwJkzZ/D+++/jf//3fz1aIDV/whFu2HNDRERqaFS4KS0tRUBAAABg06ZNmDBhAnQ6HW6++WacOXPGowVS82eTGG6IiEg9jQo3nTp1wmeffYbMzEx8/fXXuOOOOwAAFy9e5CUNqBbnnBtOKCYiIjU0KtwsXLgQTz75JOLi4jBo0CAMHjwYgNyL069fP48WSM2f3XFQHg8FJyIiNTTqUPBJkyZh6NChuHDhguscNwAwcuRIjB8/3mPFkXewc1iKiIhU1KhwAwCRkZGIjIzE2bNnAQBt27blCfyoTq5wY2e4ISIi5TVqWMput2PJkiUICgpCbGwsYmNjERwcjOeffx52u93TNVIzJ1znueGwFBERKa9RPTcLFizAu+++ixdffBFDhgwBAGzbtg2LFy9GeXk5XnjhBY8WSc2bs+fGxgnFRESkgkaFm/feew///Oc/XVcDB4A+ffogOjoas2bNYrghN0JyTijmsBQRESmvUcNSeXl56NatW6313bp1Q15e3g0XRd5FuCYUc1iKiIiU16hw07dvX7zxxhu11r/xxhvo06fPDRdF3kXo2HNDRETqadSw1Msvv4y77roLW7ZscZ3jJi0tDZmZmdiwYYNHC6TmT/DCmUREpKJG9dzceuutOHbsGMaPH4/8/Hzk5+djwoQJOHToEP71r395ukZq7nQ8iR8REamn0ee5adOmTa2Jw/v27cO7776Lt99++4YLI+9RPeeGR0sREZHyGtVzQ3RdHD03PIkfERGpgeGGlOcMN5xzQ0REKmC4IeXpHL9mds65ISIi5V3XnJsJEyY0uD0/P/9GaiEvJXRG+dbOOTdERKS86wo3QUFBV90+derUGyqIvJDeBACQeLQUERGp4LrCzapVq5Sqg7yZo+dGsldqXAgREbUEnHNDihN6Z7hhzw0RESmP4YYUJ7nCDY+WIiIi5THckPIcc2507LkhIiIVMNyQ4iQOSxERkYoYbkh5BvbcEBGRehhuSHE617AU59wQEZHyGG5IcZIz3Aj23BARkfIYbkhxklGec6NnuCEiIhUw3JDidHozAIYbIiJSB8MNKU5nlIel9ILXliIiIuUx3JDidHoOSxERkXoYbkhxeqM8LGUQPFqKiIiUx3BDitMZnMNSDDdERKS8JhFuli9fjri4OPj4+CAhIQHbt2+vt+369esxYMAABAcHw8/PD/Hx8fjXv/6lYrV0vVw9N7BCCKFxNURE5O00Dzdr165FcnIyFi1ahN27d6Nv374YNWoULl68WGf70NBQLFiwAGlpadi/fz+mT5+O6dOn4+uvv1a5crpWeoM858YIK2x2hhsiIlKW5uFm6dKlmDFjBqZPn44ePXpgxYoVsFgsWLlyZZ3tR4wYgfHjx6N79+7o2LEjHn/8cfTp0wfbtm1TuXK6Vs6eGyOssDLcEBGRwjQNN5WVldi1axcSExNd63Q6HRITE5GWlnbV5wshkJqaivT0dAwfPrzONhUVFSgsLHRbSF3Vw1I2VNrsGldDRETeTtNwk5OTA5vNhoiICLf1ERERyMrKqvd5BQUF8Pf3h8lkwl133YXXX38dt99+e51tU1JSEBQU5FpiYmI8+hno6gyOcGOCFVYbe26IiEhZmg9LNUZAQAD27t2LHTt24IUXXkBycjK2bt1aZ9v58+ejoKDAtWRmZqpbLEHnnHMjWVHFnhsiIlKYQcs3DwsLg16vR3Z2ttv67OxsREZG1vs8nU6HTp06AQDi4+Nx5MgRpKSkYMSIEbXams1mmM1mj9ZN18lx4UwjrChjuCEiIoVp2nNjMpnQv39/pKamutbZ7XakpqZi8ODB1/w6drsdFRUVSpRInuAKNzZUcViKiIgUpmnPDQAkJydj2rRpGDBgAAYNGoRly5ahpKQE06dPBwBMnToV0dHRSElJASDPoRkwYAA6duyIiooKbNiwAf/617/w5ptvavkxqCE6+dfMCCus7LkhIiKFaR5ukpKScOnSJSxcuBBZWVmIj4/Hxo0bXZOMMzIyoNNVdzCVlJRg1qxZOHv2LHx9fdGtWzf8+9//RlJSklYfga6mxrAUj5YiIiKlSaKFnTK2sLAQQUFBKCgoQGBgoNbltAwlucArHQAA+x46hb7tQjUuiIiImpvr+f5ulkdLUTPjuCo4AFirODeKiIiUxXBDynMLN5UaFkJERC0Bww0pzzHnBgDsVvbcEBGRshhuSHk6PWyOXzUbe26IiEhhDDekChv0ADgsRUREymO4IVVYJXnejb2qXONKiIjI2zHckCqc4cZWyTk3RESkLIYbUoVVkicV26zsuSEiImUx3JAqbDrnsBR7boiISFkMN6QKZ88Nww0RESmN4YZUYXf03Aie54aIiBTGcEOqsOnknhvBo6WIiEhhDDekCrvODIA9N0REpDyGG1KF3XkJBoYbIiJSGMMNqcIZboSNZygmIiJlMdyQOhzhRuJ5boiISGEMN6QKu16ecwP23BARkcIYbkgdzp4bhhsiIlIYww2pw9FzI9k4oZiIiJTFcEPqMMjhRseeGyIiUhjDDanDGW7sDDdERKQshhtShcRwQ0REKmG4IVXojHK40TPcEBGRwhhuSBWSwQcAww0RESmP4YZUoXMMSxkYboiISGEMN6QKnckxLCWqNK6EiIi8HcMNqUJnlIelDII9N0REpCyGG1KF3hVu2HNDRETKYrghVRhMDDdERKQOhhtShbPnxsRhKSIiUhjDDanCYJbDjRFW2OxC42qIiMibMdyQKpzDUmapClU2u8bVEBGRN2O4IVUYHeHGBCsqrAw3RESkHIYbUoXBFW6qUMlwQ0RECmK4IVU4L5xpRhUqOSxFREQKYrghdRjYc0NEROpguCF16E3yjSRQWcnDwYmISDkMN6QOx7AUAFRVlGtYCBEReTuGG1KHvjrcWCvLNCyEiIi8HcMNqUNvgM3x62atZM8NEREpp0mEm+XLlyMuLg4+Pj5ISEjA9u3b6237zjvvYNiwYQgJCUFISAgSExMbbE9NRxWMAABbFcMNEREpx6B1AWvXrkVycjJWrFiBhIQELFu2DKNGjUJ6ejrCw8Nrtd+6dSsmT56MW265BT4+PnjppZdwxx134NChQ4iOjtbgE9C1qpKM8BEV7LkhImpOhAAqi4Gyy/JSmld9/8rFuS2yFzBppWYlS0IITS/0k5CQgIEDB+KNN94AANjtdsTExGDu3Ll45plnrvp8m82GkJAQvPHGG5g6depV2xcWFiIoKAgFBQUIDAy84frp2l1eEocQ+2V8/5vPMHz4bVqXQ0TUstQMKbUCSh5Qll9/gLFXXd97RcUD/+87j5Z/Pd/fmvbcVFZWYteuXZg/f75rnU6nQ2JiItLS0q7pNUpLS1FVVYXQ0FClyiQPqZLkw8HtVZxQTER0wypLgNJceSnJrb5fmlN7fWNDSk16E+AbClhCAd8QxxIsr3M+dm7zj/DYx2wMTcNNTk4ObDYbIiLcd0JERASOHj16Ta/x9NNPo02bNkhMTKxze0VFBSoqKlyPCwsLG18w3RCrzgzYAMFwQ0TkzmaVe09cocQZUPKuCCs5jnW5gLWRf0vrDClXLG7bHPeNvoAkefZzK0TzOTc34sUXX8SaNWuwdetW+Pj41NkmJSUFzz33nMqVUV2qJPlwcPbcEJHXE0LuKSnJAUouAsUXgZJLjtuL1eHFeVue37j30ZsASxhgaQX4tZJv61xCm2VIaSxNw01YWBj0ej2ys7Pd1mdnZyMyMrLB57766qt48cUXsWXLFvTp06fedvPnz0dycrLrcWFhIWJiYm6scGoUq/NcNzzPDRE1R3ab3GtSX1hx3i++JG+77iEgydFr0grwC6sOJc7w4lofWv3Y5O/1QaUxNA03JpMJ/fv3R2pqKsaNGwdAnlCcmpqKOXPm1Pu8l19+GS+88AK+/vprDBgwoMH3MJvNMJvNDbYhdVh1cu8ah6WIqEmpKgeKs4CiGkvNx87QUpoLiOu8Np5PEODXGvALB/ydt+FySPFr7d674hsC6PTKfMYWRvNhqeTkZEybNg0DBgzAoEGDsGzZMpSUlGD69OkAgKlTpyI6OhopKSkAgJdeegkLFy7EBx98gLi4OGRlZQEA/P394e/vr9nnoKuz6R1Dh1YeCk5EKmgwtFwAirLl2+saEpLknpOaYcWv9RXBpXX1rYH/udaC5uEmKSkJly5dwsKFC5GVlYX4+Hhs3LjRNck4IyMDOl31uQbffPNNVFZWYtKkSW6vs2jRIixevFjN0uk6OcONZC3VuBIiataEAMoLgMJzQOF5oOBs9f3Cc40LLXozEBAJBEQBARHyrX+EvM4/vDq4WMIAveZfnXQVTeInNGfOnHqHobZu3er2+PTp08oXRIqwu8INe26IqB5CABWFQIEzrJx1v194Xn5cVXJtr+cKLZHV4cXfEV5qrvcJ5twVL9Ikwg21DHa9LwBA19jDF4mo+bPb5fkr+Rm1F2cPTGXxtb2WbwgQ2BYIbAMERcu3gdE1emAYWloqhhtSjTCy54bI69nt8ryW/ExHaDlTI7xkyuttFVd/HZ9gIKhtdWAJjHYEGOfSBjBZFP841Dwx3JB6DHK40THcEDVv1ko5rFw+BeSdBPIct5dPAZdPA7bKhp8v6RxhJQYIble9BLWtDjQmP1U+CnknhhtSjeT4X5bOxnBD1ORVlsihpa4AU3C24UOiJb3cyxLUzj28BDvCTGA0oDeq91moxWG4IdVIRnnOjZ7hhqhpqCoDck8Aub8COcerw0veSaA4u+HnGi1ASHsg1Ll0qH4c2JZHFJGm+NtHqtGZGG6IVCeEfFh0zjEg51cg97jj9ld5/gtE/c/1DXEElg61A4x/BCfqUpPFcEOq0TuGpYx2hhsijxNC7m25eAS4dBS4eBi4eFS+X9HABYN9goBWnYGwzkBox+oQE9peDjdEzRDDDalGZ5YnCBrs13CkBBHVrzQPyD7kCDFHHIHmiHyhxrpIeiAkTg4wrTrJt2Fd5FDjF8YeGPI6DDekGoNZHpYyMdwQXRu7Hcg/DWQdcF8Kz9XdXtLJw0bh3eWldTcgvIccaAwmVUsn0hLDDanG4Oi5MQqGG6Jaqsrl3he3IHMQqCyqu31wbI0Q47gN6ww4Ju4TtWQMN6Qao48858aEq5wDg8jb2W3ykNK53cD53cC5XfIwk91au63eLAeXyN5AZB/5NqIn4BOoft1EzQTDDanG5CP33Piw54ZaEiHkE9ud3y2HmXO7gQv76r42km9IdYBx3oZ15jlhiK4Tww2pxuTrDwDwRQVsdgG9jpMYyQtZK4ELe4GMNOBMGpD5C1CWV7udyR+Iigei+wHR/YE2N8knuOPkXqIbxnBDqjFbggAAFqkCJRWV8PM1a1wRkQeUFwCZO+Qwk5EmDzFdeYkRnRGI7FUdYqJvko9W0um1qZnIyzHckGrMftVzBCpKC+Hn21rDaogaqSgLOPMjkPGzHGayD9W+FIGlFdBuMNDuZvk2sjdgYJgnUgvDDalGZ/RBpdDDJNlQUVIAtGK4oWagLB84vQ049R1w8jsgJ712m5D21WEm9hb50GsOLxFphuGG1CNJKJV8YUIxKksbOGMqkZaqyuR5Mie/A05ulefPuPXMSHJPTOwt1T0zAZEaFUtEdWG4IVWVwhfBKIa1jOGGmpDLZ4BjXwPHNsq9NLYrjuhr1RnocCvQYQQQN5SXJSBq4hhuSFVlkgUQYLghbdltwNkdcpg59rV8HaaaAqKA9rfKgab9rUBQtDZ1ElGjMNyQqsp1FsAG2MvqOesqkVKsFfIw0+EvgGNfAaW51dskvTzE1GUU0HkU0Lor58wQNWMMN6SqCke4sZWz54ZUUFUGHN/iCDQb3a+O7RMEdLod6DIa6DQSsIRqVycReRTDDamqyuAHVDHckIJsVcCJb4EDHwFHN7ifCdg/Euh+D9Djt/JEYJ75l8grMdyQqqxGf6AMsHPODXmSEPIcmv0fAYfWuw85BcUAPcYC3X8LtB0I6HTa1UlEqmC4IVVVGeWzFEvllzWuhLxCznFg/1q5l+by6er1fq2BnhOAPvfJZwXm/BmiFoXhhlRV5SMfQmtguKHGqiwBDn8O7P4XkPFT9XqjH9D9bqD3ffIh23r+eSNqqfivn1Rl95EnbZoqGW7oOggBnN8D7H4fOPhJ9cRgSQd0HAn0SQK63QmY/LStk4iaBIYbUpelFQDAh+GGrkVlKXBwHbD9bSDrQPX6kDig3++B+ClAYBvNyiOiponhhlQl+YUBAHytBRpXQk1a3ilgxz+BPf8GyvPldXqzfJTTTVOB2KGcGExE9WK4IVXpA+Rw42djuKEr2O3AiW/kXppfNwEQ8vrgWGDQDLmXhueiIaJrwHBDqjIFhAMALKJUHnIwWTSuiDRnqwIOrgd+XOZ+GYROicCgmfKtTq9ZeUTU/DDckKp8AkJQJHwRIJUBBZnyae6pZaoslYedfnodKMiQ15kC5Lk0g2YArTpqWx8RNVsMN6Qqfx8jzorW6C5lyFdiZrhpecouA9v/CfzyZvXJ9vxaAzc/Bgx4GPAN1rQ8Imr+GG5IVf5mAw6LcHRHBpB/RutySE2FF4C0N4Bdq4HKYnldcCww5A/yfBqjr6blEZH3YLghVQX4GJApWgMAbDnHwZkULUBRFrDt78DOVYCtQl4X3hMY+keg53iebI+IPI5/VUhVgT5GHEUcAMCWuYvhxpsVXwS2LQN2vgtYy+V1MTcDw54AOt/OSyIQkWIYbkhVOp2EUz49ACtguLgfsFYABrPWZZEnleQAP/4PsP0dwFomr2s7CLjtz/JlERhqiEhhDDekulL/OGRfDkaELV8+n0n3e7QuiTyhNE8+8umXt4CqEnlddH9gxJ+BTiMZaohINQw3pLqwADPW5wzDY4b/A757CehwG2D217osaqyyy0DacuDnFUBlkbwuqi9w2wKg8x0MNUSkOoYbUl2YvxnvWu/Eg74/wDfrAPD2CGDUC/wibG7K8oGf3wR+/kf1hSwjesvDT13H8GdJRJphuCHVhfmbkIMgfNjxVTyU+Wcg91fgg/vkIYxhT8ghR2/UukyqT3mB3EuTthyocFxGI7wnMOIZoNvdvOYTEWmO4YZUF+YvTyA+IHUG5uwAfnhVPqnbuV3Amt8BviFywGk7EIi+Sf7iNPpoXDW5Tr6X9roccACgdXdgxNNA97EMNUTUZGj+12j58uWIi4uDj48PEhISsH379nrbHjp0CBMnTkRcXBwkScKyZcvUK5Q8xhluLhVVyGejveOvwLz9wJDHAUuY/CW6fy2w4Ungnd8Af2sDLL8ZWD9TnrB68jt58iqpIz8D+OoZYGlP4Nu/ysEmrCswaSXw2E/yuWoYbIioCdG052bt2rVITk7GihUrkJCQgGXLlmHUqFFIT09HeHh4rfalpaXo0KED7r33Xvzxj3/UoGLyhOgQ+Uy0mZdLq1f6hwO3LwF+sxDI+Ak49QNwbidwYZ98iv5LR+Rl/9rq5wTFAJG9gcg+8mUcgmKAoGjAP4IXWvSE83vlMwofXA8Im7wuvCcwLNkRaLiPiahpkoQQQqs3T0hIwMCBA/HGG28AAOx2O2JiYjB37lw888wzDT43Li4O8+bNw7x5867rPQsLCxEUFISCggIEBgY2tnS6AVkF5bg5JRV6nYSjz4+GUd/A//qFAIouABf2A1mO5cL+hi/doDMAAVGApZU8xOUbAlhCHbet5N4hP+dta3nhWXJllSVymNm5Eji/u3p9+1vlyyR05CHdRKSN6/n+1uwvemVlJXbt2oX58+e71ul0OiQmJiItLc1j71NRUYGKigrX48LCQo+9NjVOeIAZPkYdyqvsOHe5DHFhfvU3liQgsI28dB1dvb4sH8g+CGQdkMNO3kmg8BxQeB6wW+UrjhdkXltBkh4IiAQCo+X3CWrreM/o6vve3BtktwGnf5BDzaHPqicJ64xAj7HALXOBNvFaVkhEdF00Czc5OTmw2WyIiIhwWx8REYGjR4967H1SUlLw3HPPeez16MbpdBLahVpwLLsYZ/JKGw439fENBuKGyktNNitQnC0HnbLL8tycssuOJU8e4irJcb8VNkcwOlf/+0l6uTcoKLqOENRWvu/XuvnMPaksAc78BBzbCBz+HCi5VL0tJA7oP12+mKV/a81KJCJqLK/vi58/fz6Sk5NdjwsLCxETE6NhRQQAsa38cCy7GKdzSnBrFw9+geoNcgAJir629nabfA2kwvNA4Vmg4Fx10Ck8Lz8uuuAIQGflpd73NjveOwYIjgGC2jluHY8Do7U7xL2qXO7lOrMNOPENkPEzYKus3u4bAnT/LdBrIhA3rPmENCKiOmgWbsLCwqDX65Gdne22Pjs7G5GRkR57H7PZDLOZ1y5qarpGBGDz4WwcPFegbSE6PRAYJS/oX3cbu03uDaoZfK68X5wlX/E676S81EXSOXp/YtxDT80QZLLc+GeyVgC5J+Qwc24ncHanfN9e5d4uKAboeJscajqM4LmFiMhraBZuTCYT+vfvj9TUVIwbNw6APKE4NTUVc+bM0aosUkl8TDAAYE9mvqZ1XBOdvnreDwbW3cZWJQedfMdcn/xMoCDDcXtWXmwV1YEo8+e6X8fSqnrYKyCyemK0yQ8wWuRbvVEefrNXyVfbLsqSXz/vJHApHbh8uvropitfu+0gOdB0/A3QqhMnBxORV9J0WCo5ORnTpk3DgAEDMGjQICxbtgwlJSWYPn06AGDq1KmIjo5GSkoKAHkS8uHDh133z507h71798Lf3x+dOnXS7HPQ9YtvFwwAOH6xGAVlVQjybea9BnqjPFclJK7u7Xa7PK+lIFM+b4wrANW4rSiU5wCV5spHhd0IUwAQ3l0+63PbAfJtSBzDDBG1CJqGm6SkJFy6dAkLFy5EVlYW4uPjsXHjRtck44yMDOhqjP2fP38e/fr1cz1+9dVX8eqrr+LWW2/F1q1b1S6fbkCYvxkdwvxwMqcE3x+7hHv6ttG6JGXpdEBAhLy0HVB3m7J8OeQUnpfn+RRlyffLLgNVpfIk4MoSuZdIbwD0Jnnxj5AnNIfEAWGd5RPsBUQyyBBRi6XpeW60wPPcNB0vbTyKN7eewKieEXjrgXq+8ImIiHB93988JII081tHb03qkYvIyC29SmsiIqJrw3BDmukeFYhhncNgtQv89cvDaGGdiEREpBCGG9LUM2O6waiXsOlwNlb/dFrrcoiIyAsw3JCmerYJwp9GdQMALPnvYXzwS4bGFRERUXPHcEOae2RYezx4SxyEAP786QE888l+FJRVXf2JREREdWC4Ic1JkoRF9/RA8u1dAABrdmRi5Gtb8c73J1FSYdW4OiIiam54KDg1KWkncrHgswM4eakEABDoY8CEm9rivgEx6NGGPy8iopbqer6/GW6oyam02vHZ3nNYsfUETuaUuNb3bBOIMb0icUfPSHQO94fEk9QREbUYDDcNYLhpPmx2gW3Hc/DRjkxsOpyFKlv1r2psKwuGdgrDkE5hGNyhFUL8TBpWSkRESmO4aQDDTfOUW1yBzYezsflwNn44noNKq921TZKAHlGBGNIpDP1jQ9AvJhjhgT4aVktERJ7GcNMAhpvmr6TCip9O5OLH4zn46UQOjmUX12rTJsgH8e2CER8TjL5tg9EzOgj+Zk0vpUZERDeA4aYBDDfe52JROdJO5CLtRC72ZOTj2MUiXPlbLUlA+zA/9I4OQu/oIHSLDETHcD9EBvpw7g4RUTPAcNMAhhvvV1xhxf6z+diXWYC9mZdx4GwBzheU19nWYtKjY2t/dGztJ9+G+6Nja3/EtrLAx6hXuXIiIqoPw00DGG5appziChw4V4ADZwtw8FwBjl8qxpncUtjsdf/66ySgTbAv2oVaEBNiQbtWFrQNkR+3C7Ug1M/EHh8iIhUx3DSA4YacKq12ZOSV4sSlYnm5WOK6X1Te8MkDLSY92oVa0DbE4gg8vohxBJ+2IRb4mtjrQ0TkSdfz/c0ZltRimQw6dAr3R6dwf7f1QghcKq5ARm4pMvLkJTOvDJmO+9lF5SittOFoVhGOZhXV+dqtA8yuXp6YEDn4OMNPRKAP9Dr2+hARKYU9N0TXqbzKhnP51WEn0xF+nPeLrnLJCJNeh7YhvugU7o9uUYHoHhmArpEBiG3lx9BDRIqx2wWsdgGbXaDKbofNVuOxzQ5bje1Wu/zY5nos6n1cV9swfxNG94ryaP3suSFSkI/ROQnZv9Y2IQQKyqrcenxcAehyKc5dLkOlzY6TOSU4mVOCTYeza7yuDl0j5KDTLTIQ3aLk21CeoJBIMULIX8ZWm/yFb7XJX/RVNvm+1W5HpbX6S9zZ1vXYVjsQVK+rbl/zsRwmGn4sP889cNQKILbqmmxX1mUXrvDifKxmV0b/2BCPh5vrwXBD5EGSJCHYYkKwxYQ+bYNrbbfa7LhQUI4zuaVIzy7C0QuFSM8uQnpWEcqr7Nh3tgD7zha4PSc8wIxuUYHoFhmAjq39ENvKD7GtLIgI8IGOPT3UhFhtdlTZBCqtdlQ6AkKlVb6tsNZ8LFBps6HSKn9hW+12VFmvDBfyl3uV40vete6Kts7wYb0ymDgCw5XPtdoEKmsEl5pnPm+pjHoJep0Eg04Hg16CXnI+lqB3e6yDzrm+xmKo435d//lTE4eliJoAm13gdG4J0rPkwHMkSw48GXml9T7HbNChXajFFXbiWsn324VaEB3iC6Nep+InILXZ7QIVVjsqrDb5tqrGfavN8fiK7TY7qpzBw3FbWSOAVFlFrXU1byttApVWG6ocIcIZYpxt6jn4sNlxfkkb9fKXvVGvg0EnwaB3BADnF7legt7x2HCVx3qd5BYi9K7Xq6tNjffQSTDoG36sv3JdnXVKMOp00Ourn2N0hJXmgkdLNYDhhpqT4gor0h1BJz2rEKdyS3EmtwRnL5fVexg7IP9xbhPs45rUXH1Ul7wEW4w8lN1D7HaBcqsNZZU2lFXZUF5lR3mVfL+s0ua67x4+7KioqnG/vjDSQPvm0ONgMuhg0utgMuhg1EuO25rr5PXyra5WoDDqq3sTTI71Bp38XOeXvLFG+Gj4udVtXM9xvp7zvr75feG3JAw3DWC4IW9QZbPjfH4ZTueWIiO3BKcdoeeM4wivihrX3qqLv9ngOHrL13UenxjHOX3ahvh6xQkMrTa7HDAcwcIZNuQAYqsRQKrDSLkzkFjrWF8jrJRXVb925VX2tRp0kjwXzGzQwWzQw2zUVd836GA2yl/gZoMeRlfgkL/0jTWChnsYqQ4kpivaGPXy6xtrBpcr2hh0EgM0eRQnFBN5OaNe5xiO8gPQ2m2b3S4fyn4mt9TtiK4Mx6Tm7MIKFFdYceRCIY5cKKzz9f3NBoT6mRDiZ0IrPxNCayx+ZgP8THpYTAb4mfUwG/TQ6wCdY1zeeet2X5JgE9XzIaps7vetV6y32gQqrO4horxm+KiqGVxsNdrYUe4IMFYNxkjMBh18jHr4GvXwNenhY9TDx6iDj0F+LAeO+gNInfev1tYg90oQUTWGGyIvo9NJiAj0QUSgDwa1D621vbzKhrOXq4/kynA7pL0UJZU2FFdYUVxhbXDOT3MhSXCFC19n2KgVQPTwNerk7Sa9W3tfoxwunO19jTWeY9LDx6Bz3Oo5nEHURDDcELUwPkY9OoUHoFN4QK1tzkPZ80oqkVdSidySSlx23OY57pdUWlFaaUNJhXxbYa0+x4VduN/K9wG7EI7JktVzIZzDGQadDkaDDsYacyacwyXO8OEMJc6AYTZWBw+fOkLJlb0lHB4halkYbojIpeah7B1aX709EVFTxIFaIiIi8ioMN0RERORVGG6IiIjIqzDcEBERkVdhuCEiIiKvwnBDREREXoXhhoiIiLwKww0RERF5FYYbIiIi8ioMN0RERORVGG6IiIjIqzDcEBERkVdhuCEiIiKvwnBDREREXsWgdQFqE0IAAAoLCzWuhIiIiK6V83vb+T3ekBYXboqKigAAMTExGldCRERE16uoqAhBQUENtpHEtUQgL2K323H+/HkEBARAkiSPvnZhYSFiYmKQmZmJwMBAj742VeN+Vgf3szq4n9XDfa0OpfazEAJFRUVo06YNdLqGZ9W0uJ4bnU6Htm3bKvoegYGB/IejAu5ndXA/q4P7WT3c1+pQYj9frcfGiROKiYiIyKsw3BAREZFXYbjxILPZjEWLFsFsNmtdilfjflYH97M6uJ/Vw32tjqawn1vchGIiIiLybuy5ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsPWb58OeLi4uDj44OEhARs375d65KalcWLF0OSJLelW7duru3l5eWYPXs2WrVqBX9/f0ycOBHZ2dlur5GRkYG77roLFosF4eHheOqpp2C1WtX+KE3K999/j3vuuQdt2rSBJEn47LPP3LYLIbBw4UJERUXB19cXiYmJ+PXXX93a5OXlYcqUKQgMDERwcDAefvhhFBcXu7XZv38/hg0bBh8fH8TExODll19W+qM1KVfbzw8++GCt3+/Ro0e7teF+vrqUlBQMHDgQAQEBCA8Px7hx45Cenu7WxlN/K7Zu3YqbbroJZrMZnTp1wurVq5X+eE3GteznESNG1PqdfvTRR93aaLqfBd2wNWvWCJPJJFauXCkOHTokZsyYIYKDg0V2drbWpTUbixYtEj179hQXLlxwLZcuXXJtf/TRR0VMTIxITU0VO3fuFDfffLO45ZZbXNutVqvo1auXSExMFHv27BEbNmwQYWFhYv78+Vp8nCZjw4YNYsGCBWL9+vUCgPj000/dtr/44osiKChIfPbZZ2Lfvn3it7/9rWjfvr0oKytztRk9erTo27ev+Pnnn8UPP/wgOnXqJCZPnuzaXlBQICIiIsSUKVPEwYMHxYcffih8fX3FW2+9pdbH1NzV9vO0adPE6NGj3X6/8/Ly3NpwP1/dqFGjxKpVq8TBgwfF3r17xZ133inatWsniouLXW088bfi5MmTwmKxiOTkZHH48GHx+uuvC71eLzZu3Kjq59XKteznW2+9VcyYMcPtd7qgoMC1Xev9zHDjAYMGDRKzZ892PbbZbKJNmzYiJSVFw6qal0WLFom+ffvWuS0/P18YjUbx8ccfu9YdOXJEABBpaWlCCPnLRafTiaysLFebN998UwQGBoqKigpFa28urvzStdvtIjIyUrzyyiuudfn5+cJsNosPP/xQCCHE4cOHBQCxY8cOV5uvvvpKSJIkzp07J4QQ4h//+IcICQlx289PP/206Nq1q8KfqGmqL9yMHTu23udwPzfOxYsXBQDx3XffCSE897fiT3/6k+jZs6fbeyUlJYlRo0Yp/ZGapCv3sxByuHn88cfrfY7W+5nDUjeosrISu3btQmJiomudTqdDYmIi0tLSNKys+fn111/Rpk0bdOjQAVOmTEFGRgYAYNeuXaiqqnLbx926dUO7du1c+zgtLQ29e/dGRESEq82oUaNQWFiIQ4cOqftBmolTp04hKyvLbb8GBQUhISHBbb8GBwdjwIABrjaJiYnQ6XT45ZdfXG2GDx8Ok8nkajNq1Cikp6fj8uXLKn2apm/r1q0IDw9H165d8dhjjyE3N9e1jfu5cQoKCgAAoaGhADz3tyItLc3tNZxtWurf9Cv3s9N//vMfhIWFoVevXpg/fz5KS0td27Tezy3uwpmelpOTA5vN5vYDBICIiAgcPXpUo6qan4SEBKxevRpdu3bFhQsX8Nxzz2HYsGE4ePAgsrKyYDKZEBwc7PaciIgIZGVlAQCysrLq/Bk4t1Ftzv1S136ruV/Dw8PdthsMBoSGhrq1ad++fa3XcG4LCQlRpP7mZPTo0ZgwYQLat2+PEydO4M9//jPGjBmDtLQ06PV67udGsNvtmDdvHoYMGYJevXoBgMf+VtTXprCwEGVlZfD19VXiIzVJde1nAPjd736H2NhYtGnTBvv378fTTz+N9PR0rF+/HoD2+5nhhpqEMWPGuO736dMHCQkJiI2NxUcffdSi/pCQd7r//vtd93v37o0+ffqgY8eO2Lp1K0aOHKlhZc3X7NmzcfDgQWzbtk3rUrxafft55syZrvu9e/dGVFQURo4ciRMnTqBjx45ql1kLh6VuUFhYGPR6fa3Z+NnZ2YiMjNSoquYvODgYXbp0wfHjxxEZGYnKykrk5+e7tam5jyMjI+v8GTi3UW3O/dLQ725kZCQuXrzott1qtSIvL4/7/gZ06NABYWFhOH78OADu5+s1Z84c/Pe//8W3336Ltm3butZ76m9FfW0CAwNb1H+26tvPdUlISAAAt99pLfczw80NMplM6N+/P1JTU13r7HY7UlNTMXjwYA0ra96Ki4tx4sQJREVFoX///jAajW77OD09HRkZGa59PHjwYBw4cMDtC2Lz5s0IDAxEjx49VK+/OWjfvj0iIyPd9mthYSF++eUXt/2an5+PXbt2udp88803sNvtrj9mgwcPxvfff4+qqipXm82bN6Nr164tbqjkWp09exa5ubmIiooCwP18rYQQmDNnDj799FN88803tYbpPPW3YvDgwW6v4WzTUv6mX20/12Xv3r0A4PY7rel+vuEpySTWrFkjzGazWL16tTh8+LCYOXOmCA4OdpslTg174oknxNatW8WpU6fEjz/+KBITE0VYWJi4ePGiEEI+vLNdu3bim2++ETt37hSDBw8WgwcPdj3fedjhHXfcIfbu3Ss2btwoWrdu3eIPBS8qKhJ79uwRe/bsEQDE0qVLxZ49e8SZM2eEEPKh4MHBweLzzz8X+/fvF2PHjq3zUPB+/fqJX375RWzbtk107tzZ7RDl/Px8ERERIR544AFx8OBBsWbNGmGxWFrUIcoN7eeioiLx5JNPirS0NHHq1CmxZcsWcdNNN4nOnTuL8vJy12twP1/dY489JoKCgsTWrVvdDkEuLS11tfHE3wrnIcpPPfWUOHLkiFi+fHmLOhT8avv5+PHjYsmSJWLnzp3i1KlT4vPPPxcdOnQQw4cPd72G1vuZ4cZDXn/9ddGuXTthMpnEoEGDxM8//6x1Sc1KUlKSiIqKEiaTSURHR4ukpCRx/Phx1/aysjIxa9YsERISIiwWixg/fry4cOGC22ucPn1ajBkzRvj6+oqwsDDxxBNPiKqqKrU/SpPy7bffCgC1lmnTpgkh5MPBn332WRERESHMZrMYOXKkSE9Pd3uN3NxcMXnyZOHv7y8CAwPF9OnTRVFRkVubffv2iaFDhwqz2Syio6PFiy++qNZHbBIa2s+lpaXijjvuEK1btxZGo1HExsaKGTNm1PrPD/fz1dW1jwGIVatWudp46m/Ft99+K+Lj44XJZBIdOnRwew9vd7X9nJGRIYYPHy5CQ0OF2WwWnTp1Ek899ZTbeW6E0HY/S44PQkREROQVOOeGiIiIvArDDREREXkVhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV2G4ISIiIq/CcENELZ4kSfjss8+0LoOIPIThhog09eCDD0KSpFrL6NGjtS6NiJopg9YFEBGNHj0aq1atcltnNps1qoaImjv23BCR5sxmMyIjI90W55WuJUnCm2++iTFjxsDX1xcdOnTAunXr3J5/4MAB/OY3v4Gvry9atWqFmTNnori42K3NypUr0bNnT5jNZkRFRWHOnDlu23NycjB+/HhYLBZ07twZX3zxhbIfmogUw3BDRE3es88+i4kTJ2Lfvn2YMmUK7r//fhw5cgQAUFJSglGjRiEkJAQ7duzAxx9/jC1btriFlzfffBOzZ8/GzJkzceDAAXzxxRfo1KmT23s899xzuO+++7B//37ceeedmDJlCvLy8lT9nETkIR65/CYRUSNNmzZN6PV64efn57a88MILQgj5CsWPPvqo23MSEhLEY489JoQQ4u233xYhISGiuLjYtf3LL78UOp3OdeXtNm3aiAULFtRbAwDxl7/8xfW4uLhYABBfffWVxz4nEamHc26ISHO33XYb3nzzTbd1oaGhrvuDBw922zZ48GDs3bsXAHDkyBH07dsXfn5+ru1DhgyB3W5Heno6JEnC+fPnMXLkyAZr6NOnj+u+n58fAgMDcfHixcZ+JCLSEMMNEWnOz8+v1jCRp/j6+l5TO6PR6PZYkiTY7XYlSiIihXHODRE1eT///HOtx927dwcAdO/eHfv27UNJSYlr+48//gidToeuXbsiICAAcXFxSE1NVbVmItIOe26ISHMVFRXIyspyW2cwGBAWFgYA+PjjjzFgwAAMHToU//nPf7B9+3a8++67AIApU6Zg0aJFmDZtGhYvXoxLly5h7ty5eOCBBxAREQEAWLx4MR599FGEh4djzJgxKCoqwo8//oi5c+eq+0GJSBUMN0SkuY0bNyIqKsptXdeuXXH06FEA8pFMa9aswaxZsxAVFYUPP/wQPXr0AABYLBZ8/fXXePzxxzFw4EBYLBZMnDgRS5cudb3WtGnTUF5ejr///e948sknERYWhkmTJqn3AYlIVZIQQmhdBBFRfSRJwqeffopx48ZpXQoRNROcc0NEREReheGGiIiIvArn3BBRk8aRcyK6Xuy5ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV2G4ISIiIq/y/wFoiPa+jjwduwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model without regularisers\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 32))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=2500, batch_size=X_train.shape[0], optimizer='Momentum', learning_rate=0.1)\n",
    "\n",
    "network.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2500 --- Train Loss: 0.6925577172075759 --- Val Loss: 0.692679502173549 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 10/2500 --- Train Loss: 0.6891334241475388 --- Val Loss: 0.6946289433637719 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 20/2500 --- Train Loss: 0.6863294521077419 --- Val Loss: 0.6990837092195274 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 30/2500 --- Train Loss: 0.6856036852060223 --- Val Loss: 0.7034773120442475 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 40/2500 --- Train Loss: 0.6856172468746095 --- Val Loss: 0.7059152715441537 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 50/2500 --- Train Loss: 0.6856529682498226 --- Val Loss: 0.7065228622532403 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 60/2500 --- Train Loss: 0.6856324264097986 --- Val Loss: 0.7062139568033573 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 70/2500 --- Train Loss: 0.68560518830509 --- Val Loss: 0.7057067760824347 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 80/2500 --- Train Loss: 0.6855908078061775 --- Val Loss: 0.7053246177698285 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 90/2500 --- Train Loss: 0.6855855743945573 --- Val Loss: 0.7051283638041929 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 100/2500 --- Train Loss: 0.6855839204477991 --- Val Loss: 0.7050693168667289 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 110/2500 --- Train Loss: 0.6855842329266697 --- Val Loss: 0.7050796495415962 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 120/2500 --- Train Loss: 0.6855845267262338 --- Val Loss: 0.7051103212896239 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 130/2500 --- Train Loss: 0.6855852390496269 --- Val Loss: 0.7051366985077558 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 140/2500 --- Train Loss: 0.6855855505506484 --- Val Loss: 0.7051514425075344 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 150/2500 --- Train Loss: 0.6855843435339626 --- Val Loss: 0.7051562486905388 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 160/2500 --- Train Loss: 0.6855848412462738 --- Val Loss: 0.7051557790548005 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 170/2500 --- Train Loss: 0.6855838214335623 --- Val Loss: 0.7051536341155806 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 180/2500 --- Train Loss: 0.6855823789133936 --- Val Loss: 0.7051510933749644 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 190/2500 --- Train Loss: 0.6855816545284371 --- Val Loss: 0.705148818590035 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 200/2500 --- Train Loss: 0.685579831309347 --- Val Loss: 0.7051471584390944 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 210/2500 --- Train Loss: 0.6855777986084128 --- Val Loss: 0.7051460909691984 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 220/2500 --- Train Loss: 0.6855774548970884 --- Val Loss: 0.7051446713227468 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 230/2500 --- Train Loss: 0.6855752682488166 --- Val Loss: 0.7051428013610013 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 240/2500 --- Train Loss: 0.6855707032731877 --- Val Loss: 0.7051405838735231 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 250/2500 --- Train Loss: 0.6855670863503612 --- Val Loss: 0.7051372442845228 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 260/2500 --- Train Loss: 0.6855612272684652 --- Val Loss: 0.7051319810024845 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 270/2500 --- Train Loss: 0.6855535478953652 --- Val Loss: 0.7051244411315584 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 280/2500 --- Train Loss: 0.6855389979419774 --- Val Loss: 0.70511554274202 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 290/2500 --- Train Loss: 0.6855249522563782 --- Val Loss: 0.7051022119361665 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 300/2500 --- Train Loss: 0.6855038304502528 --- Val Loss: 0.705081414187011 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 310/2500 --- Train Loss: 0.6854562796510769 --- Val Loss: 0.7050476259440549 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 320/2500 --- Train Loss: 0.6853852250660776 --- Val Loss: 0.7049917040271106 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 330/2500 --- Train Loss: 0.6852551195740761 --- Val Loss: 0.7048941020804997 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 340/2500 --- Train Loss: 0.6850451674878628 --- Val Loss: 0.7047077758778048 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 350/2500 --- Train Loss: 0.6845702209777501 --- Val Loss: 0.7043225030895485 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 360/2500 --- Train Loss: 0.6835936964990997 --- Val Loss: 0.7034289381471354 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 370/2500 --- Train Loss: 0.6810596951258576 --- Val Loss: 0.7009968681710667 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 380/2500 --- Train Loss: 0.671807269842611 --- Val Loss: 0.6927584090826215 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 390/2500 --- Train Loss: 0.630693337317326 --- Val Loss: 0.653096756660973 --- Train Acc: 0.68 --- Val Acc: 0.55\n",
      "Epoch 400/2500 --- Train Loss: 0.4374902337301318 --- Val Loss: 0.4653857554187719 --- Train Acc: 0.94 --- Val Acc: 0.91\n",
      "Epoch 410/2500 --- Train Loss: 0.2551528248438314 --- Val Loss: 0.2796903174158699 --- Train Acc: 0.95 --- Val Acc: 0.89\n",
      "Epoch 420/2500 --- Train Loss: 0.1421373269610035 --- Val Loss: 0.18871529181401397 --- Train Acc: 0.96 --- Val Acc: 0.91\n",
      "Epoch 430/2500 --- Train Loss: 0.10360815526473403 --- Val Loss: 0.14595872194265988 --- Train Acc: 0.98 --- Val Acc: 0.95\n",
      "Epoch 440/2500 --- Train Loss: 0.09604531833765739 --- Val Loss: 0.12372674897673286 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 450/2500 --- Train Loss: 0.0779966813562053 --- Val Loss: 0.11156419772326119 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 460/2500 --- Train Loss: 0.08118003179478507 --- Val Loss: 0.10443427074589813 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 470/2500 --- Train Loss: 0.07265917067691638 --- Val Loss: 0.09940628257140806 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 480/2500 --- Train Loss: 0.05937766295675424 --- Val Loss: 0.09402064133646319 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 490/2500 --- Train Loss: 0.07587951908351372 --- Val Loss: 0.09020110641172192 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 500/2500 --- Train Loss: 0.0675947525272003 --- Val Loss: 0.08730582857671766 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 510/2500 --- Train Loss: 0.05736416633656382 --- Val Loss: 0.08366843771464715 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 520/2500 --- Train Loss: 0.06189375534887527 --- Val Loss: 0.08049217974790424 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 530/2500 --- Train Loss: 0.0657601592226128 --- Val Loss: 0.07839059963538017 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 540/2500 --- Train Loss: 0.053482606722494205 --- Val Loss: 0.07632893858512184 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 550/2500 --- Train Loss: 0.05691139069257995 --- Val Loss: 0.07404646009517234 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 560/2500 --- Train Loss: 0.0548932552638231 --- Val Loss: 0.0708717980771834 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 570/2500 --- Train Loss: 0.055757804333436474 --- Val Loss: 0.0684604995470899 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 580/2500 --- Train Loss: 0.051178977291736226 --- Val Loss: 0.06592258629508824 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 590/2500 --- Train Loss: 0.04895154156874915 --- Val Loss: 0.06383471716048918 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 600/2500 --- Train Loss: 0.052179797311194806 --- Val Loss: 0.06306790643277418 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 610/2500 --- Train Loss: 0.05475541341863779 --- Val Loss: 0.06178927589096079 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 620/2500 --- Train Loss: 0.051157639098418965 --- Val Loss: 0.06065563100603594 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 630/2500 --- Train Loss: 0.04886240588828738 --- Val Loss: 0.05837248092345561 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 640/2500 --- Train Loss: 0.042670462423276885 --- Val Loss: 0.05695016801207144 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 650/2500 --- Train Loss: 0.05051777315154005 --- Val Loss: 0.05607991875476498 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 660/2500 --- Train Loss: 0.04654215973640705 --- Val Loss: 0.05401971418738074 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 670/2500 --- Train Loss: 0.05193024613445353 --- Val Loss: 0.05256364399373361 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 680/2500 --- Train Loss: 0.03882844337771752 --- Val Loss: 0.0511690382672796 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 690/2500 --- Train Loss: 0.04281422643025754 --- Val Loss: 0.04946291947830186 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 700/2500 --- Train Loss: 0.044285021283403135 --- Val Loss: 0.04652102351066648 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 710/2500 --- Train Loss: 0.03371315936236395 --- Val Loss: 0.043104000612120406 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 720/2500 --- Train Loss: 0.03242816581113936 --- Val Loss: 0.03991860440391339 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 730/2500 --- Train Loss: 0.03594941626271351 --- Val Loss: 0.037280022592183444 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 740/2500 --- Train Loss: 0.03721866843423039 --- Val Loss: 0.03650381077613564 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 750/2500 --- Train Loss: 0.035076548041801846 --- Val Loss: 0.03493521782205544 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 760/2500 --- Train Loss: 0.027143207124121223 --- Val Loss: 0.03312502525428773 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 770/2500 --- Train Loss: 0.031835551457307466 --- Val Loss: 0.032753393831213805 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 780/2500 --- Train Loss: 0.02822166191060708 --- Val Loss: 0.03250124336217378 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 790/2500 --- Train Loss: 0.027594398625817394 --- Val Loss: 0.033076198719669235 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 800/2500 --- Train Loss: 0.025919989061970312 --- Val Loss: 0.030308377861508338 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 810/2500 --- Train Loss: 0.022491953890893212 --- Val Loss: 0.028402894015821526 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 820/2500 --- Train Loss: 0.03184995736788751 --- Val Loss: 0.027165104569708894 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 830/2500 --- Train Loss: 0.020554860029349635 --- Val Loss: 0.025805801699749685 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 840/2500 --- Train Loss: 0.02274855899152136 --- Val Loss: 0.024264682611628945 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 850/2500 --- Train Loss: 0.020541458488808072 --- Val Loss: 0.022928929717579077 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 860/2500 --- Train Loss: 0.022598643804083468 --- Val Loss: 0.021984460059653568 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/2500 --- Train Loss: 0.017800144998972676 --- Val Loss: 0.02131863626147353 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/2500 --- Train Loss: 0.016530972146817466 --- Val Loss: 0.019990763325520012 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/2500 --- Train Loss: 0.016379196047576774 --- Val Loss: 0.019858533776825362 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/2500 --- Train Loss: 0.018396432752437943 --- Val Loss: 0.018234041461422528 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/2500 --- Train Loss: 0.017443741268115234 --- Val Loss: 0.018027131583693918 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/2500 --- Train Loss: 0.016672981377504455 --- Val Loss: 0.016487010853609256 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/2500 --- Train Loss: 0.013315420416259962 --- Val Loss: 0.01552317573126966 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/2500 --- Train Loss: 0.01340080738110059 --- Val Loss: 0.014836588152534703 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/2500 --- Train Loss: 0.014956460215127212 --- Val Loss: 0.014227214209085402 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/2500 --- Train Loss: 0.01540441043074769 --- Val Loss: 0.013786793014935886 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/2500 --- Train Loss: 0.017212557723502372 --- Val Loss: 0.0135043095136448 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/2500 --- Train Loss: 0.016028904634731757 --- Val Loss: 0.013105353265873098 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/2500 --- Train Loss: 0.010889700787190645 --- Val Loss: 0.013238579730755878 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1000/2500 --- Train Loss: 0.011515051742132822 --- Val Loss: 0.012126116902025646 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1010/2500 --- Train Loss: 0.015434307715974128 --- Val Loss: 0.011883608864946852 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1020/2500 --- Train Loss: 0.010150371682015331 --- Val Loss: 0.01203416303432844 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1030/2500 --- Train Loss: 0.0107396908018682 --- Val Loss: 0.011453837338648243 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1040/2500 --- Train Loss: 0.016745086266827254 --- Val Loss: 0.01090458563948916 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1050/2500 --- Train Loss: 0.01236587402926537 --- Val Loss: 0.010345988029719524 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1060/2500 --- Train Loss: 0.009568205819248402 --- Val Loss: 0.010231263840948782 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1070/2500 --- Train Loss: 0.011176780313181077 --- Val Loss: 0.010251517125721556 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1080/2500 --- Train Loss: 0.012627837093695991 --- Val Loss: 0.009684756109527296 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1090/2500 --- Train Loss: 0.010441080335174508 --- Val Loss: 0.009546498286292431 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1100/2500 --- Train Loss: 0.010855721872207132 --- Val Loss: 0.009687517499935022 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1110/2500 --- Train Loss: 0.01335694894486748 --- Val Loss: 0.009258334245946038 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1120/2500 --- Train Loss: 0.008766907933044904 --- Val Loss: 0.008963584908522714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1130/2500 --- Train Loss: 0.008799490903683104 --- Val Loss: 0.00868129925588892 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1140/2500 --- Train Loss: 0.007478159931783346 --- Val Loss: 0.00870886447049181 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1150/2500 --- Train Loss: 0.011807462890098045 --- Val Loss: 0.008423244731008207 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1160/2500 --- Train Loss: 0.007943315151074748 --- Val Loss: 0.00806116678388466 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1170/2500 --- Train Loss: 0.010635342408091705 --- Val Loss: 0.00784736151456827 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1180/2500 --- Train Loss: 0.008347175655010013 --- Val Loss: 0.007345949945100359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1190/2500 --- Train Loss: 0.006533802443466034 --- Val Loss: 0.0071928880610624924 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1200/2500 --- Train Loss: 0.00860859676969678 --- Val Loss: 0.007059382217916327 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1210/2500 --- Train Loss: 0.007494396558749582 --- Val Loss: 0.00685813039287664 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1220/2500 --- Train Loss: 0.008220685726598495 --- Val Loss: 0.006738904705080171 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1230/2500 --- Train Loss: 0.009085533756161532 --- Val Loss: 0.006604144835489658 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1240/2500 --- Train Loss: 0.006805342514544842 --- Val Loss: 0.006688601472990807 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1250/2500 --- Train Loss: 0.006829677505765332 --- Val Loss: 0.006529809575136468 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1260/2500 --- Train Loss: 0.005584510649765428 --- Val Loss: 0.0064930163492925 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1270/2500 --- Train Loss: 0.007058498754381977 --- Val Loss: 0.006334345658533159 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1280/2500 --- Train Loss: 0.006172114295411593 --- Val Loss: 0.006626488560288286 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1290/2500 --- Train Loss: 0.009799453410386799 --- Val Loss: 0.00611034512047772 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1300/2500 --- Train Loss: 0.006395324346742605 --- Val Loss: 0.005787519124466273 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1310/2500 --- Train Loss: 0.00909095705663671 --- Val Loss: 0.00574648340131496 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1320/2500 --- Train Loss: 0.007689761920721603 --- Val Loss: 0.005784290619909677 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1330/2500 --- Train Loss: 0.006764089524500433 --- Val Loss: 0.005738167566161426 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1340/2500 --- Train Loss: 0.007451444142958114 --- Val Loss: 0.005500014825728182 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1350/2500 --- Train Loss: 0.008863569035604205 --- Val Loss: 0.005310042635886073 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1360/2500 --- Train Loss: 0.006260278880429502 --- Val Loss: 0.005114391901152316 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1370/2500 --- Train Loss: 0.008973280698511431 --- Val Loss: 0.005167994909405628 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1380/2500 --- Train Loss: 0.006147520151490131 --- Val Loss: 0.005281805304577775 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1390/2500 --- Train Loss: 0.008827449319982772 --- Val Loss: 0.00504644513759304 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1400/2500 --- Train Loss: 0.00445116008014962 --- Val Loss: 0.004905254776606873 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1410/2500 --- Train Loss: 0.006520490115931331 --- Val Loss: 0.0048671799659000745 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1420/2500 --- Train Loss: 0.005760621962825843 --- Val Loss: 0.0048599988264965135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1430/2500 --- Train Loss: 0.006668707317267629 --- Val Loss: 0.0049088597875778265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1440/2500 --- Train Loss: 0.006188624205956204 --- Val Loss: 0.00498103941545784 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1450/2500 --- Train Loss: 0.005523871783905831 --- Val Loss: 0.004805846808733509 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1460/2500 --- Train Loss: 0.004590454749358945 --- Val Loss: 0.004589572968566204 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1470/2500 --- Train Loss: 0.0053914708927028125 --- Val Loss: 0.004482732936111933 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1480/2500 --- Train Loss: 0.00547584879858105 --- Val Loss: 0.004404062360690368 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1490/2500 --- Train Loss: 0.005649384294168263 --- Val Loss: 0.004291859893025001 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1500/2500 --- Train Loss: 0.004752838077348759 --- Val Loss: 0.00421930376595559 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1510/2500 --- Train Loss: 0.0038986791970038476 --- Val Loss: 0.004145667112535133 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1520/2500 --- Train Loss: 0.005423344856032354 --- Val Loss: 0.004008631591413477 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1530/2500 --- Train Loss: 0.00376831498749665 --- Val Loss: 0.003927255238827974 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1540/2500 --- Train Loss: 0.00605861037364188 --- Val Loss: 0.0038798503212766505 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1550/2500 --- Train Loss: 0.00585762791758987 --- Val Loss: 0.003954304433869479 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1560/2500 --- Train Loss: 0.005682993664306846 --- Val Loss: 0.003877049091919389 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1570/2500 --- Train Loss: 0.0026731056991223952 --- Val Loss: 0.003722826296815434 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1580/2500 --- Train Loss: 0.0046217424596165695 --- Val Loss: 0.003663620552429227 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1590/2500 --- Train Loss: 0.005677141562176914 --- Val Loss: 0.0035824881588614466 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1600/2500 --- Train Loss: 0.004258531851546634 --- Val Loss: 0.003530594218498366 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1610/2500 --- Train Loss: 0.00446686433454183 --- Val Loss: 0.003507197747594906 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1620/2500 --- Train Loss: 0.005625003744455006 --- Val Loss: 0.003449394223827215 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1630/2500 --- Train Loss: 0.004128891977993279 --- Val Loss: 0.0034062615619243897 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1640/2500 --- Train Loss: 0.006167761762024622 --- Val Loss: 0.0033437901415674432 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1650/2500 --- Train Loss: 0.004715602752193707 --- Val Loss: 0.0032841258153674157 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1660/2500 --- Train Loss: 0.003924785867561825 --- Val Loss: 0.0032847442421720294 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1670/2500 --- Train Loss: 0.005183046818384398 --- Val Loss: 0.0033666527561367543 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1680/2500 --- Train Loss: 0.0033922710834041804 --- Val Loss: 0.003394204461319913 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1690/2500 --- Train Loss: 0.00699111577888676 --- Val Loss: 0.003365981283333395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1700/2500 --- Train Loss: 0.004236136198295884 --- Val Loss: 0.0032925452196873343 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1710/2500 --- Train Loss: 0.004630508922737061 --- Val Loss: 0.003226832980994135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1720/2500 --- Train Loss: 0.002623361571752669 --- Val Loss: 0.003101710113523808 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1730/2500 --- Train Loss: 0.004917059409304458 --- Val Loss: 0.003058955732198172 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1740/2500 --- Train Loss: 0.0038187150292149294 --- Val Loss: 0.003197282427808356 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1750/2500 --- Train Loss: 0.005476821129083707 --- Val Loss: 0.002994189846483205 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1760/2500 --- Train Loss: 0.0044265226693081285 --- Val Loss: 0.002867404938223572 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1770/2500 --- Train Loss: 0.004598156125590851 --- Val Loss: 0.0028730604865842605 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1780/2500 --- Train Loss: 0.006006738462271136 --- Val Loss: 0.0030503235194653438 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1790/2500 --- Train Loss: 0.0032304930606482556 --- Val Loss: 0.0029250646551532145 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1800/2500 --- Train Loss: 0.004820026486423086 --- Val Loss: 0.0029395718919891386 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1810/2500 --- Train Loss: 0.005055405918283756 --- Val Loss: 0.002980220842265415 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1820/2500 --- Train Loss: 0.0030207758160488854 --- Val Loss: 0.002899418999436486 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1830/2500 --- Train Loss: 0.0037117524942759093 --- Val Loss: 0.002832946142561336 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1840/2500 --- Train Loss: 0.0037392789262088687 --- Val Loss: 0.0028218760629401254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1850/2500 --- Train Loss: 0.003794404164247366 --- Val Loss: 0.002778429008324975 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1860/2500 --- Train Loss: 0.0028232252718904606 --- Val Loss: 0.0027584577255549162 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1870/2500 --- Train Loss: 0.003969838581613724 --- Val Loss: 0.0027015428592911408 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1880/2500 --- Train Loss: 0.0034413845635821306 --- Val Loss: 0.002653706060483604 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1890/2500 --- Train Loss: 0.004200461856001796 --- Val Loss: 0.0026248550511507067 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1900/2500 --- Train Loss: 0.005120497134064653 --- Val Loss: 0.002589322365284933 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1910/2500 --- Train Loss: 0.004182331398882365 --- Val Loss: 0.0025311300059961883 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1920/2500 --- Train Loss: 0.0034156387592583107 --- Val Loss: 0.0025144140115719903 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1930/2500 --- Train Loss: 0.0024525119066902815 --- Val Loss: 0.0024662208274282946 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1940/2500 --- Train Loss: 0.004025217881669416 --- Val Loss: 0.0024668127421115056 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1950/2500 --- Train Loss: 0.0032875173089876174 --- Val Loss: 0.0025004421937992126 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1960/2500 --- Train Loss: 0.004612135712848872 --- Val Loss: 0.0025085563815625267 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1970/2500 --- Train Loss: 0.004249081904233921 --- Val Loss: 0.0025621148443868698 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1980/2500 --- Train Loss: 0.004028802071775898 --- Val Loss: 0.0025274821328804555 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1990/2500 --- Train Loss: 0.005236907919154941 --- Val Loss: 0.0025376364624345666 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2000/2500 --- Train Loss: 0.007361168452557653 --- Val Loss: 0.00246117443075788 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2010/2500 --- Train Loss: 0.0023443833203713417 --- Val Loss: 0.002417988290531425 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2020/2500 --- Train Loss: 0.002447577874489802 --- Val Loss: 0.002408017923973172 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2030/2500 --- Train Loss: 0.007648311209486878 --- Val Loss: 0.002391301338372838 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2040/2500 --- Train Loss: 0.0036875646769111113 --- Val Loss: 0.0023646842185099147 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2050/2500 --- Train Loss: 0.0033021665658419133 --- Val Loss: 0.0023464109428179966 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2060/2500 --- Train Loss: 0.004121379558683919 --- Val Loss: 0.002307568313554346 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2070/2500 --- Train Loss: 0.0038907365217764386 --- Val Loss: 0.0022481488963558985 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2080/2500 --- Train Loss: 0.0032502547955569803 --- Val Loss: 0.0022367214777964115 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2090/2500 --- Train Loss: 0.0025843273563891543 --- Val Loss: 0.0022119852584569564 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2100/2500 --- Train Loss: 0.003938545988013592 --- Val Loss: 0.002212732389146638 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2110/2500 --- Train Loss: 0.004239767105579096 --- Val Loss: 0.002205671710865539 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2120/2500 --- Train Loss: 0.004241044441720289 --- Val Loss: 0.0021998857450752246 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2130/2500 --- Train Loss: 0.00966160895091894 --- Val Loss: 0.0021669723465468256 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2140/2500 --- Train Loss: 0.00322456172876762 --- Val Loss: 0.002153661975429827 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2150/2500 --- Train Loss: 0.0028126120340996627 --- Val Loss: 0.0021478427137531886 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2160/2500 --- Train Loss: 0.002676051608573846 --- Val Loss: 0.0022528758027098995 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2170/2500 --- Train Loss: 0.003529754026916811 --- Val Loss: 0.0022775356091579435 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2180/2500 --- Train Loss: 0.003191371203987122 --- Val Loss: 0.0021110392817325664 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2190/2500 --- Train Loss: 0.0024764944555032576 --- Val Loss: 0.002051156680609694 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2200/2500 --- Train Loss: 0.003934107552448639 --- Val Loss: 0.0020175811458594847 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2210/2500 --- Train Loss: 0.0023741056092781167 --- Val Loss: 0.001998069365639531 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2220/2500 --- Train Loss: 0.0035760892762159583 --- Val Loss: 0.001975363624713395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2230/2500 --- Train Loss: 0.002789422378656887 --- Val Loss: 0.0019978208519205215 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2240/2500 --- Train Loss: 0.004331634132703507 --- Val Loss: 0.0019832375434528037 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2250/2500 --- Train Loss: 0.002633284691498197 --- Val Loss: 0.001969756884107421 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2260/2500 --- Train Loss: 0.0031389997569016416 --- Val Loss: 0.0019672897920308104 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2270/2500 --- Train Loss: 0.002637054869151932 --- Val Loss: 0.001974339015603073 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2280/2500 --- Train Loss: 0.0031566176575855342 --- Val Loss: 0.0019795312647501177 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2290/2500 --- Train Loss: 0.003345533535930763 --- Val Loss: 0.001924161392397691 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2300/2500 --- Train Loss: 0.0029374646889828424 --- Val Loss: 0.0018727624595598483 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2310/2500 --- Train Loss: 0.003250169717249075 --- Val Loss: 0.001835570729435845 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2320/2500 --- Train Loss: 0.0021898457460344492 --- Val Loss: 0.0018111468596234627 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2330/2500 --- Train Loss: 0.0026234496353101885 --- Val Loss: 0.0017740748846582339 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2340/2500 --- Train Loss: 0.0019459813894549078 --- Val Loss: 0.0017474401424379573 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2350/2500 --- Train Loss: 0.0033187722741593565 --- Val Loss: 0.00175158186123569 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2360/2500 --- Train Loss: 0.0032893709906180613 --- Val Loss: 0.0017390357757778412 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2370/2500 --- Train Loss: 0.0023568888787830976 --- Val Loss: 0.0017213461456032219 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2380/2500 --- Train Loss: 0.0022216855763914794 --- Val Loss: 0.0017074384397092865 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2390/2500 --- Train Loss: 0.0040496297804489614 --- Val Loss: 0.001761271309063095 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2400/2500 --- Train Loss: 0.0032524851458473813 --- Val Loss: 0.0018033482022370274 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2410/2500 --- Train Loss: 0.0028229482282509373 --- Val Loss: 0.0017738302120820736 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2420/2500 --- Train Loss: 0.002161376201267752 --- Val Loss: 0.0017440434848295474 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2430/2500 --- Train Loss: 0.0018855834396790782 --- Val Loss: 0.0017146725871072211 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2440/2500 --- Train Loss: 0.002268527143541802 --- Val Loss: 0.0016873283837634816 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2450/2500 --- Train Loss: 0.0021518593712970065 --- Val Loss: 0.0016826504320027918 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2460/2500 --- Train Loss: 0.0025450582758539163 --- Val Loss: 0.001724849359586847 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2470/2500 --- Train Loss: 0.0022947422293078837 --- Val Loss: 0.0017302420498946439 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2480/2500 --- Train Loss: 0.00399434931625758 --- Val Loss: 0.0017098912855572153 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2490/2500 --- Train Loss: 0.002237026806635627 --- Val Loss: 0.0016587038200702882 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjH0lEQVR4nO3dd3wUdf4G8GdmW3ojpBACIfQeWmJookRDUZFyRkQp58FPQRSjnnIqzdMoKHKnKDZA71SaoJ4gLYoKROm914SSRkgvm935/v7YZGFNQgm7O8nyvF+vNbvT9rOTmDx8y4wkhBAgIiIichGy2gUQERER2RPDDREREbkUhhsiIiJyKQw3RERE5FIYboiIiMilMNwQERGRS2G4ISIiIpfCcENEREQuheGGiIiIXArDDRFRHXfmzBlIkoS3335b7VKI6gWGG6J6aPHixZAkCTt27FC7FJdQGR5qerz55ptql0hEN0GrdgFERHXFyJEjMWjQoCrLu3TpokI1RFRbDDdEdFsoKiqCp6fnNbfp2rUrHn30USdVRESOwm4pIhe2e/duDBw4ED4+PvDy8kL//v3x+++/22xTXl6OmTNnomXLlnBzc0ODBg3Qu3dvbNiwwbpNeno6xo0bh8aNG8NgMCA0NBRDhgzBmTNnrlvDTz/9hD59+sDT0xN+fn4YMmQIDh8+bF2/YsUKSJKEX375pcq+H330ESRJwoEDB6zLjhw5ghEjRiAgIABubm7o3r07vv/+e5v9KrvtfvnlF0ycOBFBQUFo3LjxjZ62a4qIiMB9992H9evXIyoqCm5ubmjXrh1WrlxZZdtTp07hL3/5CwICAuDh4YE77rgDq1evrrJdaWkpZsyYgVatWsHNzQ2hoaEYNmwYTp48WWXbjz/+GM2bN4fBYECPHj2wfft2m/W38r0ichVsuSFyUQcPHkSfPn3g4+ODv//979DpdPjoo4/Qr18//PLLL4iJiQEAzJgxA0lJSfjb3/6G6Oho5OfnY8eOHdi1axfuueceAMDw4cNx8OBBTJ48GREREcjMzMSGDRuQmpqKiIiIGmvYuHEjBg4ciMjISMyYMQMlJSV477330KtXL+zatQsREREYPHgwvLy8sGzZMtx55502+y9duhTt27dHhw4drJ+pV69eCAsLw0svvQRPT08sW7YMDz74IL755hsMHTrUZv+JEyeiYcOGmDZtGoqKiq57zoqLi5GdnV1luZ+fH7TaK78ujx8/joSEBDzxxBMYM2YMFi1ahL/85S9Yu3at9ZxlZGSgZ8+eKC4uxtNPP40GDRrg888/xwMPPIAVK1ZYazWbzbjvvvuQnJyMhx9+GM888wwKCgqwYcMGHDhwAM2bN7e+71dffYWCggL83//9HyRJwuzZszFs2DCcOnUKOp3ulr5XRC5FEFG9s2jRIgFAbN++vcZtHnzwQaHX68XJkyetyy5cuCC8vb1F3759rcs6d+4sBg8eXONxLl++LACIOXPm3HSdUVFRIigoSFy6dMm6bO/evUKWZTF69GjrspEjR4qgoCBhMpmsyy5evChkWRazZs2yLuvfv7/o2LGjKC0ttS5TFEX07NlTtGzZ0rqs8vz07t3b5pg1OX36tABQ4yMlJcW6bdOmTQUA8c0331iX5eXlidDQUNGlSxfrsilTpggA4rfffrMuKygoEM2aNRMRERHCbDYLIYRYuHChACDmzp1bpS5FUWzqa9CggcjJybGu/+677wQA8b///U8IcWvfKyJXwm4pIhdkNpuxfv16PPjgg4iMjLQuDw0NxSOPPILNmzcjPz8fgKVV4uDBgzh+/Hi1x3J3d4der8emTZtw+fLlG67h4sWL2LNnD8aOHYuAgADr8k6dOuGee+7BmjVrrMsSEhKQmZmJTZs2WZetWLECiqIgISEBAJCTk4OffvoJDz30EAoKCpCdnY3s7GxcunQJ8fHxOH78OM6fP29Tw/jx46HRaG645gkTJmDDhg1VHu3atbPZrlGjRjatRD4+Phg9ejR2796N9PR0AMCaNWsQHR2N3r17W7fz8vLChAkTcObMGRw6dAgA8M033yAwMBCTJ0+uUo8kSTavExIS4O/vb33dp08fAJbuL6D23ysiV8NwQ+SCsrKyUFxcjNatW1dZ17ZtWyiKgrS0NADArFmzkJubi1atWqFjx4544YUXsG/fPuv2BoMBb731Fn788UcEBwejb9++mD17tvWPeE3Onj0LADXWkJ2dbe0qGjBgAHx9fbF06VLrNkuXLkVUVBRatWoFADhx4gSEEHj11VfRsGFDm8f06dMBAJmZmTbv06xZs+ueq6u1bNkScXFxVR4+Pj4227Vo0aJK8Kiss3Jsy9mzZ2v87JXrAeDkyZNo3bq1TbdXTZo0aWLzujLoVAaZ2n6viFwNww3Rba5v3744efIkFi5ciA4dOuDTTz9F165d8emnn1q3mTJlCo4dO4akpCS4ubnh1VdfRdu2bbF792671GAwGPDggw9i1apVMJlMOH/+PLZs2WJttQEARVEAAM8//3y1rSsbNmxAixYtbI7r7u5ul/rqippaoYQQ1ueO/l4R1QcMN0QuqGHDhvDw8MDRo0errDty5AhkWUZ4eLh1WUBAAMaNG4evv/4aaWlp6NSpE2bMmGGzX/PmzfHcc89h/fr1OHDgAIxGI955550aa2jatCkA1FhDYGCgzdTshIQEZGdnIzk5GcuXL4cQwibcVHav6XS6altX4uLi4O3tfWMn6BZVtiJd7dixYwBgHbTbtGnTGj975XrAcl6PHj2K8vJyu9V3s98rIlfDcEPkgjQaDe6991589913NlOAMzIy8NVXX6F3797WrpZLly7Z7Ovl5YUWLVqgrKwMgGUGUWlpqc02zZs3h7e3t3Wb6oSGhiIqKgqff/45cnNzrcsPHDiA9evXV7lYXlxcHAICArB06VIsXboU0dHRNt1KQUFB6NevHz766CNcvHixyvtlZWVd+6TY0YULF7Bq1Srr6/z8fHzxxReIiopCSEgIAGDQoEHYtm0bUlJSrNsVFRXh448/RkREhHUcz/Dhw5GdnY3333+/yvv8OUBdT22/V0SuhlPBieqxhQsXYu3atVWWP/PMM/jnP/+JDRs2oHfv3pg4cSK0Wi0++ugjlJWVYfbs2dZt27Vrh379+qFbt24ICAjAjh07sGLFCjz11FMALC0S/fv3x0MPPYR27dpBq9Vi1apVyMjIwMMPP3zN+ubMmYOBAwciNjYWjz/+uHUquK+vb5WWIZ1Oh2HDhmHJkiUoKiqq9j5K8+fPR+/evdGxY0eMHz8ekZGRyMjIQEpKCs6dO4e9e/fW4ixesWvXLvz3v/+tsrx58+aIjY21vm7VqhUef/xxbN++HcHBwVi4cCEyMjKwaNEi6zYvvfQSvv76awwcOBBPP/00AgIC8Pnnn+P06dP45ptvIMuWf1uOHj0aX3zxBRITE7Ft2zb06dMHRUVF2LhxIyZOnIghQ4bccP238r0icimqztUiolqpnOpc0yMtLU0IIcSuXbtEfHy88PLyEh4eHuKuu+4SW7dutTnWP//5TxEdHS38/PyEu7u7aNOmjXj99deF0WgUQgiRnZ0tJk2aJNq0aSM8PT2Fr6+viImJEcuWLbuhWjdu3Ch69eol3N3dhY+Pj7j//vvFoUOHqt12w4YNAoCQJMn6Gf7s5MmTYvTo0SIkJETodDoRFhYm7rvvPrFixYoq5+daU+Wvdr2p4GPGjLFu27RpUzF48GCxbt060alTJ2EwGESbNm3E8uXLq611xIgRws/PT7i5uYno6Gjxww8/VNmuuLhYvPzyy6JZs2ZCp9OJkJAQMWLECOs0/sr6qpviDUBMnz5dCHHr3ysiVyEJcZPtnkREt7GIiAh06NABP/zwg9qlEFENOOaGiIiIXArDDREREbkUhhsiIiJyKRxzQ0RERC6FLTdERETkUhhuiIiIyKXcdhfxUxQFFy5cgLe3d5Ub3xEREVHdJIRAQUEBGjVqZL0IZk1uu3Bz4cIFm3vqEBERUf2RlpaGxo0bX3Ob2y7cVN5YLy0tzXpvHSIiIqrb8vPzER4efkM3yL3twk1lV5SPjw/DDRERUT1zI0NKOKCYiIiIXArDDREREbkUhhsiIiJyKbfdmBsiInItZrMZ5eXlapdBdqDX6687zftGMNwQEVG9JIRAeno6cnNz1S6F7ESWZTRr1gx6vf6WjsNwQ0RE9VJlsAkKCoKHhwcvzFrPVV5k9+LFi2jSpMktfT8ZboiIqN4xm83WYNOgQQO1yyE7adiwIS5cuACTyQSdTlfr43BAMRER1TuVY2w8PDxUroTsqbI7ymw239JxGG6IiKjeYleUa7HX95PhhoiIiFwKww0REVE9FxERgXnz5qldRp3BcENEROQkkiRd8zFjxoxaHXf79u2YMGHCLdXWr18/TJky5ZaOUVdwtpQzmU1A8SVAowPcfAFZo3ZFRETkRBcvXrQ+X7p0KaZNm4ajR49al3l5eVmfCyFgNpuh1V7/T3XDhg3tW2g9VyfCzfz58zFnzhykp6ejc+fOeO+99xAdHV3ttv369cMvv/xSZfmgQYOwevVqR5daO5mHgZ/fAI5vAEwllmWSDHgGAT6hgE8Y4B0KeAVZgo+stTzw54FV4k8vxa2tr3YbWEKXpKn4KlseNss0gCxbvuq9AI8AwKOB5XNo6sSPFBFRnRQSEmJ97uvrC0mSrMs2bdqEu+66C2vWrMErr7yC/fv3Y/369QgPD0diYiJ+//13FBUVoW3btkhKSkJcXJz1WBEREZgyZYq15UWSJHzyySdYvXo11q1bh7CwMLzzzjt44IEHal37N998g2nTpuHEiRMIDQ3F5MmT8dxzz1nXf/DBB3j33XeRlpYGX19f9OnTBytWrAAArFixAjNnzsSJEyfg4eGBLl264LvvvoOnp2et67kW1f8SLV26FImJiViwYAFiYmIwb948xMfH4+jRowgKCqqy/cqVK2E0Gq2vL126hM6dO+Mvf/mLM8u+cftXAN8+CZgra5YACEAoQGG65XFht5oV2o/WHWgUBXQcAUQ9Cujc1K6IiG4jQgiUlN/aFOLactdp7DbT56WXXsLbb7+NyMhI+Pv7Iy0tDYMGDcLrr78Og8GAL774Avfffz+OHj2KJk2a1HicmTNnYvbs2ZgzZw7ee+89jBo1CmfPnkVAQMBN17Rz50489NBDmDFjBhISErB161ZMnDgRDRo0wNixY7Fjxw48/fTT+M9//oOePXsiJycHv/32GwBLa9XIkSMxe/ZsDB06FAUFBfjtt98gqvvHtZ2oHm7mzp2L8ePHY9y4cQCABQsWYPXq1Vi4cCFeeumlKtv/+ZuyZMkSeHh41M1wc2wdsHK8Jci0iAP6TwOCO1heF18CCi4C+ReBgguWr0VZgGIGFBOg1HSflD/9z1Plf6ZbXC8EIMyWOqxfq1umWL4aC4DiHKAo29IqlZpieWz7FHj0G8A37AZOFBHRrSspN6PdtHWqvPehWfHw0NvnT+qsWbNwzz33WF8HBASgc+fO1tevvfYaVq1ahe+//x5PPfVUjccZO3YsRo4cCQB444038O9//xvbtm3DgAEDbrqmuXPnon///nj11VcBAK1atcKhQ4cwZ84cjB07FqmpqfD09MR9990Hb29vNG3aFF26dAFgCTcmkwnDhg1D06ZNAQAdO3a86Rpuhqrhxmg0YufOnZg6dap1mSzLiIuLQ0pKyg0d47PPPsPDDz9cY9NWWVkZysrKrK/z8/NvregblX4AWPFXSwiIGgU88L6lKwcAoAG8QyyPRl2cU4+jKQqQc9LS9bb5XSDrMPDlCGDCJkBrULs6IqJ6o3v37javCwsLMWPGDKxevdoaFEpKSpCamnrN43Tq1Mn63NPTEz4+PsjMzKxVTYcPH8aQIUNslvXq1Qvz5s2D2WzGPffcg6ZNmyIyMhIDBgzAgAEDMHToUHh4eKBz587o378/OnbsiPj4eNx7770YMWIE/P39a1XLjVA13GRnZ8NsNiM4ONhmeXBwMI4cOXLd/bdt24YDBw7gs88+q3GbpKQkzJw585ZrvSnFOcCSRwBjIdCsL3D/v64KNi5KloHAlpZH2/uBT+4CMg8B2z8FYiepXR0R3QbcdRocmhWv2nvby5//sf78889jw4YNePvtt9GiRQu4u7tjxIgRNkM0qvPn2xdIkgRFUexW59W8vb2xa9cubNq0CevXr8e0adMwY8YMbN++HX5+ftiwYQO2bt2K9evX47333sPLL7+MP/74A82aNXNIPfX6L+5nn32Gjh071jj4GACmTp2KvLw86yMtLc2xRZUVAksfA3LPAv4RwF8+twwSvp34hQN3vWx5vu0TS6sOEZGDSZIED71WlYcjr5S8ZcsWjB07FkOHDkXHjh0REhKCM2fOOOz9qtO2bVts2bKlSl2tWrWCRmMJdlqtFnFxcZg9ezb27duHM2fO4KeffgJg+d706tULM2fOxO7du6HX67Fq1SqH1atqy01gYCA0Gg0yMjJslmdkZNiMKK9OUVERlixZglmzZl1zO4PBAIPB8d0i4vJZbFg2H32K1sM9/7RlFlHCl5aZRLejTg8B6/4BXD5tacEJ6aB2RURE9VLLli2xcuVK3H///ZAkCa+++qrDWmCysrKwZ88em2WhoaF47rnn0KNHD7z22mtISEhASkoK3n//fXzwwQcAgB9++AGnTp1C37594e/vjzVr1kBRFLRu3Rp//PEHkpOTce+99yIoKAh//PEHsrKy0LZtW4d8BkDllhu9Xo9u3bohOTnZukxRFCQnJyM2Nvaa+y5fvhxlZWV49NFHHV3mDUnZvg33XvwI7vmnUaJvAIz+7vb+g673BJr2tDw/XXXqPhER3Zi5c+fC398fPXv2xP3334/4+Hh07drVIe/11VdfoUuXLjaPTz75BF27dsWyZcuwZMkSdOjQAdOmTcOsWbMwduxYAICfnx9WrlyJu+++G23btsWCBQvw9ddfo3379vDx8cGvv/6KQYMGoVWrVnjllVfwzjvvYODAgQ75DAAgCUfOxboBS5cuxZgxY/DRRx8hOjoa8+bNw7Jly3DkyBEEBwdj9OjRCAsLQ1JSks1+ffr0QVhYGJYsWXJT75efnw9fX1/k5eXBx8fHbp/jcl4+Uj99FMtzIvGduRcS7++Ocb0c05dYb2x+F9g4A2j3IPDQ52pXQ0QupLS0FKdPn0azZs3g5sbLTriKa31fb+bvt+pTwRMSEpCVlYVp06YhPT0dUVFRWLt2rXWQcWpqKuQ/DcY9evQoNm/ejPXr16tRcrX8fX3gl/gdVv94BAW/nsLM/x3CgfP5GN41DEE+btBpHNMfK1W50N+f1tfibSUJ0MgSNLIErSxf9fzK1xvqXw5qb/mafezmiyAiIqol1VtunM1RLTeVhBD4d/IJvLvRtf+gyxKglWV4u2nh76lHkLcB7UJ90LWpP/q3DYJBqwEunwX+1QnQ6IF/XOTVi4nIbthy45pcpuXG1UiShGfiWqJPq0As2nIG+87l4nKREYoDIuT1cum11l5rV0UIKELArIga61YEYDQruFRkxKUiI05kFmLryUvA5tMI8jZgxgPtMah9uOWqxaYSy+yxBs2v/6GIiIhuEcONg3Rt4o+uTRx3gSJnURQBc0XQMSmWr5bnCkxmgfzScuQUGXEupwQHLuRh3cF0ZOSXYeKXu/DakPZ4zDcMuHTCcjVmhhsiInIChhu6JlmWIENCTdenagR3y5PmwEMIx8uD22LKkj348UA6Xl9zGAlNg6DHCaAg3XlFExHRba1eX8SP6h6DVoP5j3RF21AflJYrOFXqZVnBcENERE7CcEN2J8sSJt/dAgCw85LesrCQ4YaIiJyD4YYc4u42QXDXaXDW6G1ZwJYbIiJyEoYbcgg3nQa9WgQiW/haFhRfUrcgIiK6bTDckMO0C/VGPirubluSq2otRESupF+/fpgyZYraZdRZDDfkMG1CfZAnKsJNaZ66xRAR1QH3338/BgwYUO263377DZIkYd++fbf8PosXL4afn98tH6e+Yrghh2kd4o18eAAABMMNEREef/xxbNiwAefOnauybtGiRejevTs6deqkQmWuheGGHKaxv/tVLTe5174sMhHRbeC+++5Dw4YNsXjxYpvlhYWFWL58OR5//HFcunQJI0eORFhYGDw8PNCxY0d8/fXXdq0jNTUVQ4YMgZeXF3x8fPDQQw8hIyPDun7v3r2466674O3tDR8fH3Tr1g07duwAAJw9exb3338//P394enpifbt22PNmjV2re9W8SJ+5DAGrQY6T3/ADEiKCSgvBvSeapdFRK5KCMvvGTXoPG7oTsVarRajR4/G4sWL8fLLL1tvQrx8+XKYzWaMHDkShYWF6NatG1588UX4+Phg9erVeOyxx9C8eXNER0ffcqmKoliDzS+//AKTyYRJkyYhISEBmzZtAgCMGjUKXbp0wYcffgiNRoM9e/ZAp9MBACZNmgSj0Yhff/0Vnp6eOHToELy8vG65LntiuCGH8vf1hXJJgiwJwFjEcENEjlNeDLzRSJ33/seFG/799te//hVz5szBL7/8gn79+gGwdEkNHz4cvr6+8PX1xfPPP2/dfvLkyVi3bh2WLVtml3CTnJyM/fv34/Tp0wgPDwcAfPHFF2jfvj22b9+OHj16IDU1FS+88ALatGkDAGjZsqV1/9TUVAwfPhwdO3YEAERGRt5yTfbGbilyqPAGXihBxYX8jIXqFkNEVAe0adMGPXv2xMKFCwEAJ06cwG+//YbHH38cAGA2m/Haa6+hY8eOCAgIgJeXF9atW4fU1FS7vP/hw4cRHh5uDTYA0K5dO/j5+eHw4cMAgMTERPztb39DXFwc3nzzTZw8edK67dNPP41//vOf6NWrF6ZPn26XAdD2xpYbcqhGfm4ohgGeKAOMKjUXE9HtQedhaUFR671vwuOPP47Jkydj/vz5WLRoEZo3b44777wTADBnzhz861//wrx589CxY0d4enpiypQpMBqNjqi8WjNmzMAjjzyC1atX48cff8T06dOxZMkSDB06FH/7298QHx+P1atXY/369UhKSsI777yDyZMnO62+62HLDTmUn4cexcLN8kKtvnAiuj1IkqVrSI3HDYy3udpDDz0EWZbx1Vdf4YsvvsBf//pX6/ibLVu2YMiQIXj00UfRuXNnREZG4tixY3Y7TW3btkVaWhrS0tKsyw4dOoTc3Fy0a9fOuqxVq1Z49tlnsX79egwbNgyLFi2yrgsPD8cTTzyBlStX4rnnnsMnn3xit/rsgS035FABnnoUw2B5wW4pIiIAgJeXFxISEjB16lTk5+dj7Nix1nUtW7bEihUrsHXrVvj7+2Pu3LnIyMiwCR43wmw2Y8+ePTbLDAYD4uLi0LFjR4waNQrz5s2DyWTCxIkTceedd6J79+4oKSnBCy+8gBEjRqBZs2Y4d+4ctm/fjuHDhwMApkyZgoEDB6JVq1a4fPkyfv75Z7Rt2/ZWT4ldMdyQQ/l76FBiDTdsuSEiqvT444/js88+w6BBg9Co0ZWB0K+88gpOnTqF+Ph4eHh4YMKECXjwwQeRl3dz1wsrLCxEly5dbJY1b94cJ06cwHfffYfJkyejb9++kGUZAwYMwHvvvQcA0Gg0uHTpEkaPHo2MjAwEBgZi2LBhmDlzJgBLaJo0aRLOnTsHHx8fDBgwAO++++4tng37koS4vS4+kp+fD19fX+Tl5cHHx0ftclze76cuoXzRA+ijOQAM+wTo9JDaJRGRCygtLcXp06fRrFkzuLm5qV0O2cm1vq838/ebY27IobwM2qtabtgtRUREjsdwQw7lrtegtHIquKlM3WKIiOi2wHBDDuWu08AIy1UtGW6IiMgZGG7Iodx0GhiFZdy6Ul6qcjVERHQ7YLghh3LXaVBW0XJjYrghIju7zebEuDx7fT8ZbsihDFrZGm7MRoYbIrKPyps4FhfzEhOupPIqzBqN5paOw+vckEPJsgRFrgg35RxzQ0T2odFo4Ofnh8zMTACAh4eH9Qq/VD8pioKsrCx4eHhAq721eMJwQw6naAyAAMzGErVLISIXEhISAgDWgEP1nyzLaNKkyS0HVYYbcjhF1gNmQGHLDRHZkSRJCA0NRVBQEMrLy9Uuh+xAr9dDlm99xAzDDTmexgCYAcGp4ETkABqN5pbHaJBr4YBicjihsVzET2G4ISIiJ2C4IYeTtBW3X+BUcCIicgKGG3I4URFuhNmociVERHQ7YLghh5O1lju7SuyWIiIiJ2C4IYeTdZaWG4ktN0RE5AQMN+RwkjXcsOWGiIgcj+GGHE6js3RLyQpbboiIyPFUDzfz589HREQE3NzcEBMTg23btl1z+9zcXEyaNAmhoaEwGAxo1aoV1qxZ46RqqTZkhhsiInIiVS/it3TpUiQmJmLBggWIiYnBvHnzEB8fj6NHjyIoKKjK9kajEffccw+CgoKwYsUKhIWF4ezZs/Dz83N+8XTDdHqGGyIich5Vw83cuXMxfvx4jBs3DgCwYMECrF69GgsXLsRLL71UZfuFCxciJycHW7dutd4RNiIiwpklUy1oK8KNluGGiIicQLVuKaPRiJ07dyIuLu5KMbKMuLg4pKSkVLvP999/j9jYWEyaNAnBwcHo0KED3njjDZjNZmeVTbWg1VmuUCwLk8qVEBHR7UC1lpvs7GyYzWYEBwfbLA8ODsaRI0eq3efUqVP46aefMGrUKKxZswYnTpzAxIkTUV5ejunTp1e7T1lZGcrKrszSyc/Pt9+HoBuiN1hmS2kYboiIyAlUH1B8MxRFQVBQED7++GN069YNCQkJePnll7FgwYIa90lKSoKvr6/1ER4e7sSKCQB0FVPBNVAAIVSuhoiIXJ1q4SYwMBAajQYZGRk2yzMyMhASElLtPqGhoWjVqpXN3V/btm2L9PR0GI3Vj+eYOnUq8vLyrI+0tDT7fQi6IYaKlhsAgMLWGyIicizVwo1er0e3bt2QnJxsXaYoCpKTkxEbG1vtPr169cKJEyegKIp12bFjxxAaGgq9Xl/tPgaDAT4+PjYPci6dVnflhblcvUKIiOi2oGq3VGJiIj755BN8/vnnOHz4MJ588kkUFRVZZ0+NHj0aU6dOtW7/5JNPIicnB8888wyOHTuG1atX44033sCkSZPU+gh0A2TdVeFGYbghIiLHUnUqeEJCArKysjBt2jSkp6cjKioKa9eutQ4yTk1NhSxfyV/h4eFYt24dnn32WXTq1AlhYWF45pln8OKLL6r1EegGaHVXd0txZhsRETmWJMTtNcIzPz8fvr6+yMvLYxeVk/xx6hJ6fN4csiSA544B3sHX34mIiOgqN/P3u17NlqL6SauRUY6KQeDsliIiIgdjuCGH02tkmKzhhrOliIjIsRhuyOG0GgnmynBjZrghIiLHYrghh9NpJHZLERGR0zDckMNpZXZLERGR8zDckMNpNdKVcMOL+BERkYMx3JDD6TQyTIItN0RE5BwMN+RwuqtmSykmttwQEZFjMdyQw13dLWUyVX+DUyIiInthuCGH0101oNjMlhsiInIwhhtyOO1VU8HN5Wy5ISIix2K4IYfTylcu4seWGyIicjSGG3I4SboSbhQzW26IiMixGG7IKcySFgBnSxERkeMx3JBTmCV2SxERkXMw3JBTKKhoueEViomIyMEYbsgprnRLccwNERE5FsMNOYUiVQ4o5u0XiIjIsRhuyCmUipYbwTE3RETkYAw35BSV4YZjboiIyNEYbsgphGzplhIMN0RE5GAMN+QUiqQDAAiOuSEiIgdjuCGnUGR2SxERkXMw3JBzVMyWgsKWGyIiciyGG3IKUTlbymxWuRIiInJ1DDfkFJUDiqGwW4qIiByL4YacwjpbSmHLDRERORbDDTlFZbcUOKCYiIgcjOGGnKNithQEBxQTEZFjMdyQU4iKcCPxOjdERORgDDfkHNaWG465ISIix2K4IeeoDDe8zg0RETkYww05h3UqOFtuiIjIsRhuyDkqx9yw5YaIiByM4Yacg+GGiIichOGGnELSVIQbTgUnIiIHY7gh55B1AACJs6WIiMjB6kS4mT9/PiIiIuDm5oaYmBhs27atxm0XL14MSZJsHm5ubk6slmpF5l3BiYjIOVQPN0uXLkViYiKmT5+OXbt2oXPnzoiPj0dmZmaN+/j4+ODixYvWx9mzZ51YMdWGpLG03MhsuSEiIgdTPdzMnTsX48ePx7hx49CuXTssWLAAHh4eWLhwYY37SJKEkJAQ6yM4ONiJFVNtSDLH3BARkXOoGm6MRiN27tyJuLg46zJZlhEXF4eUlJQa9yssLETTpk0RHh6OIUOG4ODBgzVuW1ZWhvz8fJsHOZ91QDGvc0NERA6marjJzs6G2Wyu0vISHByM9PT0avdp3bo1Fi5ciO+++w7//e9/oSgKevbsiXPnzlW7fVJSEnx9fa2P8PBwu38Ouj65ItzIbLkhIiIHU71b6mbFxsZi9OjRiIqKwp133omVK1eiYcOG+Oijj6rdfurUqcjLy7M+0tLSnFwxAbBe54ZjboiIyNG0ar55YGAgNBoNMjIybJZnZGQgJCTkho6h0+nQpUsXnDhxotr1BoMBBoPhlmulWyNbr3PDcENERI6lasuNXq9Ht27dkJycbF2mKAqSk5MRGxt7Q8cwm83Yv38/QkNDHVUm2YGk5WwpIiJyDlVbbgAgMTERY8aMQffu3REdHY158+ahqKgI48aNAwCMHj0aYWFhSEpKAgDMmjULd9xxB1q0aIHc3FzMmTMHZ8+exd/+9jc1PwZdB8fcEBGRs6gebhISEpCVlYVp06YhPT0dUVFRWLt2rXWQcWpqKmT5SgPT5cuXMX78eKSnp8Pf3x/dunXD1q1b0a5dO7U+At0AqeIKxTLYckNERI4lCSGE2kU4U35+Pnx9fZGXlwcfHx+1y7lt/PTrJtz90xDkyb7wnZaqdjlERFTP3Mzf73o3W4rqp8orFGs45oaIiByM4YaconLMjYbdUkRE5GAMN+QUMu8tRURETsJwQ04hV0wFZ8sNERE5GsMNOYWmItxoYQZurzHsRETkZAw35BSVY24AAEJRrxAiInJ5DDfkFJUtNwAAhRfyIyIix2G4IaeQGW6IiMhJGG7IKbRXd0uZy9UrhIiIXB7DDTmFRqu/8kLhjCkiInIchhtyCq1WA7OQLC/YLUVERA7EcENOoZElmKCxvGC4ISIiB2K4IafQyTLMDDdEROQEDDfkFBqNBFPljxvDDRERORDDDTmFVpauarnhgGIiInIchhtyCq18peVGmI0qV0NERK6M4YacQnvVmBuzid1SRETkOAw35BSWMTcV4YYX8SMiIgdiuCGn0MoSTIItN0RE5HgMN+QUlgHFlh83xcSWGyIichyGG3KKqy/ix24pIiJyJIYbcgpJujIVXDGzW4qIiByH4YacRpEqx9xwKjgRETkOww05DcfcEBGRMzDckNOYJS0AdksREZFjMdyQ0ygcc0NERE7AcENOY5Yqww27pYiIyHEYbshp2HJDRETOwHBDTlM5W0ow3BARkQMx3JDTiMpuKc6WIiIiB2K4IaexttwoDDdEROQ4DDfkNGZJB4DdUkRE5FgMN+Q0gmNuiIjICRhuyGkUmeGGiIgcj+GGnMc65obhhoiIHIfhhpxGqbj9AltuiIjIkRhuyGlERbcUOFuKiIgcqE6Em/nz5yMiIgJubm6IiYnBtm3bbmi/JUuWQJIkPPjgg44tkOxCVLbcsFuKiIgcSPVws3TpUiQmJmL69OnYtWsXOnfujPj4eGRmZl5zvzNnzuD5559Hnz59nFQp3SohW8IN2C1FREQOpHq4mTt3LsaPH49x48ahXbt2WLBgATw8PLBw4cIa9zGbzRg1ahRmzpyJyMhIJ1ZLt0JwQDERETmBquHGaDRi586diIuLsy6TZRlxcXFISUmpcb9Zs2YhKCgIjz/++HXfo6ysDPn5+TYPUklFy43EcENERA6karjJzs6G2WxGcHCwzfLg4GCkp6dXu8/mzZvx2Wef4ZNPPrmh90hKSoKvr6/1ER4efst1U+1UdksJxaxyJURE5MpU75a6GQUFBXjsscfwySefIDAw8Ib2mTp1KvLy8qyPtLQ0B1dJNbLOlmLLDREROY5WzTcPDAyERqNBRkaGzfKMjAyEhIRU2f7kyZM4c+YM7r//fusyRVEAAFqtFkePHkXz5s1t9jEYDDAYDA6onm6aVNktxangRETkOKq23Oj1enTr1g3JycnWZYqiIDk5GbGxsVW2b9OmDfbv3489e/ZYHw888ADuuusu7Nmzh11OdZ2mIkuzW4qIiBxI1ZYbAEhMTMSYMWPQvXt3REdHY968eSgqKsK4ceMAAKNHj0ZYWBiSkpLg5uaGDh062Ozv5+cHAFWWU90jOKCYiIicQPVwk5CQgKysLEybNg3p6emIiorC2rVrrYOMU1NTIcv1amgQ1UCqvM6NYLghIiLHUT3cAMBTTz2Fp556qtp1mzZtuua+ixcvtn9B5BjWlht2SxERkeOwSYSch+GGiIicgOGGnEauGFAssVuKiIgciOGGnEdmuCEiIserVbhJS0vDuXPnrK+3bduGKVOm4OOPP7ZbYeR6JA27pYiIyPFqFW4eeeQR/PzzzwCA9PR03HPPPdi2bRtefvllzJo1y64FkguRdZYvbLkhIiIHqlW4OXDgAKKjowEAy5YtQ4cOHbB161Z8+eWXnL1ENbK23Ai23BARkePUKtyUl5dbb2mwceNGPPDAAwAsVxC+ePGi/aojlyLJDDdEROR4tQo37du3x4IFC/Dbb79hw4YNGDBgAADgwoULaNCggV0LJNchaS3hRma4ISIiB6pVuHnrrbfw0UcfoV+/fhg5ciQ6d+4MAPj++++t3VVEfybLleGGY26IiMhxanWF4n79+iE7Oxv5+fnw9/e3Lp8wYQI8PDzsVhy5lsoxNww3RETkSLVquSkpKUFZWZk12Jw9exbz5s3D0aNHERQUZNcCyXXIGstsKQ27pYiIyIFqFW6GDBmCL774AgCQm5uLmJgYvPPOO3jwwQfx4Ycf2rVAch2ytnIqOMMNERE5Tq3Cza5du9CnTx8AwIoVKxAcHIyzZ8/iiy++wL///W+7Fkiuo7LlRgbDDREROU6twk1xcTG8vb0BAOvXr8ewYcMgyzLuuOMOnD171q4Fkuuwhhu23BARkQPVKty0aNEC3377LdLS0rBu3Trce++9AIDMzEz4+PjYtUByHXLFVHANW26IiMiBahVupk2bhueffx4RERGIjo5GbGwsAEsrTpcuXexaILkOjVYPANBythQRETlQraaCjxgxAr1798bFixet17gBgP79+2Po0KF2K45ci0Znuaq1Fgw3RETkOLUKNwAQEhKCkJAQ693BGzduzAv40TXJ1nBjBhQFkGvVcEhERHRNtfrroigKZs2aBV9fXzRt2hRNmzaFn58fXnvtNSiKYu8ayUVodforL8xG9QohIiKXVquWm5dffhmfffYZ3nzzTfTq1QsAsHnzZsyYMQOlpaV4/fXX7VokuQaNzu3KC7MRuPo1ERGRndQq3Hz++ef49NNPrXcDB4BOnTohLCwMEydOZLihaun0bLkhIiLHq1W3VE5ODtq0aVNleZs2bZCTk3PLRZFr0mq0KBcaywuGGyIicpBahZvOnTvj/fffr7L8/fffR6dOnW65KHJNWo0EY2VjIcMNERE5SK26pWbPno3Bgwdj48aN1mvcpKSkIC0tDWvWrLFrgeQ6dBoZ5dACKAPM5WqXQ0RELqpWLTd33nknjh07hqFDhyI3Nxe5ubkYNmwYDh48iP/85z/2rpFcxJVwAyjlpSpXQ0RErkoSQgh7HWzv3r3o2rUrzOa6e3n9/Px8+Pr6Ii8vj7eKcLL80nIUJLVGmHQJxr8mQ9+ku9olERFRPXEzf795FTVyGp0swygsLTfmco65ISIix2C4IafRaiRrt5TZyG4pIiJyDIYbchqtfCXcmExlKldDRESu6qZmSw0bNuya63Nzc2+lFnJxkiShHDoAgMJuKSIicpCbCje+vr7XXT969OhbKohcm0mqnC3FlhsiInKMmwo3ixYtclQddJswSZaWGzO7pYiIyEE45oacit1SRETkaAw35FTmym4pE2dLERGRYzDckFOZK7qlhIktN0RE5BgMN+RUJpnhhoiIHKtOhJv58+cjIiICbm5uiImJwbZt22rcduXKlejevTv8/Pzg6emJqKgo3s+qHqlsuVEYboiIyEFUDzdLly5FYmIipk+fjl27dqFz586Ij49HZmZmtdsHBATg5ZdfRkpKCvbt24dx48Zh3LhxWLdunZMrp9pQKlpuwNlSRETkIKqHm7lz52L8+PEYN24c2rVrhwULFsDDwwMLFy6sdvt+/fph6NChaNu2LZo3b45nnnkGnTp1wubNm51cOdWGUjnmxsyWGyIicgxVw43RaMTOnTsRFxdnXSbLMuLi4pCSknLd/YUQSE5OxtGjR9G3b19Hlkp2Yq4cc2MuV7kSIiJyVTd1ET97y87OhtlsRnBwsM3y4OBgHDlypMb98vLyEBYWhrKyMmg0GnzwwQe45557qt22rKwMZWVXukDy8/PtUzzVSmW3lMRuKSIichBVw01teXt7Y8+ePSgsLERycjISExMRGRmJfv36Vdk2KSkJM2fOdH6RVC0hs1uKiIgcS9VwExgYCI1Gg4yMDJvlGRkZCAkJqXE/WZbRokULAEBUVBQOHz6MpKSkasPN1KlTkZiYaH2dn5+P8PBw+3wAumlmWV/xhN1SRETkGKqOudHr9ejWrRuSk5OtyxRFQXJyMmJjY2/4OIqi2HQ9Xc1gMMDHx8fmQeoRFeFGMrNbioiIHEP1bqnExESMGTMG3bt3R3R0NObNm4eioiKMGzcOADB69GiEhYUhKSkJgKWbqXv37mjevDnKysqwZs0a/Oc//8GHH36o5segGyQ0FWNu2C1FREQOonq4SUhIQFZWFqZNm4b09HRERUVh7dq11kHGqampkOUrDUxFRUWYOHEizp07B3d3d7Rp0wb//e9/kZCQoNZHoJugaNwAADJbboiIyEEkIYRQuwhnys/Ph6+vL/Ly8thFpYIvPnkHo8/Pwnm/HgibslHtcoiIqJ64mb/fql/Ej24zGgMAQFbYckNERI7BcENOJevcLV/ZLUVERA7CcENOpTV4AABkU6nKlRARkatiuCGn0rpZBhRr2C1FREQOwnBDTqWvaLnRKJwKTkREjsFwQ86ls7Tc6ARbboiIyDEYbsipJG1FuGHLDREROQjDDTmV0FpmS+lQDiiKytUQEZErYrghp9JUdEsBADgdnIiIHIDhhpxLZ7jynNPBiYjIARhuyKk0Wj1MouLHrpzhhoiI7I/hhpxKK8sog+XO4Gy5ISIiR2C4IafSaCSUQm95YeKYGyIisj+GG3IqrSxd1XJTom4xRETkkhhuyKm0sowyURlu2HJDRET2x3BDTqXXyle6pcrZckNERPbHcENO5WnQXNUtxZYbIiKyP4YbcipPvRZl1gHFnC1FRET2x3BDTuVp0FrH3CjsliIiIgdguCGnMmhla8tNeRnDDRER2R/DDTmVZUCxpeXGbCxWuRoiInJFDDfkVJbr3FhabhQjx9wQEZH9MdyQU0mSBJNkCTdm3luKiIgcgOGGnK5crmy54ZgbIiKyP4YbcjqTZAAACM6WIiIiB2C4Iacrl90AAIIDiomIyAEYbsjpjBr3iidF6hZCREQuieGGnK68ItxI5Wy5ISIi+2O4Iacrlz0AAFI5W26IiMj+GG7I6UxattwQEZHjMNyQ05k0lpYbmS03RETkAAw35HRKRcuNxsSWGyIisj+GG3I6RecJANCYeZ0bIiKyP4YbcjpFZ+mW0rLlhoiIHIDhhpxOaCvCjVIGKGaVqyEiIlfDcENOJ/SeV17wQn5ERGRnDDfkdLLODSZR8aPH6eBERGRndSLczJ8/HxEREXBzc0NMTAy2bdtW47affPIJ+vTpA39/f/j7+yMuLu6a21Pdo9NqUAzLzTPZckNERPamerhZunQpEhMTMX36dOzatQudO3dGfHw8MjMzq91+06ZNGDlyJH7++WekpKQgPDwc9957L86fP+/kyqm29FoZxbDcPBPGQnWLISIil6N6uJk7dy7Gjx+PcePGoV27dliwYAE8PDywcOHCarf/8ssvMXHiRERFRaFNmzb49NNPoSgKkpOTnVw51ZZBq0GxqGy5YbcUERHZl6rhxmg0YufOnYiLi7Muk2UZcXFxSElJuaFjFBcXo7y8HAEBAdWuLysrQ35+vs2D1GVgyw0RETmQquEmOzsbZrMZwcHBNsuDg4ORnp5+Q8d48cUX0ahRI5uAdLWkpCT4+vpaH+Hh4bdcN90ag1ZGgbBMB0dpnrrFEBGRy1G9W+pWvPnmm1iyZAlWrVoFNze3areZOnUq8vLyrI+0tDQnV0l/ZtBpkIeK6eAMN0REZGdaNd88MDAQGo0GGRkZNsszMjIQEhJyzX3ffvttvPnmm9i4cSM6depU43YGgwEGg8Eu9ZJ9GLQy8q0tN7mq1kJERK5H1ZYbvV6Pbt262QwGrhwcHBsbW+N+s2fPxmuvvYa1a9eie/fuziiV7MiglZEPdksREZFjqNpyAwCJiYkYM2YMunfvjujoaMybNw9FRUUYN24cAGD06NEICwtDUlISAOCtt97CtGnT8NVXXyEiIsI6NsfLywteXl6qfQ66cQatjDzBbikiInIM1cNNQkICsrKyMG3aNKSnpyMqKgpr1661DjJOTU2FLF9pYPrwww9hNBoxYsQIm+NMnz4dM2bMcGbpVEsGrQb5lWNuSnJVrYWIiFyP6uEGAJ566ik89dRT1a7btGmTzeszZ844viByKLbcEBGRI9Xr2VJUPxl0V4+5yVW1FiIicj0MN+R0Bq3mqtlSbLkhIiL7YrghpzNoZeShYvA3ww0REdkZww05nU3LTUkuIISq9RARkWthuCGnM+hk5Fa23AgzW2+IiMiuGG7I6fQaGWXQo0C4WxYUZatbEBERuRSGG3I6g87yY5ctfCwLirJUrIaIiFwNww05nV5j+bG7BF/LgqJMFashIiJXw3BDTqetDDdsuSEiIgdguCFVaGXpqnDDMTdERGQ/DDekivgOIciq7JYqZLcUERHZD8MNqcJNq2G3FBEROQTDDalCr5VwSVQOKGa3FBER2Q/DDalCK8u4BLbcEBGR/THckCq0GglZlS03BenqFkNERC6F4YZUodPIuCgaWF4YC3gLBiIishuGG1KFVpZQDDcUayq6pvLOq1sQERG5DIYbUkXlhfzy9MGWBXnnVKyGiIhcCcMNqUInSwCAy9ogy4K8NBWrISIiV8JwQ6qobLnZftnDsoAtN0REZCcMN6SKtMvFAIALItCyIDdVxWqIiMiVMNyQKgxay4/eKRFqWXDpuIrVEBGRK2G4IVVM6BsJADgpGlkWZB8HFEXFioiIyFUw3JAq/D30AIBUEQQha4HyYiCf08GJiOjWMdyQKjQVs6VM0ELxt7TiIPuYihUREZGrYLghVWgkyfrcHNDS8oThhoiI7IDhhlQhyxIq8015g1aWJxkH1CuIiIhcBsMNqUZb0TVlbNjJsuDCHvWKISIil8FwQ6opNwsAwNsHPC0LMg8DxmIVKyIiIlfAcEOq+/KwCfAKAYQZuLhH7XKIiKieY7ihuiGil+XrqU2qlkFERPUfww3VDc3vtnw9kaxuHUREVO8x3FDdEHmX5euFXUBxjrq1EBFRvcZwQ3WDbxgQ0hEQCnDoO7WrISKieozhhlTzv6d6W5+XlpuBjg9ZXuxbqlJFRETkChhuSDUNvQ3W5wu3nAY6jgAgAakpQM4p9QojIqJ6jeGGVKPTXLkFQ3peKeDTCGjR37Ig5QOVqiIiovpO9XAzf/58REREwM3NDTExMdi2bVuN2x48eBDDhw9HREQEJEnCvHnznFco2Z1Oe+XHr6jMbHnS6xnL193/AXJOq1AVERHVd6qGm6VLlyIxMRHTp0/Hrl270LlzZ8THxyMzM7Pa7YuLixEZGYk333wTISEhTq6W7E0nXx1uTJYnEX2Apr0BUymwfCyvWExERDdN1XAzd+5cjB8/HuPGjUO7du2wYMECeHh4YOHChdVu36NHD8yZMwcPP/wwDAZDtdtQ/XF1t9Tag+lQFAFIEjB0AeAeYLla8Zrn1SuQiIjqJdXCjdFoxM6dOxEXF3elGFlGXFwcUlJS7PY+ZWVlyM/Pt3lQ3aCRJZvX8fN+hcmsAH7hwEOfA5IM7PkSOLBSpQqJiKg+Ui3cZGdnw2w2Izg42GZ5cHAw0tPT7fY+SUlJ8PX1tT7Cw8Ptdmy6NZIkYdaQ9tbXxzMLcSyj0PKiWV+gd6Ll+f+mALlpzi+QiIjqJdUHFDva1KlTkZeXZ32kpfGPZF3y2B1NbV4XlJZfedHvJSCsO1CWB6ycAChmJ1dHRET1kWrhJjAwEBqNBhkZGTbLMzIy7DpY2GAwwMfHx+ZBdYck2XZN5ZeaIISwDDDW6IDhnwB6LyB1K/DTaypVSURE9Ylq4Uav16Nbt25ITr5yo0RFUZCcnIzY2Fi1yiKVzfj+ILq+tgHtp6/D01/vBgIigcHvWFZufhf4+Q1ACHWLJCKiOk3VbqnExER88skn+Pzzz3H48GE8+eSTKCoqwrhx4wAAo0ePxtSpU63bG41G7NmzB3v27IHRaMT58+exZ88enDhxQq2PQHZ2PrcEl4stXVPf771g6abq/DBwzyzLBr+8BaxOBMzl1zgKERHdzrRqvnlCQgKysrIwbdo0pKenIyoqCmvXrrUOMk5NTYV81bVQLly4gC5dulhfv/3223j77bdx5513YtOmTc4un+zknnbB2HAoo9p1R9IL0CMiwHJxP50HsOYFYMdCIOuYZUaVZ6CTqyUiorpOEuL2auPPz8+Hr68v8vLyOP6mjliyLRUvrdxf4/ozbw6+8uLIasvgYmMh4NcUGLvaMnWciIhc2s38/Xb52VJU9/1pTHEVjy/ejv3n8iwv2gwG/pYM+EcAuWeBz+4BLux2eI1ERFR/MNyQ6v48Y+rPko9k4v73NyMtp+JWDEFtgLFrgIZtgIKLwMKBwKHvnFApERHVBww3pLrBHUMR5ud+3e36zP4Zhy/mY9Xuc9hx2R3ir+uA5v0BUwmwbDSw+D7g8P8As8kJVRMRUV3FMTdUJyiKQEm5Ge2nr7vhfWY+0B5jYhoDG6cDfywAlIpQ0zgaGPEZ4NfEQdUSEZGzccwN1TuyLMHToEUDT/0N7/PJb6cAjRaIfx14Zq/ldg0GH+DcNuD9HsDGGUB5qeOKJiKiOonhhuqUe9oFX3+jCnrNVT++vo2BuOnAhE1A016AqdRy0b/P7gFyTtu/UCIiqrMYbqje0mmq+fFt0NwyPfzhrwCPBkD6PuDjO4FD3zu/QCIiUgXDDdUpBu2N/0jqtDXMspIky5Tx//sVaNwDKM0Dlj0GLH0MyDtvp0qJiKiuYrihOuWpu1uieUNPvDigzXW3FQL4Yd8FPLQgBREvrUbES6ux8VAGYt7YiOU70ixdVWPXAL2fBSQZOPw98GGsZdr47TWOnojotsLZUlRnRby0+pb23z/jXnjqtZBlCeUX9kHzwxTIF3ZaVga1s7TqtIgD2twHyMz5RER12c38/Vb13lJEjtRxxnr0bxOET8d0R///XkJZ6fPY2vMPaLZ9BGQesjx2fQ74NgEiegGRdwEdhgEandqlExHRLeA/V8mlJR/JxOYT2UjNKUZGsUBq178Dzx1G3r3zkBI8EoreG8hLBfZ+DayaAHxwh2Xw8e3VoElE5FLYLUV11pP/3YmTWYVw12mwt+LeUjtficODH2xBWk5JrY/bu0UgCstM2JOWiyaeCn4d6Q6c3QrsXAwUX7JsFNYNaD8MaNoTCO0MyBo7fCIiIqotdkuRS/jw0W4QQiC/xIRfj2fhnnbBcNNpUG66tTy++US29XlqkYxll1sjU2qKSU8/g/3L/4m2pxZDd34ncL5ifI6bLxD1KHD3K4De45bem4iIHI8tN1TvDP73bzh4IR8AMHVgGyT9eMSuxw9EHpb3TEWzwj3A2S1AmeW9hHcozNFPQBvaAYjoC2hv/GrKRER0a3j7BXJp/3q4CwCgb6uGGNMzwu7Hz4YvzrV9HHhkCY6P3YcJxmdxTgRCKrgIbfJ04L/DIf7dBdixCDCVAQDySsqRU2S0ey1ERHTz2HJD9d4vx7KgCAFZkjBm4Ta7HDPYx4DVT/dB939uBADoUY6HNT+hj3wAUfIJNJQsY4BMniGQB81G5H8sPbxHXhsANx3H5xAR2dvN/P1muCGX8v3eC3j6690AgC//FgM3nQbDP9xq1/cwwIhRmmRM0P6AEOkyAOAD0wOYY3oIPz9/Ny4XG+HjrkPzhl52fV8iotsZw801MNy4tnKzgtd+OISezRtgQIdQAMD53BKsO5CODzadRHZhmd3eSwcTXtAuxQSt5WKD35p74tumL2PTCUurzohujfHW8E7QyDXcJoKIiG4Yw801MNzcvsyKwL82HsO/fzph1+MOlX/DbN3H0Elm/GbugCfKn0UR3AEA/x7ZBQ90bmTX9yMiuh1xQDFRNTSyhMR7W+Pn5/tZl42ObYpeLRrc0nFXKX3wePnzKBIG9NEcwDL9LDSWMgEAT3+9G//9/SwOVczuOnA+D+sOpt/S+xER0bWx5YZuSxn5pdh8PBv3dQ6FQaup9j5WknRzFyruKJ3CQv1sNJTyUSDc8bppFJaa+0FU/Bti+ROx+MuCFADAD5N7o0OYL4QQEAKQ2XVFRHRN7Ja6BoYbqs43O8/hs82n0TbUB9/sOoeFY7sjKtwfi7eewc9HMrH/fN4NHScUl/Bv/XvoIR8DAFwWXjgqwrHJ3BlfmO9FMdwAAP96OApDosLwf//ZgSPpBVg3pa91lpUQApLEsENEdDWGm2tguKHrMZkVaDVXemyfW7YX3+w6Z7ONh16DZ+Na4fU1h6vsL0PBOM2PeFb7DbykUuvycyIQs8sfxhERjlaRkWgS3gQfbDoBHxRh/iNd0adTS3y7+zymf38Q743sgr6tGjruQxIR1TMMN9fAcEM3a/+5PNz//mbc3SYIPx2xjKUZ2zMC0+9vh2ZT19S4nwdK0Uy6iCj5JJ7Q/A/hcpbN+sNKE/hLBQiRLsMsJJyJfASDDt+DMliufPzoHU0wKqYp2oby55SIiOHmGhhuqDZyiozwc9fh3Y3H8PW2NPxvci+E+rpjxvcH8e2e81g/pS9W7j6PfycfR7HRXGV/d5TiGe0qPKjZDC3MCEABZKnq/3rHlTBMKZ+EgyLCuuzMm4Md+dGIiOoFhptrYLihW/XnMTFmRVivZZNZUIro15MBAN5uWqye3Ad95/xc5RiByMNdmt0wCQ02KN3QTT6OObqPECTlQhES9olIJJu7YKW5D7a8ORaKIjB5yW6E+rjhlfvaOeeDEhHVIQw318BwQ462/mA6pn9/EPMSohAT2aDamVjV8Uc+3tB9hoGa7TbLt5rb4RtzX/yoRKMYbjj1xiBIEjjomIhuKww318BwQ8723Z7zyCooQ2zzBhj8780AgEa+briQV1rt9o2lTMTKh/CgvAW9NAety/OFB5aa++FnJQrezbrjvXF3Qa+9fS9VZTQpt/XnJ7rdMNxcA8MNqUkIgbyScngZtGjx8o8AgA9GdcXEL3dVu30YsjBUsxkjNL8iQs6wLjcLCQdEM0iBrRAU8xdcCukNLy9vrNmfjiYBHhjcyXLrieMZBfhs82k8dXcLNPb3QInRjHJFgY+bzvEf1oG2nMjGqE//wLT72uGvvZupXQ4ROQHDzTUw3FBdcTq7CCazgpbB3tftupKgoJ+8FyM0v6C9dNYm6ABAmdDhnAhELrywztwdI/9vKs6VeeCxzyx3Se8Q5oP/PdUbA//1G85eKsaWl+6Gv4cOBy/kI7KhJzz0Wny46SQaehswoltjh31me+nx+kZkFVjuE3bmzcFYuesc5m44hk9Gd+fsMiIXxXBzDQw3VBct256GV749gP+7MxJdm/hj6sr9SM+vvtsKAEJwCd3k44iST2CgZhsaS9k260uFDqdFCNxgxEpzH3xuvhdazwDkFBmrHKt9Ix+0DvHGyl3nAVjCwoXcEvx8NBPDuzbG01/vRttQHzx7TyvrPkaTgv3nc9G5sZ/NNYGcpfs/N1pvgnrmzcHWcNi+kQ9WP93H6fUQkeMx3FwDww3VVVePIckpMuJoegEKy0y4IzIA3m469J39M1JziqvZUyBCSkcQctFCvoCHNT+hk3zaZotyocFh0QT7lEicEo1wRgTjrAhGmgiCETV3UXUM87VenXl0bFOM7xOJ8AAPTF25H19vS8Uz/VvahJ4boSgCR9IL0DzIEwatpsr6gxfyUG4WiAr3q/EY3f+5AdmFlqB2dbhp3tATyc/1u6l6iKh+uJm/31on1URE13H14NgATz1im9ve0HP9s31RWm7Gf38/ixU7z8FoUioGJUs4I0JxBqHYZm6Lr8x3o6N0GoFSHhpKuRinWYe2cio6SaerhB6zkFAEd2QJX6SKIBTBDY2lbJwRwfjN3AnbL7SGDzxRBj2+SjmJr1JOomVoAA5ftNwI9F/Jx+Gh12DV7vMoKDXhr72bYceZHHRq7IdHYpogLacYb609gtGxEbhcZMS07w+gtFwBANzbLhgfj+4OADiSng+dRkZaTjHGLrLMFvvybzGIjWxQ7X23lBr+SSbbYQbZz0cysWR7Kt4Y2hENvAy3fLz6LLuwDP4eeuulDojqC7bcENVziiIQ+Y/qr5Tcp2UgfjuehTBko4t8Au3lM2giZSBCykBTKcPm9hA3Kk1piBOiEU6IMBwXYcgSfiiHFiZoYBIy0kQQMhBwQ8eafn87hPq644n/7qx2/QvxrZFVUIZGfm6Y0Lc5Jn65E2v2295Vfder96DraxsAAK2DvbHmmT7QyBLMisC8jcewet9FzHigPXq1CERaTjHyS8sR4uOGIB836zEuFxmxfGcaekQEYOgHWwEAD/cIx5vDO930+XEVe9NyMWT+FsS1DcKnY3qoXU6dNnvtEaTnleKdhzrzEg0OxG6pa2C4IVd0PKMA5y6XoF/rhjieWYimDTxgVgTctBqknLqEFTvPQStLeO7e1kjNKUaPCH+s3X8Ri9f/gb92D8Dn639HEykT3ijGRdEAbeWz6CkfQnvpNPRS1SsuX0+hcEMuvFAmdDBCi3JoYYQOpUKHfHiiQHigAO4oEB4ohR5myMgQ/kgVQciBN3KFNwrgDsCxfyg6h/vh2biW1taiP3vizuYoM5khBNCliR+GRIXh0IV8fPLbKbjpZGhlGbOGtIckScgrLsdfP9+O+zqFYlyvG5/BJYTAhP/sxJYT2Vj/bF809veocdt3NxxDgKceY3pG3PDxc4qMOHwxHz2bN6jyh/fqC1D+sO8CmgZ4omNjXwBA4tI9WLn7yjisqzlzGn5WQRm+23MeI7o1hp+H3inveTOEENbbsOg1MvbNuNd6E9y6ICO/FBsPZ2BolzB46Ot3Z029Czfz58/HnDlzkJ6ejs6dO+O9995DdHR0jdsvX74cr776Ks6cOYOWLVvirbfewqBBg27ovRhuiKr618bjeHfjMbwyuC0e6NwIH/16CqG+bvjn6kPQwgwDyqGFGXqY0Ey6iBbyBbSQzqOFdB6+UhH0MEELM3QwIVzKhKaaW0vcLKPQIBu+uCR8UAw3lAo9SmBACfQoEXoYoYMJGpRDi3JoUC60MEELIzQwQwMFEhTIMMAIL5QiHQFIEw2RJfxQKNxRBh1MkKFAhqliHxNkCNz6H+3uTf3Ru2Ugtp/JwZYTl6qsf/svnbH1RLY1PFQyaGV0buyHvw9oDaNZwdH0AhzLKMCa/ekwmRUUVdzaY2R0EwzvGoZnl+3BtPvao3+bIJSZFAz9YAsCPPXYdjoH8R1CMP+Rrrhn7i84nlkIwHZM0udbz2D69wfx6n3t0DrYG49+9odl+V+jcSqrEEu3p+FIegEA23Bz6EI+Bv37N4ztGYF/DGqLDzedRO+WDRDq647PU85gVHRThAe4w6wIfJ5yFpGBnrirTRAASxAwKQLlZgW/n7qEXi0CreOuvt97Ad/uPo9XBrdFZEMvHLqQj33ncvHSyv0AgHvaBeOdhzrju93nMahj6A13GeaVlOPg+Tz4eegxeuEfeHlwW/RrFQR/T0tQem7ZXhxJz8fKiT1h0Gqq3Dj3ekrLzWjz6lrr69eHdsComKbW10VlJngabjxU/PkK6Leq/zubcDKrCGN7RmDGA+3tdlzAEnJ1GslprVX1KtwsXboUo0ePxoIFCxATE4N58+Zh+fLlOHr0KIKCgqpsv3XrVvTt2xdJSUm477778NVXX+Gtt97Crl270KFDh+u+H8MNUVVCCGQXGtHQ2/YPRmZBKXacuYwB7UOsY19OZRXixW/2YfuZy1WOE98+GF1DdGisK8DJs2m4s4UvNu5PRUFRCTw1ZihlhfAUJTAWXYaHKIIPimGQyqGBGY2lbIRKlxCAArhLVWd1OUu50KAMlhanMuhRJnTW16aKIGUWGkugqmiVMkOCBgIaKBVxSYGABDM0MFZuJypbsLTW/cqhhVFobY5lhBYmoalx21LoUYaKmoTOUg80FQFNtn51dKvXzere1B87zlb9mQnxcbvmzMCaeBm06Nm8AXKLy7HtTA5kCXg4ugm++iPVus297YKx/lBGtfsv+79YtG/kg/bT11nrKzWZceB8Pv7aqxnuatMQb/54BKeyitC+kQ92nL2MEd0ao0/LQLjpNGgV7I0vfz+L6GYBmPAf227VhWO745VVB6wX6pz/SFf0bxuEOeuO4rPNV8a9RUcEQK+V8W5CFNbsv4i2oT54fvledAzzxer9F2HQyvjt73fhtdWHcV+nUHi7aeHjpoOvuw6hvm5YdzADrYK9cCS9AIoQiGsbjE1Hs9CvdUN46DWQJMnmMhM/PtMHBy/kY8uJbHgZtJj5QHvIsoScIiNMZgUCQFpOMbpHWLqVS8vN0GtklJkUPP75dvRp2RB+Hjr0iPCHh16Le9/9FYM7huLFgW1w19ub8GBUI8wc0gFbT2QjJrKB3cdq1atwExMTgx49euD9998HACiKgvDwcEyePBkvvfRSle0TEhJQVFSEH374wbrsjjvuQFRUFBYsWHDd92O4IbKfyosS1qa7QAiBlJOXkF1kRJifGz7cdBJDuzSGLAHPfpmCABQgSMrFpBh/XLx0GQX5+QjxEGgVoEGIh4L1+9KghQm9Inyw8eB56GGCDiboJFNFu40lbJigQYFwR6iUg3ApCwFSPrxQUqvutvrELCQISBVnQq5oybK0ZglIEAAEJMgVzyqXmmEJbpWtYiZR+fxKiCqHBqaKUFZJQKo4KireDZAgrO+pVCw1V7yT2VrTlXUKZCii6rrKfa7UX1kxKj6fVPFfWNdWvq6sqXJ/WI8jWfcVkKAI2fb1VV8rz6Hl/apup1z1roq4sp1tnX9eXvV9YD3ulXN69fdKXBVYBSQIcfU6WNdf/bW6dVde266rzpX9/vza8lWp5nMIyPD19sLKvw+tdkZkbdWb2VJGoxE7d+7E1KlTrctkWUZcXBxSUlKq3SclJQWJiYk2y+Lj4/Htt986slQiqoYkSbUeByFJEnq2CLS+/nTMlUHIA98cdt39R101DGQsgILScpQYzfBx10ErW35pm8wCRpOCk9mFiGrsh9yScvi4aaGRJZzPLUFhSRkaemrgqRMwm8zIyitEbkERWgbocOLiJXQI0kMxlcFUVoJ9ZzJxMScfZ7Pz0cRPj8Y+WmReLkCwl4yzWflo5O+FrKJyRDcPxvncUny97Sw0UDCgTQCOXbyM/MIidA3zRLMAPTYdOg9zuaVjrHmAHumXCyzBrOJxdVCzvDZDj3LoUQ43qbziuQkGGGvsArQsr1x3C0GubjUAUT1xQmoHg3aEau+varjJzs6G2WxGcHCwzfLg4GAcOXKk2n3S09Or3T49Pb3a7cvKylBWVmZ9nZ+ff4tVE1Fd5O2mg/efbiuh0wDueg26NvEHYJliXynM3wP40+Ddpj4BqBwt0SmkBQBAhuUXZfQ1LufT7U+vgwF0HXLldb8/ra9puPHVA3XNiuXfxmUmBe5623/9HryQh8tF5WgS4AGDFjDICvwMMqCYkJ1fDC1M8HPXotRYDjetDAjlTw9h+QqBnBIzjArQwNMAnUYGFDOglAPmcuxPy8aZzDyUlJRiRJcQpF3Kh2Iqh59Bgr+b5R+oAKDXSMjIL4GPux4GrQaXispRZhZo6G1A8uEMNPLRQ4aCC5eL0K2JL7ILSrA3NQduGgktGrojsoE7jmXko6GnFrlFZZa2EaEgwEMLCDMKS8ohw4yLeSUI8tIjv7gM28/kQCcJNPDSo2VDDzTwMuB4VjEa+bnjSHohGnq7wWgW0GhknM8tRWm5Cd2b+MFdJ2H9gYuQINDAQ4vc4jLIEAj3d4csKQjy0qPUWI70vBL4umlRWGqEj0FGA0890nOLEe7vhvS8YuQUlsJTr4HZbIZWFgj21uP85WLIoqLtQgL83DVQhIDZrEAvAyZFQbifG4rKypGZXwIJAj4GDcrKzTArZmvbjJtORlm52dq2opEtLZ0eOhklRhM89Rq462QUlJpgMluC65W2JNvnACBJAnqNhHKzUvM2ENW24FzdjnRl2ZV1lW1asvW5pW2qaZB/DT/lzlG/h07fgKSkJMycOVPtMoiIruvqGUiV4xX+HGwAoH0j3xqPEeh2pbnercatrrjWpP2OoUDHq143bWG7/uo2u6v/ydnwqueDulx53umq9W3/9F5RFV/DqqmjcvRlxFXLulxju+vNVXv8OutrUvnxm9aw/kYvZxkAILyWNfz5OFQ9VW+pGxgYCI1Gg4wM2wFfGRkZCAkJqXafkJCQm9p+6tSpyMvLsz7S0tLsUzwRERHVSaqGG71ej27duiE5Odm6TFEUJCcnIzY2ttp9YmNjbbYHgA0bNtS4vcFggI+Pj82DiIiIXJfq3VKJiYkYM2YMunfvjujoaMybNw9FRUUYN24cAGD06NEICwtDUlISAOCZZ57BnXfeiXfeeQeDBw/GkiVLsGPHDnz88cdqfgwiIiKqI1QPNwkJCcjKysK0adOQnp6OqKgorF271jpoODU1FbJ8pYGpZ8+e+Oqrr/DKK6/gH//4B1q2bIlvv/32hq5xQ0RERK5P9evcOBuvc0NERFT/3Mzfb1XH3BARERHZG8MNERERuRSGGyIiInIpDDdERETkUhhuiIiIyKUw3BAREZFLYbghIiIil8JwQ0RERC6F4YaIiIhciuq3X3C2ygsy5+fnq1wJERER3ajKv9s3cmOF2y7cFBQUAADCw8NVroSIiIhuVkFBAXx9fa+5zW13bylFUXDhwgV4e3tDkiS7Hjs/Px/h4eFIS0vjfasciOfZOXienYPn2Xl4rp3DUedZCIGCggI0atTI5oba1bntWm5kWUbjxo0d+h4+Pj78H8cJeJ6dg+fZOXienYfn2jkccZ6v12JTiQOKiYiIyKUw3BAREZFLYbixI4PBgOnTp8NgMKhdikvjeXYOnmfn4Hl2Hp5r56gL5/m2G1BMREREro0tN0RERORSGG6IiIjIpTDcEBERkUthuCEiIiKXwnBjJ/Pnz0dERATc3NwQExODbdu2qV1SvTJjxgxIkmTzaNOmjXV9aWkpJk2ahAYNGsDLywvDhw9HRkaGzTFSU1MxePBgeHh4ICgoCC+88AJMJpOzP0qd8uuvv+L+++9Ho0aNIEkSvv32W5v1QghMmzYNoaGhcHd3R1xcHI4fP26zTU5ODkaNGgUfHx/4+fnh8ccfR2Fhoc02+/btQ58+feDm5obw8HDMnj3b0R+tTrneeR47dmyVn+8BAwbYbMPzfH1JSUno0aMHvL29ERQUhAcffBBHjx612cZevys2bdqErl27wmAwoEWLFli8eLGjP16dcSPnuV+/flV+pp944gmbbVQ9z4Ju2ZIlS4RerxcLFy4UBw8eFOPHjxd+fn4iIyND7dLqjenTp4v27duLixcvWh9ZWVnW9U888YQIDw8XycnJYseOHeKOO+4QPXv2tK43mUyiQ4cOIi4uTuzevVusWbNGBAYGiqlTp6rxceqMNWvWiJdfflmsXLlSABCrVq2yWf/mm28KX19f8e2334q9e/eKBx54QDRr1kyUlJRYtxkwYIDo3Lmz+P3338Vvv/0mWrRoIUaOHGldn5eXJ4KDg8WoUaPEgQMHxNdffy3c3d3FRx995KyPqbrrnecxY8aIAQMG2Px85+Tk2GzD83x98fHxYtGiReLAgQNiz549YtCgQaJJkyaisLDQuo09flecOnVKeHh4iMTERHHo0CHx3nvvCY1GI9auXevUz6uWGznPd955pxg/frzNz3ReXp51vdrnmeHGDqKjo8WkSZOsr81ms2jUqJFISkpSsar6Zfr06aJz587VrsvNzRU6nU4sX77cuuzw4cMCgEhJSRFCWP64yLIs0tPTrdt8+OGHwsfHR5SVlTm09vriz390FUURISEhYs6cOdZlubm5wmAwiK+//loIIcShQ4cEALF9+3brNj/++KOQJEmcP39eCCHEBx98IPz9/W3O84svvihat27t4E9UN9UUboYMGVLjPjzPtZOZmSkAiF9++UUIYb/fFX//+99F+/btbd4rISFBxMfHO/oj1Ul/Ps9CWMLNM888U+M+ap9ndkvdIqPRiJ07dyIuLs66TJZlxMXFISUlRcXK6p/jx4+jUaNGiIyMxKhRo5CamgoA2LlzJ8rLy23OcZs2bdCkSRPrOU5JSUHHjh0RHBxs3SY+Ph75+fk4ePCgcz9IPXH69Gmkp6fbnFdfX1/ExMTYnFc/Pz90797duk1cXBxkWcYff/xh3aZv377Q6/XWbeLj43H06FFcvnzZSZ+m7tu0aROCgoLQunVrPPnkk7h06ZJ1Hc9z7eTl5QEAAgICANjvd0VKSorNMSq3uV1/p//5PFf68ssvERgYiA4dOmDq1KkoLi62rlP7PN92N860t+zsbJjNZptvIAAEBwfjyJEjKlVV/8TExGDx4sVo3bo1Ll68iJkzZ6JPnz44cOAA0tPTodfr4efnZ7NPcHAw0tPTAQDp6enVfg8q11FVleeluvN29XkNCgqyWa/VahEQEGCzTbNmzaoco3Kdv7+/Q+qvTwYMGIBhw4ahWbNmOHnyJP7xj39g4MCBSElJgUaj4XmuBUVRMGXKFPTq1QsdOnQAALv9rqhpm/z8fJSUlMDd3d0RH6lOqu48A8AjjzyCpk2bolGjRti3bx9efPFFHD16FCtXrgSg/nlmuKE6YeDAgdbnnTp1QkxMDJo2bYply5bdVr9IyDU9/PDD1ucdO3ZEp06d0Lx5c2zatAn9+/dXsbL6a9KkSThw4AA2b96sdikurabzPGHCBOvzjh07IjQ0FP3798fJkyfRvHlzZ5dZBbulblFgYCA0Gk2V0fgZGRkICQlRqar6z8/PD61atcKJEycQEhICo9GI3Nxcm22uPschISHVfg8q11FVleflWj+7ISEhyMzMtFlvMpmQk5PDc38LIiMjERgYiBMnTgDgeb5ZTz31FH744Qf8/PPPaNy4sXW5vX5X1LSNj4/PbfWPrZrOc3ViYmIAwOZnWs3zzHBzi/R6Pbp164bk5GTrMkVRkJycjNjYWBUrq98KCwtx8uRJhIaGolu3btDpdDbn+OjRo0hNTbWe49jYWOzfv9/mD8SGDRvg4+ODdu3aOb3++qBZs2YICQmxOa/5+fn4448/bM5rbm4udu7cad3mp59+gqIo1l9msbGx+PXXX1FeXm7dZsOGDWjduvVt11Vyo86dO4dLly4hNDQUAM/zjRJC4KmnnsKqVavw008/Vemms9fvitjYWJtjVG5zu/xOv955rs6ePXsAwOZnWtXzfMtDkkksWbJEGAwGsXjxYnHo0CExYcIE4efnZzNKnK7tueeeE5s2bRKnT58WW7ZsEXFxcSIwMFBkZmYKISzTO5s0aSJ++uknsWPHDhEbGytiY2Ot+1dOO7z33nvFnj17xNq1a0XDhg1v+6ngBQUFYvfu3WL37t0CgJg7d67YvXu3OHv2rBDCMhXcz89PfPfdd2Lfvn1iyJAh1U4F79Kli/jjjz/E5s2bRcuWLW2mKOfm5org4GDx2GOPiQMHDoglS5YIDw+P22qK8rXOc0FBgXj++edFSkqKOH36tNi4caPo2rWraNmypSgtLbUeg+f5+p588knh6+srNm3aZDMFubi42LqNPX5XVE5RfuGFF8Thw4fF/Pnzb6up4Nc7zydOnBCzZs0SO3bsEKdPnxbfffediIyMFH379rUeQ+3zzHBjJ++9955o0qSJ0Ov1Ijo6Wvz+++9ql1SvJCQkiNDQUKHX60VYWJhISEgQJ06csK4vKSkREydOFP7+/sLDw0MMHTpUXLx40eYYZ86cEQMHDhTu7u4iMDBQPPfcc6K8vNzZH6VO+fnnnwWAKo8xY8YIISzTwV999VURHBwsDAaD6N+/vzh69KjNMS5duiRGjhwpvLy8hI+Pjxg3bpwoKCiw2Wbv3r2id+/ewmAwiLCwMPHmm2866yPWCdc6z8XFxeLee+8VDRs2FDqdTjRt2lSMHz++yj9+eJ6vr7pzDEAsWrTIuo29flf8/PPPIioqSuj1ehEZGWnzHq7ueuc5NTVV9O3bVwQEBAiDwSBatGghXnjhBZvr3Aih7nmWKj4IERERkUvgmBsiIiJyKQw3RERE5FIYboiIiMilMNwQERGRS2G4ISIiIpfCcENEREQuheGGiIiIXArDDRHd9iRJwrfffqt2GURkJww3RKSqsWPHQpKkKo8BAwaoXRoR1VNatQsgIhowYAAWLVpks8xgMKhUDRHVd2y5ISLVGQwGhISE2Dwq73QtSRI+/PBDDBw4EO7u7oiMjMSKFSts9t+/fz/uvvtuuLu7o0GDBpgwYQIKCwtttlm4cCHat28Pg8GA0NBQPPXUUzbrs7OzMXToUHh4eKBly5b4/vvvHfuhichhGG6IqM579dVXMXz4cOzduxejRo3Cww8/jMOHDwMAioqKEB8fD39/f2zfvh3Lly/Hxo0bbcLLhx9+iEmTJmHChAnYv38/vv/+e7Ro0cLmPWbOnImHHnoI+/btw6BBgzBq1Cjk5OQ49XMSkZ3Y5fabRES1NGbMGKHRaISnp6fN4/XXXxdCWO5Q/MQTT9jsExMTI5588kkhhBAff/yx8Pf3F4WFhdb1q1evFrIsW++83ahRI/Hyyy/XWAMA8corr1hfFxYWCgDixx9/tNvnJCLn4ZgbIlLdXXfdhQ8//NBmWUBAgPV5bGyszbrY2Fjs2bMHAHD48GF07twZnp6e1vW9evWCoig4evQoJEnChQsX0L9//2vW0KlTJ+tzT09P+Pj4IDMzs7YfiYhUxHBDRKrz9PSs0k1kL+7u7je0nU6ns3ktSRIURXFESUTkYBxzQ0R13u+//17lddu2bQEAbdu2xd69e1FUVGRdv2XLFsiyjNatW8Pb2xsRERFITk52as1EpB623BCR6srKypCenm6zTKvVIjAwEACwfPlydO/eHb1798aXX36Jbdu24bPPPgMAjBo1CtOnT8eYMWMwY8YMZGVlYfLkyXjssccQHBwMAJgxYwaeeOIJBAUFYeDAgSgoKMCWLVswefJk535QInIKhhsiUt3atWsRGhpqs6x169Y4cuQIAMtMpiVLlmDixIkIDQ3F119/jXbt2gEAPDw8sG7dOjzzzDPo0aMHPDw8MHz4cMydO9d6rDFjxqC0tBTvvvsunn/+eQQGBmLEiBHO+4BE5FSSEEKoXQQRUU0kScKqVavw4IMPql0KEdUTHHNDRERELoXhhoiIiFwKx9wQUZ3GnnMiullsuSEiIiKXwnBDRERELoXhhoiIiFwKww0RERG5FIYbIiIicikMN0RERORSGG6IiIjIpTDcEBERkUthuCEiIiKX8v/9wVHtjDblCwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model without regularisers\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(32, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=2500, batch_size=X_train.shape[0], optimizer='Momentum', learning_rate=0.1)\n",
    "\n",
    "network.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset = load_digits()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 --- Train Loss: 2.3023879790487753 --- Val Loss: 2.305273436526423 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/100 --- Train Loss: 2.3019574800343614 --- Val Loss: 2.3016964452929347 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 1.5630365946370406 --- Val Loss: 1.6107506223628791 --- Train Acc: 0.28 --- Val Acc: 0.26\n",
      "Epoch 30/100 --- Train Loss: 0.08807105862077456 --- Val Loss: 0.03045468656032309 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/100 --- Train Loss: 0.05195805726455089 --- Val Loss: 0.021985014658750524 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 50/100 --- Train Loss: 0.050625489490753364 --- Val Loss: 0.0019081891933467687 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/100 --- Train Loss: 0.016361226678667108 --- Val Loss: 0.002906693433754871 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/100 --- Train Loss: 0.007259688073261395 --- Val Loss: 0.0014735301195625296 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.0039056024972497785 --- Val Loss: 0.00011814931610913092 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.015275384513741982 --- Val Loss: 0.0002055721416081518 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model with Momentum optimizer (mini-batch)\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32)) \n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=100, learning_rate=0.1, optimizer='Momentum', momentum=0.5, batch_size=16)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test_ = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test_)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100 --- Train Loss: 2.300465775753062 --- Val Loss: 2.2957617876197984 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 10/100 --- Train Loss: 2.2997559802021392 --- Val Loss: 2.29073007239778 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 20/100 --- Train Loss: 2.2997789997244102 --- Val Loss: 2.2894558231269904 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 30/100 --- Train Loss: 2.299647149671354 --- Val Loss: 2.2912954576688414 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 40/100 --- Train Loss: 0.8697648010962172 --- Val Loss: 0.8181989530182614 --- Train Acc: 0.71 --- Val Acc: 0.71\n",
      "Epoch 50/100 --- Train Loss: 0.05155568324117095 --- Val Loss: 0.0376633636154862 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 60/100 --- Train Loss: 0.019557659092154803 --- Val Loss: 0.009643434421184567 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/100 --- Train Loss: 0.016816690620951917 --- Val Loss: 0.009130538952580359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.01818667284173495 --- Val Loss: 0.0032750205690032863 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.09942491046212466 --- Val Loss: 0.022037243442566162 --- Train Acc: 0.99 --- Val Acc: 0.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model with GD optimizer (mini-batch variant)\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32)) \n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=100, learning_rate=0.1, optimizer='GD', batch_size=16)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test_ = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test_)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49815a38",
   "metadata": {},
   "source": [
    "# Next Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "702fa55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21dae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    def __init__(self, network, param_grid, n_iter=10):\n",
    "        self.network = NeuralNetwork\n",
    "        self.param_grid = param_grid\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def sample_params(self):\n",
    "        sampled_params = {}\n",
    "        for param, values in self.param_grid.items():\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        return sampled_params\n",
    "\n",
    "    def evaluate(self, X_train, y_train, X_val, y_val, params):\n",
    "        network = self.network()\n",
    "\n",
    "        config = params['layer_configs']\n",
    "        # Add the first Dense layer\n",
    "        network.add_layer(Layer(64, config['layer1_nodes'], l1=config['layer1_l1'], l2=config['layer1_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        network.add_layer(Dropout(0.25))\n",
    "\n",
    "        print(config['layer1_nodes'])\n",
    "        # Add the second Dense layer\n",
    "        network.add_layer(Layer(config['layer1_nodes'], config['layer2_nodes'], l1=config['layer2_l1'], l2=config['layer2_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        # Add the output Softmax layer\n",
    "        network.add_layer(Layer(config['layer2_nodes'], 10))\n",
    "        network.add_layer(Softmax())\n",
    "        \n",
    "        network.train(X_train, y_train, epochs=params['epochs'], learning_rate=params['learning_rate'],\n",
    "                      optimizer=params['optimizer'], momentum=params['momentum'], batch_size=params['batch_size'])\n",
    "        \n",
    "        y_pred = network.predict(X_val)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def search(self, X, y):\n",
    "        best_params = None\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            params = self.sample_params()\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            accuracy = self.evaluate(X_train, y_train, X_val, y_val, params)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "\n",
    "            print(f\"Params: {params}, Accuracy: {accuracy}\")\n",
    "\n",
    "        return best_params, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20145077",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'layer_configs': [\n",
    "        {\n",
    "            'layer1_nodes': layer1_nodes,\n",
    "            'layer1_l1': layer1_l1,\n",
    "            'layer1_l2': layer1_l2,\n",
    "            'layer2_nodes': layer2_nodes,\n",
    "            'layer2_l1': layer2_l1,\n",
    "            'layer2_l2': layer2_l2\n",
    "        }\n",
    "        for layer1_nodes in [32, 64, 128]  # Possible node counts for the first Dense layer\n",
    "        for layer1_l1 in [0.0, 0.01]       # L1 regularization for the first Dense layer\n",
    "        for layer1_l2 in [0.0, 0.01]       # L2 regularization for the first Dense layer\n",
    "        for layer2_nodes in [16, 32, 64]   # Possible node counts for the second Dense layer\n",
    "        for layer2_l1 in [0.0, 0.01]       # L1 regularization for the second Dense layer\n",
    "        for layer2_l2 in [0.0, 0.01]       # L2 regularization for the second Dense layer\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'epochs': [100, 500, 1000],\n",
    "    'optimizer': ['GD', 'Momentum'],\n",
    "    'momentum': [0.5, 0.9],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd34073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "Epoch 0/100 --- Train Loss: 0.9720286903360774 --- Val Loss: 0.9480966451249073 --- Train Acc: 0.82 --- Val Acc: 0.79\n",
      "Epoch 10/100 --- Train Loss: 2.301390965555449 --- Val Loss: 2.2989578059814524 --- Train Acc: 0.11 --- Val Acc: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_21800\\118437895.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3013990820322334 --- Val Loss: 2.3023683696672776 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 2.3004087741988375 --- Val Loss: 2.303804685994088 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.3003952956772755 --- Val Loss: 2.304050296731623 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.30023336843938 --- Val Loss: 2.3038384911865424 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.0356762829910147 --- Val Loss: 2.033811233329953 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Epoch 50/100 --- Train Loss: 0.4281770733958992 --- Val Loss: 0.3431704247922756 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 60/100 --- Train Loss: 0.1278440632988667 --- Val Loss: 0.05471193742606455 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 70/100 --- Train Loss: 0.11466880056346171 --- Val Loss: 0.02315853538375091 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 80/100 --- Train Loss: 0.18805308751230065 --- Val Loss: 0.03995852167520661 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 90/100 --- Train Loss: 0.14488716185166514 --- Val Loss: 0.01594243877063699 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.9444444444444444\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3022861741988114 --- Val Loss: 2.3022720510780097 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 0.992663588383937 --- Val Loss: 0.8655796886438524 --- Train Acc: 0.77 --- Val Acc: 0.77\n",
      "Epoch 20/100 --- Train Loss: 0.4450405517993542 --- Val Loss: 0.19069159491179832 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 30/100 --- Train Loss: 0.5874015823081199 --- Val Loss: 0.19474682853565084 --- Train Acc: 0.93 --- Val Acc: 0.94\n",
      "Epoch 40/100 --- Train Loss: 0.8481894543775839 --- Val Loss: 0.1805010246388722 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Epoch 50/100 --- Train Loss: 1.427178844855295 --- Val Loss: 0.24309610507212578 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 60/100 --- Train Loss: 1.6840861076036147 --- Val Loss: 0.24169479283673462 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 70/100 --- Train Loss: 1.83170640284497 --- Val Loss: 0.5320526424447273 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 80/100 --- Train Loss: 1.807588737536074 --- Val Loss: 0.6837439334712431 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 90/100 --- Train Loss: 1.4667378560878641 --- Val Loss: 0.2246884325205268 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.9138888888888889\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 4.197260259199846 --- Val Loss: 3.415077631775704 --- Train Acc: 0.53 --- Val Acc: 0.50\n",
      "Epoch 10/500 --- Train Loss: 2.3084693436626744 --- Val Loss: 2.310284530475599 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 20/500 --- Train Loss: 2.3148996887294864 --- Val Loss: 2.322757735466148 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.313362161747939 --- Val Loss: 2.3049661250273887 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.3198455124703745 --- Val Loss: 2.3247461146848973 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/500 --- Train Loss: 2.3091513302504167 --- Val Loss: 2.305772769563143 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 60/500 --- Train Loss: 2.307164577520285 --- Val Loss: 2.3043765184358533 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: 2.3096080231457465 --- Val Loss: 2.304539995913704 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.3150783926937204 --- Val Loss: 2.312048672829302 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3182397134014394 --- Val Loss: 2.3192402698143324 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.31707192576852 --- Val Loss: 2.307312412128349 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 110/500 --- Train Loss: 2.315997380536906 --- Val Loss: 2.3101706675369638 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.3165074909167664 --- Val Loss: 2.3163417921071163 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 130/500 --- Train Loss: 2.311953532506169 --- Val Loss: 2.325803266774157 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 140/500 --- Train Loss: 2.3070602352964173 --- Val Loss: 2.3110354596201708 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 150/500 --- Train Loss: 2.307030394592343 --- Val Loss: 2.3162675081963404 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/500 --- Train Loss: 2.316894831503646 --- Val Loss: 2.3245551901456465 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: 2.309935403803325 --- Val Loss: 2.3019681352142634 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: 2.3118895279246803 --- Val Loss: 2.316043069155898 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: 2.3077469695349344 --- Val Loss: 2.298724868748383 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.317980662946341 --- Val Loss: 2.316635971127967 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: 2.305903608865214 --- Val Loss: 2.3066622999482287 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: 2.3085665187215167 --- Val Loss: 2.3164513758079255 --- Train Acc: 0.09 --- Val Acc: 0.07\n",
      "Epoch 230/500 --- Train Loss: 2.3150883038545356 --- Val Loss: 2.312434109434539 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: 2.317023162128901 --- Val Loss: 2.30810163128255 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.3051559971483084 --- Val Loss: 2.3049567214348143 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: 2.3045372597543 --- Val Loss: 2.2988774304410122 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.3164627408113643 --- Val Loss: 2.318830550858873 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 280/500 --- Train Loss: 2.31052768133426 --- Val Loss: 2.3126040315783376 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.3138943166276786 --- Val Loss: 2.3128939122570205 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: 2.309785619965761 --- Val Loss: 2.3140073564531254 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: 2.3106368686486944 --- Val Loss: 2.306456272897387 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: 2.313817214680663 --- Val Loss: 2.309342284768357 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: 2.309173752070067 --- Val Loss: 2.3120790175395056 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.3236698162435117 --- Val Loss: 2.316495972663897 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: 2.31130058091043 --- Val Loss: 2.304580692670185 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.3082533536754903 --- Val Loss: 2.304494363165621 --- Train Acc: 0.09 --- Val Acc: 0.07\n",
      "Epoch 370/500 --- Train Loss: 2.3053722145972015 --- Val Loss: 2.2992789958566684 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 380/500 --- Train Loss: 2.3090813445510068 --- Val Loss: 2.313227660655462 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.316830946675951 --- Val Loss: 2.3185444146103196 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: 2.3094605831097295 --- Val Loss: 2.3086490198546503 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.350337425872289 --- Val Loss: 2.3057598599471514 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 10/100 --- Train Loss: 2.306439350144305 --- Val Loss: 2.2948746549526096 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 2.32212090081842 --- Val Loss: 2.302260542626415 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.3160491656439177 --- Val Loss: 2.3268298990124308 --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 40/100 --- Train Loss: 2.3082625980078277 --- Val Loss: 2.314916528820395 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.3064855137757623 --- Val Loss: 2.3074275046436683 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.3048071272405712 --- Val Loss: 2.3045032889126107 --- Train Acc: 0.10 --- Val Acc: 0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_21800\\2185905466.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  return output_gradient * self.mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 1.0248027485793723 --- Val Loss: 0.8899847247934278 --- Train Acc: 0.69 --- Val Acc: 0.71\n",
      "Epoch 10/100 --- Train Loss: 2.3052970506730377 --- Val Loss: 2.307542743906033 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 20/100 --- Train Loss: 2.3152022578294167 --- Val Loss: 2.302718984076048 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 30/100 --- Train Loss: 2.3061194882121305 --- Val Loss: 2.3069267668979845 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/100 --- Train Loss: 2.3040624662716183 --- Val Loss: 2.306722639517705 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/100 --- Train Loss: 2.3069314447829004 --- Val Loss: 2.30832760043844 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3045898347077034 --- Val Loss: 2.3095447500381066 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 70/100 --- Train Loss: 2.304624337947112 --- Val Loss: 2.3034460976828957 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 80/100 --- Train Loss: 2.303916510606519 --- Val Loss: 2.3063887533401686 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 90/100 --- Train Loss: 2.304092932254967 --- Val Loss: 2.3072691158013185 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.1419467052386025 --- Val Loss: 2.1579935393750533 --- Train Acc: 0.29 --- Val Acc: 0.26\n",
      "Epoch 10/100 --- Train Loss: 1.9023717604380745 --- Val Loss: 1.3151024330637147 --- Train Acc: 0.87 --- Val Acc: 0.85\n",
      "Epoch 20/100 --- Train Loss: 1.510797140503049 --- Val Loss: 1.0308164675444158 --- Train Acc: 0.59 --- Val Acc: 0.61\n",
      "Epoch 30/100 --- Train Loss: 2.384232530403613 --- Val Loss: 2.2970456803597528 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/100 --- Train Loss: 2.3486160067534265 --- Val Loss: 2.2964666407105336 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/100 --- Train Loss: 2.348389304994959 --- Val Loss: 2.297541257472008 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/100 --- Train Loss: 2.324400535473724 --- Val Loss: 2.2964638466641576 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.300442122137168 --- Val Loss: 2.297553045224223 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.3004541199245416 --- Val Loss: 2.2967008451635675 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 90/100 --- Train Loss: 2.312395551814788 --- Val Loss: 2.2973138585101633 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09444444444444444\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3022868728050576 --- Val Loss: 2.3022958348064066 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3008076945957097 --- Val Loss: 2.301053663585955 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.300451101840619 --- Val Loss: 2.3007962495816936 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.300366204786056 --- Val Loss: 2.300746617206589 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.3003470191267756 --- Val Loss: 2.3007329540380708 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/500 --- Train Loss: 2.3003417131645794 --- Val Loss: 2.300722893871694 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3003404811862795 --- Val Loss: 2.300721252576754 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.30034009496468 --- Val Loss: 2.300729069002685 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.3003397430900256 --- Val Loss: 2.3007279069885813 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.300339469039864 --- Val Loss: 2.300722904141101 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3003392182436184 --- Val Loss: 2.3007321463995623 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.300338940701774 --- Val Loss: 2.3007082269093413 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.300338722692911 --- Val Loss: 2.30071251077135 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.3003383738252468 --- Val Loss: 2.300723676687138 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.300338023674527 --- Val Loss: 2.3006996164969418 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.300337543301205 --- Val Loss: 2.300682210049318 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/500 --- Train Loss: 2.300337136649051 --- Val Loss: 2.3007072022414525 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/500 --- Train Loss: 2.300336518756451 --- Val Loss: 2.300691941047465 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.300335646205035 --- Val Loss: 2.300704087322011 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/500 --- Train Loss: 2.300334916958419 --- Val Loss: 2.300699943505208 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.300333820785315 --- Val Loss: 2.3006935250613223 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3003323102589457 --- Val Loss: 2.3007164951131394 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.30033073568532 --- Val Loss: 2.300694127962121 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.300328882277713 --- Val Loss: 2.300697905616057 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.3003260267014247 --- Val Loss: 2.3006917137050134 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.300322606083629 --- Val Loss: 2.3006773501280775 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.30031749723439 --- Val Loss: 2.3006644476454086 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.300311216209578 --- Val Loss: 2.3006934525867364 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.3003016664273406 --- Val Loss: 2.3006975970225745 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.3002872093005213 --- Val Loss: 2.300673414325906 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.3002662221615844 --- Val Loss: 2.300658754771615 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.3002263420858284 --- Val Loss: 2.3006188621223145 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: 2.300155873922191 --- Val Loss: 2.300551107045172 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3000192595748454 --- Val Loss: 2.3004206825904667 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.299668875567267 --- Val Loss: 2.300079896762721 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: 2.298581640768009 --- Val Loss: 2.2990473391368345 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.2925036160257064 --- Val Loss: 2.293519473941833 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.1594159809588063 --- Val Loss: 2.1733002307364773 --- Train Acc: 0.17 --- Val Acc: 0.14\n",
      "Epoch 380/500 --- Train Loss: 1.846524093388142 --- Val Loss: 1.8653879688547719 --- Train Acc: 0.23 --- Val Acc: 0.20\n",
      "Epoch 390/500 --- Train Loss: 1.610434303446535 --- Val Loss: 1.605614647260071 --- Train Acc: 0.31 --- Val Acc: 0.31\n",
      "Epoch 400/500 --- Train Loss: 1.3938724426518965 --- Val Loss: 1.3683109581680588 --- Train Acc: 0.46 --- Val Acc: 0.50\n",
      "Epoch 410/500 --- Train Loss: 1.028811294969959 --- Val Loss: 0.9272795784248402 --- Train Acc: 0.71 --- Val Acc: 0.74\n",
      "Epoch 420/500 --- Train Loss: 0.6910754105974403 --- Val Loss: 0.5619908865381019 --- Train Acc: 0.82 --- Val Acc: 0.86\n",
      "Epoch 430/500 --- Train Loss: 0.49559332682698387 --- Val Loss: 0.3777501269397289 --- Train Acc: 0.89 --- Val Acc: 0.90\n",
      "Epoch 440/500 --- Train Loss: 0.39416178100341204 --- Val Loss: 0.27121361601056965 --- Train Acc: 0.91 --- Val Acc: 0.94\n",
      "Epoch 450/500 --- Train Loss: 0.33005084857825534 --- Val Loss: 0.19772245647970838 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 460/500 --- Train Loss: 0.28486344587734647 --- Val Loss: 0.1484753803725148 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 470/500 --- Train Loss: 0.2096359538072765 --- Val Loss: 0.11285635493656918 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 480/500 --- Train Loss: 0.17441956149470605 --- Val Loss: 0.08347729296060483 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 490/500 --- Train Loss: 0.1655200324872631 --- Val Loss: 0.067137473948026 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.9388888888888889\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 1.080620173887577 --- Val Loss: 0.6996198360412778 --- Train Acc: 0.75 --- Val Acc: 0.74\n",
      "Epoch 10/500 --- Train Loss: 2.318598000289774 --- Val Loss: 2.3039966656171003 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.316386102019226 --- Val Loss: 2.3000066655760314 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/500 --- Train Loss: 2.3003329578615004 --- Val Loss: 2.297960984655956 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.3023812502938927 --- Val Loss: 2.2969763615788414 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/500 --- Train Loss: 2.303534825257779 --- Val Loss: 2.3044462405512474 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3039064996090484 --- Val Loss: 2.3070471620419117 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: 2.303315650090512 --- Val Loss: 2.3013419992647726 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/500 --- Train Loss: 2.30277245478248 --- Val Loss: 2.3046204079690233 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 90/500 --- Train Loss: 2.303401219075208 --- Val Loss: 2.2993539720074576 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/500 --- Train Loss: 2.303145122598324 --- Val Loss: 2.304303882380188 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/500 --- Train Loss: 2.30237517645776 --- Val Loss: 2.303624140339522 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.3037094126210316 --- Val Loss: 2.3033876764700127 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.30175276687789 --- Val Loss: 2.2996803611488184 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/500 --- Train Loss: 2.3028411476966597 --- Val Loss: 2.300593624099247 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/500 --- Train Loss: 2.302199245152733 --- Val Loss: 2.2996700686367006 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: 2.3018004582399727 --- Val Loss: 2.3023709170985382 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/500 --- Train Loss: 2.3025339156483104 --- Val Loss: 2.3046911314601 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: 2.302090911120927 --- Val Loss: 2.303863698277343 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/500 --- Train Loss: 2.303197300039681 --- Val Loss: 2.303814295171859 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.3019103509960597 --- Val Loss: 2.2985276489491757 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/500 --- Train Loss: 2.3030246639876935 --- Val Loss: 2.297877891591686 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: 2.3025076762606704 --- Val Loss: 2.300274325833888 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/500 --- Train Loss: 2.30338223185191 --- Val Loss: 2.2992751551004815 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.303441753238642 --- Val Loss: 2.3000582317003317 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/500 --- Train Loss: 2.3031930999657133 --- Val Loss: 2.3046148872888303 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.3022255317816667 --- Val Loss: 2.297880864192649 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: 2.302143880230148 --- Val Loss: 2.3024203256207847 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: 2.30228199659176 --- Val Loss: 2.3036498174392555 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 290/500 --- Train Loss: 2.303336396101936 --- Val Loss: 2.3035055265039026 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.302103578605657 --- Val Loss: 2.3009515763892616 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/500 --- Train Loss: 2.3020290855069483 --- Val Loss: 2.3022829568547754 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: 2.3037057574510156 --- Val Loss: 2.2982475593432565 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/500 --- Train Loss: 2.3024082537156083 --- Val Loss: 2.3028348018015405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/500 --- Train Loss: 2.3019688762899464 --- Val Loss: 2.3033527006279835 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/500 --- Train Loss: 2.301630603530794 --- Val Loss: 2.3003384552159822 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/500 --- Train Loss: 2.301892104846442 --- Val Loss: 2.2991402102583907 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/500 --- Train Loss: 2.3031842626245584 --- Val Loss: 2.3042749765169646 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.3017400255890705 --- Val Loss: 2.299093924022981 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/500 --- Train Loss: 2.3029763255482716 --- Val Loss: 2.3045660343570473 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: 2.3019758754679307 --- Val Loss: 2.2979364652891614 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/500 --- Train Loss: 2.302878400315224 --- Val Loss: 2.305111446825362 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.30168225914854 --- Val Loss: 2.2986432209203986 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/500 --- Train Loss: 2.3028524967996815 --- Val Loss: 2.302955393106475 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/500 --- Train Loss: 2.3021661589470104 --- Val Loss: 2.297976119835093 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/500 --- Train Loss: 2.302865233137414 --- Val Loss: 2.2983265258717385 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/500 --- Train Loss: 2.301347635103507 --- Val Loss: 2.300672867988494 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/500 --- Train Loss: 2.3017813480793965 --- Val Loss: 2.297560604150463 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/500 --- Train Loss: 2.3019044112582954 --- Val Loss: 2.2997404595050877 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/500 --- Train Loss: 2.3027283849339475 --- Val Loss: 2.298821771824478 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.08333333333333333\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 3.2877339732496504 --- Val Loss: 2.731558533546249 --- Train Acc: 0.60 --- Val Acc: 0.61\n",
      "Epoch 10/100 --- Train Loss: 2.31242119957699 --- Val Loss: 2.3025162895779054 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.300877883658162 --- Val Loss: 2.296822275417238 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.304338344172444 --- Val Loss: 2.2960932854378906 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3030284436182353 --- Val Loss: 2.2979934875534065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.597162423604096 --- Val Loss: 2.3280612153980123 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 10/1000 --- Train Loss: 2.321786049750232 --- Val Loss: 2.2977857524768286 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.3128549359452975 --- Val Loss: 2.2966448475259935 --- Train Acc: 0.10 --- Val Acc: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_21800\\118437895.py:19: RuntimeWarning: overflow encountered in subtract\n",
      "  exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 1.6344040246159313 --- Val Loss: 1.6245288255902173 --- Train Acc: 0.55 --- Val Acc: 0.52\n",
      "Epoch 10/500 --- Train Loss: 1.4118233009446683 --- Val Loss: 1.1714809997355096 --- Train Acc: 0.60 --- Val Acc: 0.61\n",
      "Epoch 20/500 --- Train Loss: 2.294884926678296 --- Val Loss: 2.266481447920308 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 30/500 --- Train Loss: 2.332229048661626 --- Val Loss: 2.3027025389055416 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/500 --- Train Loss: 2.3251285796950167 --- Val Loss: 2.3048151056401904 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/500 --- Train Loss: 2.325463866038639 --- Val Loss: 2.2999129756376364 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/500 --- Train Loss: 2.299691179913849 --- Val Loss: 2.3024970277087315 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/500 --- Train Loss: 2.3016016860962147 --- Val Loss: 2.300840454605066 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/500 --- Train Loss: 2.3015100607371313 --- Val Loss: 2.302908375580289 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/500 --- Train Loss: 2.301359962482282 --- Val Loss: 2.302249555531646 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/500 --- Train Loss: 2.301849962665582 --- Val Loss: 2.3018800450576036 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 110/500 --- Train Loss: 2.301550492728762 --- Val Loss: 2.304611715005657 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.3015941355562135 --- Val Loss: 2.301449226519466 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/500 --- Train Loss: 2.3013622923860773 --- Val Loss: 2.3037830140938023 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/500 --- Train Loss: 2.3014210250225786 --- Val Loss: 2.3031294610657023 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/500 --- Train Loss: 2.3118777164896622 --- Val Loss: 2.3023138681678454 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/500 --- Train Loss: 2.3016781839229656 --- Val Loss: 2.305644879159593 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/500 --- Train Loss: 2.301537466472654 --- Val Loss: 2.3030329842568777 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 180/500 --- Train Loss: 2.301638680601557 --- Val Loss: 2.301876249910983 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 190/500 --- Train Loss: 2.301469500559528 --- Val Loss: 2.303295752798212 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/500 --- Train Loss: 2.301384175945784 --- Val Loss: 2.302499663397396 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/500 --- Train Loss: 2.301401774338714 --- Val Loss: 2.30313466724056 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 220/500 --- Train Loss: 2.3013252683815084 --- Val Loss: 2.301610773906344 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.3015736584665047 --- Val Loss: 2.302030379577968 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/500 --- Train Loss: 2.3015278827352663 --- Val Loss: 2.30288411901843 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: 2.301515890182367 --- Val Loss: 2.300092275579537 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/500 --- Train Loss: 2.3013251996703414 --- Val Loss: 2.3026724581122893 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/500 --- Train Loss: 2.3014872963823994 --- Val Loss: 2.303283939639428 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/500 --- Train Loss: 2.3016234165178338 --- Val Loss: 2.3025225178992894 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.301404659051791 --- Val Loss: 2.302633788053763 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/500 --- Train Loss: 2.3014028907736526 --- Val Loss: 2.303189921705246 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/500 --- Train Loss: 2.30148610294555 --- Val Loss: 2.302220019805596 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/500 --- Train Loss: 2.3015700719819536 --- Val Loss: 2.3045329073060348 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/500 --- Train Loss: 2.30133983021146 --- Val Loss: 2.3020708745294187 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/500 --- Train Loss: 2.301680142812197 --- Val Loss: 2.301270481241388 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.301391845322612 --- Val Loss: 2.3023359420389697 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/500 --- Train Loss: 2.301548440905769 --- Val Loss: 2.303264472707912 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/500 --- Train Loss: 2.3017984630566812 --- Val Loss: 2.3023319981227544 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 380/500 --- Train Loss: 2.3019401504956027 --- Val Loss: 2.303278370357781 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/500 --- Train Loss: 2.301673370587769 --- Val Loss: 2.304295588406097 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: 2.3015371142581125 --- Val Loss: 2.3040083699526215 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 410/500 --- Train Loss: 2.3016907974628316 --- Val Loss: 2.3047412284532616 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 420/500 --- Train Loss: 2.3015533324254185 --- Val Loss: 2.3009089721726097 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/500 --- Train Loss: 2.301530174393857 --- Val Loss: 2.3049131917814916 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/500 --- Train Loss: 2.301711791109008 --- Val Loss: 2.302068341153292 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/500 --- Train Loss: 2.301604008465422 --- Val Loss: 2.3013227052999556 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/500 --- Train Loss: 2.301681959414291 --- Val Loss: 2.3050182514443414 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/500 --- Train Loss: 2.301433389232454 --- Val Loss: 2.3024568762757522 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/500 --- Train Loss: 2.301769757296021 --- Val Loss: 2.301214027739579 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/500 --- Train Loss: 2.3015130301281856 --- Val Loss: 2.3039905050646023 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09722222222222222\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 4.443031594445748 --- Val Loss: 3.6018042309439555 --- Train Acc: 0.59 --- Val Acc: 0.57\n",
      "Epoch 10/500 --- Train Loss: 2.3636642736909828 --- Val Loss: 2.3089462663086486 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.3143227179593944 --- Val Loss: 2.300415692503316 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.327366064455619 --- Val Loss: 2.3121877395268835 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.3040728179277763 --- Val Loss: 2.295614168789662 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 50/500 --- Train Loss: 2.3043157744607736 --- Val Loss: 2.305599669139441 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3179013136003275 --- Val Loss: 2.3034151703432357 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.302858870183929 --- Val Loss: 2.3014550757614 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.3038500678125176 --- Val Loss: 2.3080983870622145 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: 2.3021772082830156 --- Val Loss: 2.3014100769153556 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/500 --- Train Loss: 2.302553767860294 --- Val Loss: 2.299583848773778 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/500 --- Train Loss: 2.302620626043542 --- Val Loss: 2.3037132915106464 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.317577190517515 --- Val Loss: 2.308201252345933 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.275194320694509 --- Val Loss: 2.2718879488584562 --- Train Acc: 0.42 --- Val Acc: 0.46\n",
      "Epoch 10/1000 --- Train Loss: 1.4322261007331971 --- Val Loss: 0.23425068618224137 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 20/1000 --- Train Loss: 1.6500684913542591 --- Val Loss: 0.478109089136957 --- Train Acc: 0.89 --- Val Acc: 0.90\n",
      "Epoch 30/1000 --- Train Loss: 1.809217862026321 --- Val Loss: 0.8632165360332997 --- Train Acc: 0.69 --- Val Acc: 0.71\n",
      "Epoch 40/1000 --- Train Loss: 1.5839240199648201 --- Val Loss: 1.3358803920251425 --- Train Acc: 0.50 --- Val Acc: 0.49\n",
      "Epoch 50/1000 --- Train Loss: 1.9721632854398492 --- Val Loss: 1.822450519584695 --- Train Acc: 0.31 --- Val Acc: 0.32\n",
      "Epoch 60/1000 --- Train Loss: 2.020814397254629 --- Val Loss: 2.0131355420115753 --- Train Acc: 0.23 --- Val Acc: 0.23\n",
      "Epoch 70/1000 --- Train Loss: 2.063560517633236 --- Val Loss: 2.081200537307081 --- Train Acc: 0.19 --- Val Acc: 0.20\n",
      "Epoch 80/1000 --- Train Loss: 2.1531151503922894 --- Val Loss: 2.1020032453212827 --- Train Acc: 0.19 --- Val Acc: 0.19\n",
      "Epoch 90/1000 --- Train Loss: 2.3757459549112503 --- Val Loss: 2.3042165817601004 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.3637752931618903 --- Val Loss: 2.2875563046018197 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.333758081802547 --- Val Loss: 2.304131641421038 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.35010064683612 --- Val Loss: 2.3025107190949727 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.3117749415617586 --- Val Loss: 2.3020921364449105 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.301836180618496 --- Val Loss: 2.302024913256755 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.299842984022077 --- Val Loss: 2.3024059101072787 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3018528885797833 --- Val Loss: 2.3032392652584126 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.2998535401959286 --- Val Loss: 2.3025004572244585 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.313997300429295 --- Val Loss: 2.304643895142324 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.3018541166202753 --- Val Loss: 2.3023974303271815 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.301816081883422 --- Val Loss: 2.3027084257326473 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.3018370303387288 --- Val Loss: 2.3021025623537827 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.301788163528375 --- Val Loss: 2.301973506173428 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.3018476699565245 --- Val Loss: 2.3018386021522454 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.3018531065401255 --- Val Loss: 2.300898382415672 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3018962085629835 --- Val Loss: 2.3007370975995483 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.301869978069932 --- Val Loss: 2.3017479277433215 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.3138138709820226 --- Val Loss: 2.303242456342727 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.301833223964373 --- Val Loss: 2.3023487213257794 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.3018144752629417 --- Val Loss: 2.3017324234865293 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3138821422418907 --- Val Loss: 2.3008622164538335 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3018714694525046 --- Val Loss: 2.30171410379851 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.3018558178799227 --- Val Loss: 2.3009473072255613 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.301792486410188 --- Val Loss: 2.3022477734377094 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.3258522872425473 --- Val Loss: 2.3017547204833133 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3018321960622625 --- Val Loss: 2.301718626914732 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.301855599416886 --- Val Loss: 2.30098950792652 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3018371094164944 --- Val Loss: 2.302063491980621 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3017906005489186 --- Val Loss: 2.3022910501143623 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3017959403004573 --- Val Loss: 2.3019789335543432 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3018139744848614 --- Val Loss: 2.3021578654927604 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.301797633082041 --- Val Loss: 2.301693476809307 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3138330961573246 --- Val Loss: 2.3026457312060087 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.3018321519397813 --- Val Loss: 2.3029164905475814 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3138677527426865 --- Val Loss: 2.301542268231253 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3018456932718814 --- Val Loss: 2.3022564020589216 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3018233480518773 --- Val Loss: 2.3023338534814464 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.301817590509595 --- Val Loss: 2.3017159219721535 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.3018261942527216 --- Val Loss: 2.3016401286863344 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.3138741600922543 --- Val Loss: 2.301836496482876 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.30181741201069 --- Val Loss: 2.302723079665557 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.301830259634813 --- Val Loss: 2.301034165387434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.301818071758862 --- Val Loss: 2.3020473091605576 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.3018414246679204 --- Val Loss: 2.3021109020026294 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3018711864135355 --- Val Loss: 2.3024636665177 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.3018125014632136 --- Val Loss: 2.302304392361732 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.301878668363588 --- Val Loss: 2.302560836178254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3018531735623897 --- Val Loss: 2.301691184244513 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.301893904667531 --- Val Loss: 2.3028676756959947 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.3018012220010937 --- Val Loss: 2.3016354813083124 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.301823340197498 --- Val Loss: 2.3018679535932605 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.30186364444906 --- Val Loss: 2.300770923783215 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3018559432515873 --- Val Loss: 2.3010030507721844 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.301900944450401 --- Val Loss: 2.3020063524883376 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.301856317374443 --- Val Loss: 2.3029698743126605 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.3018215737715675 --- Val Loss: 2.3017627770551066 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3018616319798326 --- Val Loss: 2.302391715997248 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.301823498630064 --- Val Loss: 2.3012759905795064 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.3018552473078846 --- Val Loss: 2.3019962964280065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.3018017685802157 --- Val Loss: 2.3013806132182433 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.301895378338152 --- Val Loss: 2.3015284694735834 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.3017957449160282 --- Val Loss: 2.3017284141740273 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3018067189601368 --- Val Loss: 2.302268834350813 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.301817347870371 --- Val Loss: 2.3023445941726832 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.301838258791442 --- Val Loss: 2.301209368741317 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.3018286693217576 --- Val Loss: 2.302130394443615 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3018337591387 --- Val Loss: 2.302065347316032 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.301884624645268 --- Val Loss: 2.302327052341232 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.301830568670137 --- Val Loss: 2.3023204053122206 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.3018418245171435 --- Val Loss: 2.3018077949899607 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.301842593531231 --- Val Loss: 2.30111578622407 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.301930869171542 --- Val Loss: 2.300890892005176 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.301803552944965 --- Val Loss: 2.3018611892495713 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.301860837001255 --- Val Loss: 2.300961411697464 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.301847169216254 --- Val Loss: 2.300931257799685 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.301824496759251 --- Val Loss: 2.301949101293993 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.301834554817157 --- Val Loss: 2.302307975994201 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.3018595405738256 --- Val Loss: 2.3030449985506554 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.301807963719143 --- Val Loss: 2.3023673237416133 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3018264526730143 --- Val Loss: 2.302134934017729 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3018894609141287 --- Val Loss: 2.302331703263012 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.3018389054610373 --- Val Loss: 2.301097324407573 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.3017975555843924 --- Val Loss: 2.302045453440266 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.3018143139393206 --- Val Loss: 2.302147220701589 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3018485388024326 --- Val Loss: 2.3023417011899867 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3018321456796413 --- Val Loss: 2.3009557898385884 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.3018197440214556 --- Val Loss: 2.3019444368377933 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: 2.3018277200509987 --- Val Loss: 2.3014781626794405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3018196924339764 --- Val Loss: 2.3019509721252245 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3018601438435446 --- Val Loss: 2.3016391517898067 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.3018793290966437 --- Val Loss: 2.301573835721121 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/500 --- Train Loss: 2.300186433369093 --- Val Loss: 2.2979012130015253 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/500 --- Train Loss: 2.300130891728288 --- Val Loss: 2.2976887181977808 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 30/500 --- Train Loss: 2.3001278676251156 --- Val Loss: 2.297523878833886 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/500 --- Train Loss: 2.300128938917267 --- Val Loss: 2.297677523485715 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/500 --- Train Loss: 2.300126302989253 --- Val Loss: 2.2975534943926785 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/500 --- Train Loss: 2.30012469584421 --- Val Loss: 2.2973936429639665 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 70/500 --- Train Loss: 2.300122886350104 --- Val Loss: 2.2973972273668335 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 80/500 --- Train Loss: 2.300119068996588 --- Val Loss: 2.297476168689457 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 90/500 --- Train Loss: 2.3001129611112754 --- Val Loss: 2.297580896444794 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/500 --- Train Loss: 2.30009469221162 --- Val Loss: 2.297499312833957 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 110/500 --- Train Loss: 2.300045184603211 --- Val Loss: 2.297318398165758 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 120/500 --- Train Loss: 2.299799873409256 --- Val Loss: 2.2970917589178015 --- Train Acc: 0.12 --- Val Acc: 0.15\n",
      "Epoch 130/500 --- Train Loss: 2.2950685827962363 --- Val Loss: 2.29236545677201 --- Train Acc: 0.22 --- Val Acc: 0.22\n",
      "Epoch 140/500 --- Train Loss: 1.6801409656226478 --- Val Loss: 1.6964007645208352 --- Train Acc: 0.22 --- Val Acc: 0.18\n",
      "Epoch 150/500 --- Train Loss: 1.4418314284481002 --- Val Loss: 1.4654024267489627 --- Train Acc: 0.34 --- Val Acc: 0.33\n",
      "Epoch 160/500 --- Train Loss: 1.1005326967196785 --- Val Loss: 1.0574286350450257 --- Train Acc: 0.56 --- Val Acc: 0.55\n",
      "Epoch 170/500 --- Train Loss: 0.6481090073278801 --- Val Loss: 0.5467108621593949 --- Train Acc: 0.81 --- Val Acc: 0.82\n",
      "Epoch 180/500 --- Train Loss: 0.3225538049960464 --- Val Loss: 0.23696405637491486 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 190/500 --- Train Loss: 0.16461341879574937 --- Val Loss: 0.1130765781846348 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 200/500 --- Train Loss: 0.08738680044797241 --- Val Loss: 0.07163365832487303 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 210/500 --- Train Loss: 0.06282190666404108 --- Val Loss: 0.044318603377787494 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/500 --- Train Loss: 0.060976049915437314 --- Val Loss: 0.02820589346075516 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 230/500 --- Train Loss: 0.03735393997304549 --- Val Loss: 0.019275070440306692 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 240/500 --- Train Loss: 0.030640966462936205 --- Val Loss: 0.014917414214688477 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.021723021820206943 --- Val Loss: 0.011135633082590439 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.026526802707169554 --- Val Loss: 0.008291125999430639 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.020897964769939405 --- Val Loss: 0.006932346321817868 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/500 --- Train Loss: 0.012282183103046245 --- Val Loss: 0.005567040215080704 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.01586169143715852 --- Val Loss: 0.004395580144611135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.01813492946491114 --- Val Loss: 0.003918938847523667 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.015371739508022533 --- Val Loss: 0.0039031383253545107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.01189871763806093 --- Val Loss: 0.002795362935792607 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/500 --- Train Loss: 0.011827656369617691 --- Val Loss: 0.003042635422507147 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/500 --- Train Loss: 0.010547765086346174 --- Val Loss: 0.00225418922493566 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/500 --- Train Loss: 0.01722654145552126 --- Val Loss: 0.0016695953930675313 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.009602015892874032 --- Val Loss: 0.0015543290387956656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.00779367301843147 --- Val Loss: 0.0011915293213732047 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/500 --- Train Loss: 0.007382781324435203 --- Val Loss: 0.0012387082882469869 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/500 --- Train Loss: 0.010503103426621517 --- Val Loss: 0.0012950440360221425 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.00364651987319583 --- Val Loss: 0.0011246214801057953 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/500 --- Train Loss: 0.004676622819855327 --- Val Loss: 0.0009686872654518113 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.009871826771733644 --- Val Loss: 0.0009287105530931584 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/500 --- Train Loss: 0.006707884014374529 --- Val Loss: 0.0007542909672765902 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/500 --- Train Loss: 0.004579556135041819 --- Val Loss: 0.0008159212164846931 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/500 --- Train Loss: 0.007135874338567206 --- Val Loss: 0.0006994088845707506 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/500 --- Train Loss: 0.011821631769148064 --- Val Loss: 0.0006738394862726001 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/500 --- Train Loss: 0.004910550608187781 --- Val Loss: 0.0005938735232260063 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/500 --- Train Loss: 0.009375436479749092 --- Val Loss: 0.0006246425632025586 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/500 --- Train Loss: 0.005556123543493287 --- Val Loss: 0.0006541232191599018 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.9583333333333334\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3025185924637896 --- Val Loss: 2.302557095447874 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.3019665275587062 --- Val Loss: 2.3023568044208895 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.3015813746196496 --- Val Loss: 2.3022669441141117 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.301312061204263 --- Val Loss: 2.3022455935361896 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.301123974690253 --- Val Loss: 2.302263709790712 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.3009916936977413 --- Val Loss: 2.302297432296734 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.3008996602382896 --- Val Loss: 2.3023427254663327 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.300834943688806 --- Val Loss: 2.302387193369032 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: 2.3007893209386348 --- Val Loss: 2.3024374400350682 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3007572852004934 --- Val Loss: 2.3024835288339522 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3984116851606077 --- Val Loss: 2.395936614826256 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 60/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 80/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 100/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 110/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3011397221357983 --- Val Loss: 2.299291369995128 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 0.8176339043749458 --- Val Loss: 0.48054385417447776 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 20/100 --- Train Loss: 1.7837617898511455 --- Val Loss: 1.7267586405751596 --- Train Acc: 0.38 --- Val Acc: 0.35\n",
      "Epoch 30/100 --- Train Loss: 2.400020655385426 --- Val Loss: 2.29220445255342 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/100 --- Train Loss: 2.312791014667904 --- Val Loss: 2.298024467599561 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/100 --- Train Loss: 2.300858483927397 --- Val Loss: 2.297914158081735 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.300792838531182 --- Val Loss: 2.2973888245952505 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 70/100 --- Train Loss: 2.3007978363783828 --- Val Loss: 2.2979071996261218 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/100 --- Train Loss: 2.300809458123703 --- Val Loss: 2.2977247302297474 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/100 --- Train Loss: 2.3007854582598606 --- Val Loss: 2.2976113432008614 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3023065127969478 --- Val Loss: 2.3022398853045223 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.3019314373445994 --- Val Loss: 2.3013591480608477 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 0.2812932839801202 --- Val Loss: 0.1122237782385202 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 30/1000 --- Train Loss: 0.5354297361129814 --- Val Loss: 0.2262629185561617 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 40/1000 --- Train Loss: 1.8266028880826157 --- Val Loss: 1.7747347202145856 --- Train Acc: 0.33 --- Val Acc: 0.33\n",
      "Epoch 50/1000 --- Train Loss: 2.2685075148230776 --- Val Loss: 2.2889037001811405 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.302184858109927 --- Val Loss: 2.3004678257790077 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.3018757715437097 --- Val Loss: 2.3002579283567037 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.302708198794436 --- Val Loss: 2.301025936002573 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.302206440134188 --- Val Loss: 2.3013449672821933 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.300174060568619 --- Val Loss: 2.3009806921525935 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.302158292185305 --- Val Loss: 2.300366266901206 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.302238274753007 --- Val Loss: 2.3027178849072083 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.3120616846584547 --- Val Loss: 2.301311897329378 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.3022933687836953 --- Val Loss: 2.300317683167624 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.302061798931493 --- Val Loss: 2.3014397525363903 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3021102891545815 --- Val Loss: 2.299558721661312 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.3021752210914967 --- Val Loss: 2.3002416176300042 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3016815249396543 --- Val Loss: 2.30004258672406 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.3019796063922886 --- Val Loss: 2.3005501662426457 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.3023082011616474 --- Val Loss: 2.3018212225642998 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.3024423317145883 --- Val Loss: 2.3001060417682453 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.3018470077878286 --- Val Loss: 2.300273489497206 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.301915113279526 --- Val Loss: 2.2999002314967867 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.302065585535913 --- Val Loss: 2.3002233077592047 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: 2.3023231630042025 --- Val Loss: 2.300833887105008 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.3019182101013103 --- Val Loss: 2.2997290070723353 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.302427413476067 --- Val Loss: 2.3001334877998905 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.3021036432828037 --- Val Loss: 2.299277557286336 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.302047919890823 --- Val Loss: 2.300617884885576 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.301928560733645 --- Val Loss: 2.301698619672098 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3022683182874544 --- Val Loss: 2.2989420423219564 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.3018890146875695 --- Val Loss: 2.3008075979156457 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.302616712376096 --- Val Loss: 2.3022967274960866 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.3020445731778882 --- Val Loss: 2.299366775926749 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.302458000438598 --- Val Loss: 2.3025878387404575 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.302120187684624 --- Val Loss: 2.3013041853893 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3021142047159655 --- Val Loss: 2.3003392272201335 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 380/1000 --- Train Loss: 2.302463232960916 --- Val Loss: 2.301869513887605 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.3022549873552953 --- Val Loss: 2.298969898318592 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 400/1000 --- Train Loss: 2.3023021407987265 --- Val Loss: 2.3000571013418254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.3019678830485204 --- Val Loss: 2.29974627337447 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3025042042651065 --- Val Loss: 2.302245090068375 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3005970974408796 --- Val Loss: 2.3008843479673122 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.303376550566291 --- Val Loss: 2.299913295735341 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.30263813065274 --- Val Loss: 2.2981108482977377 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.302108097874705 --- Val Loss: 2.301476823932579 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.302166488317337 --- Val Loss: 2.301579356671236 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.3022092675628665 --- Val Loss: 2.300470595108153 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.302198554463755 --- Val Loss: 2.2988274918683063 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 500/1000 --- Train Loss: 2.3019756919490084 --- Val Loss: 2.2998103920607917 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.301929756683309 --- Val Loss: 2.3004859762098757 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.3020931169185888 --- Val Loss: 2.3010006098712728 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.3024921034547727 --- Val Loss: 2.299404526720335 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3022797215490174 --- Val Loss: 2.3012345778183048 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.3021706767629238 --- Val Loss: 2.3013657147103346 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.30220643941121 --- Val Loss: 2.3007803787741965 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3020196190049833 --- Val Loss: 2.3007993220872196 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.30212740777111 --- Val Loss: 2.3018303905900113 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3020772260559976 --- Val Loss: 2.300282373003371 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.302338890166602 --- Val Loss: 2.3008583428651965 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.302463789024496 --- Val Loss: 2.3005653520290474 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.3020949037069722 --- Val Loss: 2.3003185424733834 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.301899049352813 --- Val Loss: 2.3008530975711636 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3019602734771016 --- Val Loss: 2.3006045634729992 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.3021434498063917 --- Val Loss: 2.301573151120254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3020261356896072 --- Val Loss: 2.301580524970227 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3023483739097017 --- Val Loss: 2.2987684204702363 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.302071839749454 --- Val Loss: 2.3013665950634623 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.302235411218285 --- Val Loss: 2.2991816344619744 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.3019935649821184 --- Val Loss: 2.2993452953858178 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.302212069839904 --- Val Loss: 2.298956786290405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3022402354308706 --- Val Loss: 2.302966848633982 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.3022043142637307 --- Val Loss: 2.3011804750330644 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.3023788515608756 --- Val Loss: 2.3000683725717295 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.3020702228117638 --- Val Loss: 2.2997397466201694 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3020572094563403 --- Val Loss: 2.2998577862352314 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.302024113866609 --- Val Loss: 2.2996253507982107 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.302055469241349 --- Val Loss: 2.3011358806830247 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.3018305289382 --- Val Loss: 2.299568478757737 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.3020689709932185 --- Val Loss: 2.3002302183556274 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3022508485768656 --- Val Loss: 2.302365431221482 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.302031615681927 --- Val Loss: 2.299561002491288 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 830/1000 --- Train Loss: 2.3024497796025516 --- Val Loss: 2.3019819286923124 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.3027993355415233 --- Val Loss: 2.301638067550108 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.3022728709479328 --- Val Loss: 2.299216778138044 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.3021253601801064 --- Val Loss: 2.3002592544556393 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.3021470693728645 --- Val Loss: 2.300693636317537 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.30200168809009 --- Val Loss: 2.3009752713853446 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3019893257040698 --- Val Loss: 2.301329207119691 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3018807516116104 --- Val Loss: 2.300299692373579 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.3021063017063415 --- Val Loss: 2.2998615403301343 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.3021339826695004 --- Val Loss: 2.3002257300387274 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.3019848872840143 --- Val Loss: 2.301078747459067 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3021523774152093 --- Val Loss: 2.2997537386744935 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 950/1000 --- Train Loss: 2.3019418649380445 --- Val Loss: 2.299406284426953 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.3019987866106977 --- Val Loss: 2.2996488461496987 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: 2.3022389145269426 --- Val Loss: 2.3014257974634282 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.302081791329573 --- Val Loss: 2.3023959428922574 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.30233080434896 --- Val Loss: 2.301930644990493 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.302419576212732 --- Val Loss: 2.3025776222527576 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/500 --- Train Loss: 2.301460556837265 --- Val Loss: 2.3030057926810494 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.288317508745249 --- Val Loss: 2.2890764120616818 --- Train Acc: 0.14 --- Val Acc: 0.14\n",
      "Epoch 30/500 --- Train Loss: 1.280687639831665 --- Val Loss: 1.2632758598416578 --- Train Acc: 0.69 --- Val Acc: 0.65\n",
      "Epoch 40/500 --- Train Loss: 0.5438311615150127 --- Val Loss: 0.4865017452476228 --- Train Acc: 0.87 --- Val Acc: 0.85\n",
      "Epoch 50/500 --- Train Loss: 0.36873791742524875 --- Val Loss: 0.2369765994071122 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 60/500 --- Train Loss: 0.26194979455276507 --- Val Loss: 0.12768954882440503 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 70/500 --- Train Loss: 0.2265183261779575 --- Val Loss: 0.0823052580159458 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 80/500 --- Train Loss: 0.18085936877462475 --- Val Loss: 0.059605576091066426 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 90/500 --- Train Loss: 0.2033034739688955 --- Val Loss: 0.05600059072089691 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 100/500 --- Train Loss: 0.2309458466637222 --- Val Loss: 0.04109020268145561 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 110/500 --- Train Loss: 0.22803975315085123 --- Val Loss: 0.03728236487841879 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 120/500 --- Train Loss: 0.19344497808668432 --- Val Loss: 0.023369537160693375 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 130/500 --- Train Loss: 0.15533395999361063 --- Val Loss: 0.017055224337356366 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.20735759216213956 --- Val Loss: 0.020028676371729545 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 150/500 --- Train Loss: 0.20445101706085653 --- Val Loss: 0.03201334930672279 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 160/500 --- Train Loss: 0.17136408764273062 --- Val Loss: 0.018540311795047813 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 170/500 --- Train Loss: 0.12975899909296093 --- Val Loss: 0.025066453517155005 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 180/500 --- Train Loss: 0.1977220491714363 --- Val Loss: 0.007565153857858701 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.17841350800422115 --- Val Loss: 0.016255336743119256 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 200/500 --- Train Loss: 0.19559057680619868 --- Val Loss: 0.02028711342227115 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 210/500 --- Train Loss: 0.2004091561255886 --- Val Loss: 0.006757077857368581 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.23752206211056284 --- Val Loss: 0.07240069001308974 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 230/500 --- Train Loss: 0.24576147620959576 --- Val Loss: 0.016195215861423744 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 240/500 --- Train Loss: 0.28255512529512417 --- Val Loss: 0.056807246348626404 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 250/500 --- Train Loss: 0.3341828842709432 --- Val Loss: 0.07872057233962895 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 260/500 --- Train Loss: 0.19949977261205237 --- Val Loss: 0.02253638632294788 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 270/500 --- Train Loss: 0.28810300057593446 --- Val Loss: 0.01609327579115498 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/500 --- Train Loss: 0.3401797349815958 --- Val Loss: 0.0031708299967192675 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.3664649474294998 --- Val Loss: 0.07350604925157815 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 300/500 --- Train Loss: 0.33199294862099143 --- Val Loss: 0.03769476674384298 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 310/500 --- Train Loss: 0.5049488775996697 --- Val Loss: 0.01308438683869843 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 320/500 --- Train Loss: 0.32869676655505214 --- Val Loss: 0.12883631482151867 --- Train Acc: 0.99 --- Val Acc: 0.97\n",
      "Epoch 330/500 --- Train Loss: 0.41325415914090935 --- Val Loss: 0.018777133768362948 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 340/500 --- Train Loss: 0.3752660501742652 --- Val Loss: 0.021231640835831272 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 350/500 --- Train Loss: 0.5157458290709608 --- Val Loss: 0.0005684721620213342 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.4848854143866939 --- Val Loss: 0.017009548953922562 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.5329554576031738 --- Val Loss: 0.08390452171058253 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 380/500 --- Train Loss: 0.5672941713931567 --- Val Loss: 0.05602747960510308 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 390/500 --- Train Loss: 0.6437816690977338 --- Val Loss: 0.041583376764831904 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 400/500 --- Train Loss: 0.549207643227496 --- Val Loss: 0.020411450858931264 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 410/500 --- Train Loss: 0.44090119926728666 --- Val Loss: 0.08524261451830824 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 420/500 --- Train Loss: 0.5034117438361912 --- Val Loss: 0.12684502548496066 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 430/500 --- Train Loss: 0.7247458261516484 --- Val Loss: 0.15665188673982358 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 440/500 --- Train Loss: 0.7389188947330908 --- Val Loss: 0.21649947317656265 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 450/500 --- Train Loss: 0.6955074800297352 --- Val Loss: 0.12805222660774343 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 460/500 --- Train Loss: 0.7145652748255672 --- Val Loss: 0.21731848512901422 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 470/500 --- Train Loss: 0.5798830311010036 --- Val Loss: 0.0898284078281085 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 480/500 --- Train Loss: 0.4715066008787225 --- Val Loss: 0.19218984947762163 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 490/500 --- Train Loss: 0.7022590966291874 --- Val Loss: 0.21028816327005745 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.8944444444444445\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.302221830691788 --- Val Loss: 2.302336918175789 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/500 --- Train Loss: 2.302572115389213 --- Val Loss: 2.3030053252971667 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 1.5081410604232641 --- Val Loss: 1.5162825185410025 --- Train Acc: 0.34 --- Val Acc: 0.34\n",
      "Epoch 30/500 --- Train Loss: 0.03476148726840835 --- Val Loss: 0.02249903705523755 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 40/500 --- Train Loss: 0.020363885443301016 --- Val Loss: 0.007316906165889983 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/500 --- Train Loss: 0.010927320247277899 --- Val Loss: 0.0007391094673786177 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/500 --- Train Loss: 0.006817206465390019 --- Val Loss: 0.001142983604652769 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/500 --- Train Loss: 0.02657492967747754 --- Val Loss: 0.006098335616527337 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/500 --- Train Loss: 0.009401129822126464 --- Val Loss: 0.0010694359734943527 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/500 --- Train Loss: 0.004688718955455316 --- Val Loss: 0.00021597371725731254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/500 --- Train Loss: 0.007882772336556411 --- Val Loss: 0.0008654272512225318 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/500 --- Train Loss: 0.004907030074034028 --- Val Loss: 0.00017962484468309978 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/500 --- Train Loss: 0.0075269406867978905 --- Val Loss: 0.0036450406472865013 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.0016198419299578255 --- Val Loss: 5.7888091987121545e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.0016112734875916418 --- Val Loss: 1.6685762887783775e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.0016346642914181505 --- Val Loss: 3.506682371750114e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.0009793557223280228 --- Val Loss: 1.2230640602701632e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.00021467377365623057 --- Val Loss: 1.2208462094643568e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/500 --- Train Loss: 0.0014534408007494304 --- Val Loss: 1.8006991575350983e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.0006528952880274223 --- Val Loss: 1.5968437537722843e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/500 --- Train Loss: 0.0017367692347787143 --- Val Loss: 6.472060665997681e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.002107057034889838 --- Val Loss: 0.00037415265069746357 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.00044019653083935843 --- Val Loss: 2.8042404989672745e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.0015781593498663665 --- Val Loss: 3.078114063567908e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.0038911116543609552 --- Val Loss: 2.4157659868496412e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.00017839178109524016 --- Val Loss: 1.5428080345949432e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.0004516491296139876 --- Val Loss: 2.3080743943730806e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.000572968214703311 --- Val Loss: 8.615306175755957e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/500 --- Train Loss: 0.0002795649344654953 --- Val Loss: 8.249466023230055e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.00012036565272944972 --- Val Loss: 4.908663747883716e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.00010009377705153793 --- Val Loss: 5.358360109793546e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.00011114190746757858 --- Val Loss: 4.016024293512538e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.0005705606517848697 --- Val Loss: 4.857301541390356e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/500 --- Train Loss: 0.00017115914520331136 --- Val Loss: 7.277732722058524e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/500 --- Train Loss: 0.00030627095593717326 --- Val Loss: 4.196962350062645e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/500 --- Train Loss: 0.000106032671782387 --- Val Loss: 6.9574011918189244e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.0014071179019663938 --- Val Loss: 5.91843111736438e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.0004762461403701763 --- Val Loss: 6.359613945932767e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/500 --- Train Loss: 0.001545021811128134 --- Val Loss: 1.8717083059697074e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/500 --- Train Loss: 0.002038141935693027 --- Val Loss: 4.607667978593496e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.01446177332374147 --- Val Loss: 6.982353231705882e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/500 --- Train Loss: 0.002039371095641834 --- Val Loss: 1.8390599815470474e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.0004739187692175491 --- Val Loss: 2.632320530014409e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/500 --- Train Loss: 0.0012155582004522354 --- Val Loss: 9.073910500715557e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/500 --- Train Loss: 0.00010347074327367488 --- Val Loss: 2.1450768210124263e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/500 --- Train Loss: 0.003827470088578325 --- Val Loss: 7.904241858438798e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/500 --- Train Loss: 0.009820135186578323 --- Val Loss: 7.346704056334194e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/500 --- Train Loss: 0.00016873924294460893 --- Val Loss: 2.0172516546046167e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/500 --- Train Loss: 4.193106270303984e-05 --- Val Loss: 1.9523848424710723e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/500 --- Train Loss: 4.550613323211267e-05 --- Val Loss: 6.097526203855558e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.9777777777777777\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.301331413117834 --- Val Loss: 2.300523303022929 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/1000 --- Train Loss: 0.5247919270776216 --- Val Loss: 0.43235750035837506 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 20/1000 --- Train Loss: 0.12122915570112458 --- Val Loss: 0.028049937666835827 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 30/1000 --- Train Loss: 0.9600653293641584 --- Val Loss: 0.2732055653553141 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 40/1000 --- Train Loss: 1.6746718498787678 --- Val Loss: 0.8497876652291971 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 50/1000 --- Train Loss: 1.9082836237435035 --- Val Loss: 1.0624674279266464 --- Train Acc: 0.84 --- Val Acc: 0.83\n",
      "Epoch 60/1000 --- Train Loss: 1.796711214277105 --- Val Loss: 1.7908274028624476 --- Train Acc: 0.53 --- Val Acc: 0.51\n",
      "Epoch 70/1000 --- Train Loss: 2.3150501355602566 --- Val Loss: 2.296163232488542 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.2964931345725 --- Val Loss: 2.296753245680728 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/1000 --- Train Loss: 2.3061186694403384 --- Val Loss: 2.2969468264552404 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/1000 --- Train Loss: 2.2984938824591077 --- Val Loss: 2.297119664861072 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.298195265645449 --- Val Loss: 2.2972444485748738 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.312223857062946 --- Val Loss: 2.2970100101602644 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: 2.310243971469789 --- Val Loss: 2.2966208045723167 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: 2.3002272563792103 --- Val Loss: 2.2966456468262244 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: 2.300230607461568 --- Val Loss: 2.29648451783779 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 160/1000 --- Train Loss: 2.312315493612741 --- Val Loss: 2.296578376653416 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: 2.300228625764434 --- Val Loss: 2.296207358760597 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: 2.3102792332392332 --- Val Loss: 2.296228205107139 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 190/1000 --- Train Loss: 2.300429948084646 --- Val Loss: 2.2956645410732857 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: 2.3002259734296406 --- Val Loss: 2.2968413116114834 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/1000 --- Train Loss: 2.3002218262977823 --- Val Loss: 2.296838368726589 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/1000 --- Train Loss: 2.3002215924941227 --- Val Loss: 2.2969321149677633 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 230/1000 --- Train Loss: 2.300224426284446 --- Val Loss: 2.2970141644181985 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/1000 --- Train Loss: 2.3002250371388766 --- Val Loss: 2.29704285880846 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: 2.3002249226377613 --- Val Loss: 2.296695452240199 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 260/1000 --- Train Loss: 2.3002308111829155 --- Val Loss: 2.296480966581871 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.3002234160135777 --- Val Loss: 2.296495758068019 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/1000 --- Train Loss: 2.300221457733054 --- Val Loss: 2.2965814973726415 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: 2.3002235157905355 --- Val Loss: 2.2965805470791874 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: 2.3002250632567254 --- Val Loss: 2.2965762048254006 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: 2.300228915857528 --- Val Loss: 2.29663913029752 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/1000 --- Train Loss: 2.300227388156542 --- Val Loss: 2.2963656242201536 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/1000 --- Train Loss: 2.3002232965301364 --- Val Loss: 2.2965250507419666 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 340/1000 --- Train Loss: 2.300220547951262 --- Val Loss: 2.2965917022534597 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 350/1000 --- Train Loss: 2.3002288494019196 --- Val Loss: 2.296484584465044 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 360/1000 --- Train Loss: 2.3002252204899007 --- Val Loss: 2.2964165107097902 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/1000 --- Train Loss: 2.300222567790604 --- Val Loss: 2.2964480621672787 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/1000 --- Train Loss: 2.3002212689870456 --- Val Loss: 2.2966453902899233 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 390/1000 --- Train Loss: 2.300227212369227 --- Val Loss: 2.297144094546064 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/1000 --- Train Loss: 2.300228774922176 --- Val Loss: 2.296235416529023 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/1000 --- Train Loss: 2.3002233988946665 --- Val Loss: 2.2963400326408596 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: 2.3002187000059875 --- Val Loss: 2.296775714174746 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: 2.300220538790205 --- Val Loss: 2.2968500375522862 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/1000 --- Train Loss: 2.30022027536431 --- Val Loss: 2.297059303408592 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 450/1000 --- Train Loss: 2.3002270455195513 --- Val Loss: 2.2968832066526246 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/1000 --- Train Loss: 2.3002302717511767 --- Val Loss: 2.297076362707428 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 470/1000 --- Train Loss: 2.300221158672288 --- Val Loss: 2.296888078629081 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 480/1000 --- Train Loss: 2.300224241936359 --- Val Loss: 2.297080650556178 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 490/1000 --- Train Loss: 2.300223163965944 --- Val Loss: 2.296957584077086 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 500/1000 --- Train Loss: 2.3002243328235923 --- Val Loss: 2.2965718347444604 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 510/1000 --- Train Loss: 2.3002219661268435 --- Val Loss: 2.2968852142557967 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 520/1000 --- Train Loss: 2.3002208654672516 --- Val Loss: 2.2968286197744376 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 530/1000 --- Train Loss: 2.3002233249926127 --- Val Loss: 2.29655218632269 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 540/1000 --- Train Loss: 2.300230729627532 --- Val Loss: 2.296562973137658 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 550/1000 --- Train Loss: 2.300218745049295 --- Val Loss: 2.2967166492796647 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: 2.3002239321948075 --- Val Loss: 2.296823990531328 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: 2.3002224399270483 --- Val Loss: 2.2963715644308036 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 580/1000 --- Train Loss: 2.3002246495383374 --- Val Loss: 2.2971251408286673 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 590/1000 --- Train Loss: 2.3002202581180735 --- Val Loss: 2.2967659943073846 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 600/1000 --- Train Loss: 2.3002241082593016 --- Val Loss: 2.296885605657634 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 610/1000 --- Train Loss: 2.300221086374412 --- Val Loss: 2.2967660259024956 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 620/1000 --- Train Loss: 2.3002253660000505 --- Val Loss: 2.2968029362309355 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 630/1000 --- Train Loss: 2.3002253270890565 --- Val Loss: 2.296420780599492 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 640/1000 --- Train Loss: 2.3002317808661057 --- Val Loss: 2.2968990040041324 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 650/1000 --- Train Loss: 2.3002237259113874 --- Val Loss: 2.2971614903952693 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: 2.300229449236324 --- Val Loss: 2.2965967602780157 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: 2.3002207178061487 --- Val Loss: 2.2966226787081765 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: 2.3002229694504566 --- Val Loss: 2.2967736436224557 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: 2.3002259707042545 --- Val Loss: 2.2966531816072693 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.300219727241603 --- Val Loss: 2.2967803462420084 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.300224897371989 --- Val Loss: 2.2965212210987986 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.3002231933606265 --- Val Loss: 2.2970711104526718 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: 2.3002223523868746 --- Val Loss: 2.296377598168504 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 740/1000 --- Train Loss: 2.3002231319597968 --- Val Loss: 2.296828912551583 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: 2.3002204337818526 --- Val Loss: 2.2967807020350133 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: 2.3002235509710394 --- Val Loss: 2.2965457942406178 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: 2.3002190127116218 --- Val Loss: 2.296670457782775 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 780/1000 --- Train Loss: 2.3002272742836194 --- Val Loss: 2.2969526463481547 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 790/1000 --- Train Loss: 2.300224146963425 --- Val Loss: 2.296783960262582 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.300229006097944 --- Val Loss: 2.297027290624727 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.3002277374675963 --- Val Loss: 2.2963521491471606 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: 2.300229203427028 --- Val Loss: 2.2969269963607366 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: 2.3002326735635 --- Val Loss: 2.296735239690776 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: 2.3002366121921645 --- Val Loss: 2.297429581424247 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: 2.300221627402122 --- Val Loss: 2.297026938662892 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.300228233704499 --- Val Loss: 2.2970946936281447 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: 2.300219581463479 --- Val Loss: 2.2964444524772696 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: 2.300227162110673 --- Val Loss: 2.296641943591214 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: 2.3002199886584624 --- Val Loss: 2.296875086515745 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: 2.3002189921083236 --- Val Loss: 2.296678069744341 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: 2.3002216651386447 --- Val Loss: 2.296664267290803 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.300220302633132 --- Val Loss: 2.2970270896743883 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.300221674762306 --- Val Loss: 2.2966695493065767 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: 2.300222008314409 --- Val Loss: 2.2965561922698847 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: 2.30021904411374 --- Val Loss: 2.296760584215123 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.3002333027773747 --- Val Loss: 2.2968117138134843 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.300227059218565 --- Val Loss: 2.297155070836063 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.300218203578984 --- Val Loss: 2.2967374438233468 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: 2.3002298675806028 --- Val Loss: 2.2968569509327477 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.3005767667505204 --- Val Loss: 2.3041105124742653 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.296429954369257 --- Val Loss: 2.3027117419406244 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 20/100 --- Train Loss: 2.3137216613684717 --- Val Loss: 2.3033874099797242 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.312769160060896 --- Val Loss: 2.3053698538678495 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/100 --- Train Loss: 2.3130422277681317 --- Val Loss: 2.304830416209625 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.5752080729643003 --- Val Loss: 2.23398854038162 --- Train Acc: 0.43 --- Val Acc: 0.45\n",
      "Epoch 10/1000 --- Train Loss: 2.30340245640005 --- Val Loss: 2.3043379540161713 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.3027841965990663 --- Val Loss: 2.2991753407150046 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.31051717634691 --- Val Loss: 2.307915825528596 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.312525548837551 --- Val Loss: 2.295642175428643 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.3069414620392195 --- Val Loss: 2.3032834160912383 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: 2.3042381836811323 --- Val Loss: 2.293579933149408 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.303660231060502 --- Val Loss: 2.300680198155558 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.313518366968209 --- Val Loss: 2.298242962798205 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.3162790858808133 --- Val Loss: 2.312664487405863 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.3030806031714275 --- Val Loss: 2.295611466629785 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.3153600531533436 --- Val Loss: 2.300890950032165 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3082511798235084 --- Val Loss: 2.305401596522714 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.305043252760684 --- Val Loss: 2.294809062548845 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.312851551837343 --- Val Loss: 2.301440132732109 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.315259381935557 --- Val Loss: 2.304483800182837 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.30964034499951 --- Val Loss: 2.295404236152895 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.3114400938073545 --- Val Loss: 2.3028160063111156 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3139140334682735 --- Val Loss: 2.298297906391859 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.3143612650694267 --- Val Loss: 2.313582454532156 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.3090353726212007 --- Val Loss: 2.301976937679243 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.309939387906963 --- Val Loss: 2.311104068956096 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.302912376767578 --- Val Loss: 2.292149742494551 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.3086115402745038 --- Val Loss: 2.3026484896052017 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.3131796250684977 --- Val Loss: 2.303251451802966 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3108277411342977 --- Val Loss: 2.30760696797695 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.306271767428211 --- Val Loss: 2.2967843804598305 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.303381836492276 --- Val Loss: 2.2967264006459795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.312470996858639 --- Val Loss: 2.311709631513646 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: 2.3046267656542336 --- Val Loss: 2.290077086454098 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.329278882336682 --- Val Loss: 2.334225687270093 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.301691735825572 --- Val Loss: 2.2924148307150887 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.306443310110997 --- Val Loss: 2.300465570364496 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.308096163031702 --- Val Loss: 2.3029348812527175 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3135040503058244 --- Val Loss: 2.304393141733189 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.307841538486579 --- Val Loss: 2.2992424267030342 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.3102830652741506 --- Val Loss: 2.2978151256867467 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3061722745673525 --- Val Loss: 2.2964070570562507 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3081054364727804 --- Val Loss: 2.292202541532666 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3071943767273813 --- Val Loss: 2.3013480061663216 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: 2.3102444034477023 --- Val Loss: 2.302746502833938 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.307247002904628 --- Val Loss: 2.3029286991876368 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.3103235189617695 --- Val Loss: 2.3043978189028116 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: 2.306411049880091 --- Val Loss: 2.3073960020908286 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3038200802975415 --- Val Loss: 2.302838058039875 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3077582366473663 --- Val Loss: 2.3096920892358765 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3162939813149994 --- Val Loss: 2.3204955731749872 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.3119453281490574 --- Val Loss: 2.319300156248146 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.3129093982958606 --- Val Loss: 2.3053467551535705 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: 2.309319507113797 --- Val Loss: 2.3156473819497667 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3193570726584505 --- Val Loss: 2.324230792582095 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 510/1000 --- Train Loss: 2.317128784585414 --- Val Loss: 2.313922204206917 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.3145502028757585 --- Val Loss: 2.304782291954294 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.312306003068619 --- Val Loss: 2.3064660952756872 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.301940715950591 --- Val Loss: 2.301815109736457 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/500 --- Train Loss: 2.3001128270702957 --- Val Loss: 2.2995284240076916 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.300004677853725 --- Val Loss: 2.2992876653978667 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: 2.2999952272208692 --- Val Loss: 2.29923847011952 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.2999945428148227 --- Val Loss: 2.2993505854072565 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/500 --- Train Loss: 2.2999911111261757 --- Val Loss: 2.299307671342952 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/500 --- Train Loss: 2.2999877262146082 --- Val Loss: 2.299315413706644 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: 2.2999796173508176 --- Val Loss: 2.2991475255460467 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/500 --- Train Loss: 2.2999620359080226 --- Val Loss: 2.299162484025774 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/500 --- Train Loss: 2.29991896907552 --- Val Loss: 2.299332803732462 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/500 --- Train Loss: 2.2997777937830453 --- Val Loss: 2.2991254264477416 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/500 --- Train Loss: 2.299035771117139 --- Val Loss: 2.2984431453077616 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.2791052560347445 --- Val Loss: 2.278012011692703 --- Train Acc: 0.19 --- Val Acc: 0.19\n",
      "Epoch 130/500 --- Train Loss: 1.731510282254465 --- Val Loss: 1.7307024517603964 --- Train Acc: 0.31 --- Val Acc: 0.31\n",
      "Epoch 140/500 --- Train Loss: 0.7377990179794943 --- Val Loss: 0.7252883136320005 --- Train Acc: 0.75 --- Val Acc: 0.74\n",
      "Epoch 150/500 --- Train Loss: 0.3016714889396129 --- Val Loss: 0.30041980967705434 --- Train Acc: 0.93 --- Val Acc: 0.90\n",
      "Epoch 160/500 --- Train Loss: 0.12654009713949682 --- Val Loss: 0.1085895724539461 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 170/500 --- Train Loss: 0.0712008700036625 --- Val Loss: 0.0510762306200178 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 180/500 --- Train Loss: 0.034747452951757025 --- Val Loss: 0.025580042534341093 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.026031592201567633 --- Val Loss: 0.011861080049441183 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/500 --- Train Loss: 0.013592078677789339 --- Val Loss: 0.006780234238096667 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.015116655609498477 --- Val Loss: 0.004220706007106406 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.054675699828324584 --- Val Loss: 0.05139197701536875 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.009659335305505964 --- Val Loss: 0.0008844354221287551 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.005307817877227362 --- Val Loss: 0.0004567013122398131 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.01681751610460483 --- Val Loss: 0.0004395871852129258 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.0072756201950498756 --- Val Loss: 0.0005016049997917024 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.019391850174168467 --- Val Loss: 0.0002155068813066636 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/500 --- Train Loss: 0.029971978789574426 --- Val Loss: 5.373267899184369e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.022293657967362414 --- Val Loss: 0.0004665633160346949 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.041345918557077345 --- Val Loss: 0.0007581898158134014 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.025866253393054744 --- Val Loss: 0.003543401383553519 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.24831528423916818 --- Val Loss: 0.1336359534825384 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 330/500 --- Train Loss: 0.6852174030880829 --- Val Loss: 0.5212697827078043 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 340/500 --- Train Loss: 0.27620636445137603 --- Val Loss: 0.15397774988999086 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 350/500 --- Train Loss: 0.7746006593222758 --- Val Loss: 0.739632702294563 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 360/500 --- Train Loss: 0.15791714219493608 --- Val Loss: 0.11232132335163082 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 370/500 --- Train Loss: 0.3431400726681281 --- Val Loss: 0.14958572211118787 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 380/500 --- Train Loss: 2.44792752770365 --- Val Loss: 1.4601760089865532 --- Train Acc: 0.87 --- Val Acc: 0.91\n",
      "Epoch 390/500 --- Train Loss: 0.258653176150383 --- Val Loss: 0.05616285968229567 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.5606295104681203 --- Val Loss: 0.2808031583790697 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 410/500 --- Train Loss: 0.11212598209362803 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.2685595745130124 --- Val Loss: 0.1684819350274438 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 430/500 --- Train Loss: 0.7258682774977383 --- Val Loss: 0.5731544515662383 --- Train Acc: 0.87 --- Val Acc: 0.86\n",
      "Epoch 440/500 --- Train Loss: 0.658445984251929 --- Val Loss: 0.6701223415347737 --- Train Acc: 0.82 --- Val Acc: 0.79\n",
      "Epoch 450/500 --- Train Loss: 0.8329068908856099 --- Val Loss: 0.5973803238925063 --- Train Acc: 0.82 --- Val Acc: 0.79\n",
      "Epoch 460/500 --- Train Loss: 0.6166030488430359 --- Val Loss: 0.6359280012788994 --- Train Acc: 0.79 --- Val Acc: 0.74\n",
      "Epoch 470/500 --- Train Loss: 1.1131058907149276 --- Val Loss: 0.7615414345608011 --- Train Acc: 0.73 --- Val Acc: 0.71\n",
      "Epoch 480/500 --- Train Loss: 0.9984139464823882 --- Val Loss: 0.9247829789642954 --- Train Acc: 0.69 --- Val Acc: 0.67\n",
      "Epoch 490/500 --- Train Loss: 1.7248016113716704 --- Val Loss: 1.743767767316815 --- Train Acc: 0.32 --- Val Acc: 0.34\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.25277777777777777\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3023876126522365 --- Val Loss: 2.30239117986405 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 0.045247569140519486 --- Val Loss: 0.021962044839737474 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 20/100 --- Train Loss: 0.014114573604819417 --- Val Loss: 0.00018937133035027597 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 30/100 --- Train Loss: 0.08892903569561789 --- Val Loss: 0.0038566734467904346 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 40/100 --- Train Loss: 0.5709357103604998 --- Val Loss: 0.2246450128341635 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 50/100 --- Train Loss: 1.501785069175382 --- Val Loss: 0.9547304984888253 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 60/100 --- Train Loss: 2.298861696652973 --- Val Loss: 2.302260702965739 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.3120514571374793 --- Val Loss: 2.3009931760720135 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: 2.302081718483295 --- Val Loss: 2.301149508888411 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3190278355048672 --- Val Loss: 2.3008566564869546 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09444444444444444\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.301941827967292 --- Val Loss: 2.3029696815740457 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 0.3271154681684309 --- Val Loss: 0.18372000198948327 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 20/100 --- Train Loss: 0.22925972824623198 --- Val Loss: 0.056233262978332685 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 30/100 --- Train Loss: 0.36543625889697484 --- Val Loss: 0.12732100302039265 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 40/100 --- Train Loss: 1.0500641370054076 --- Val Loss: 0.6035927844074641 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 50/100 --- Train Loss: 1.3946264065380005 --- Val Loss: 1.3359960518922456 --- Train Acc: 0.52 --- Val Acc: 0.51\n",
      "Epoch 60/100 --- Train Loss: 1.7314082989598958 --- Val Loss: 1.790187722694842 --- Train Acc: 0.34 --- Val Acc: 0.32\n",
      "Epoch 70/100 --- Train Loss: 2.137303111824301 --- Val Loss: 2.102038082728582 --- Train Acc: 0.19 --- Val Acc: 0.18\n",
      "Epoch 80/100 --- Train Loss: 2.266386643701638 --- Val Loss: 2.2752771079788827 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 90/100 --- Train Loss: 2.2993020791823287 --- Val Loss: 2.306129657227405 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.30212782758597 --- Val Loss: 2.3026018057076363 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 10/500 --- Train Loss: 0.3440834063842424 --- Val Loss: 0.2185300432935592 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 20/500 --- Train Loss: 0.5954493736671304 --- Val Loss: 0.10584865535366401 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Epoch 30/500 --- Train Loss: 1.422478990597817 --- Val Loss: 0.1958005353204815 --- Train Acc: 0.93 --- Val Acc: 0.97\n",
      "Epoch 40/500 --- Train Loss: 1.15901971702631 --- Val Loss: 0.1521069265740595 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 50/500 --- Train Loss: 1.3880445356979763 --- Val Loss: 0.3317683256111542 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 60/500 --- Train Loss: 1.2265503889241216 --- Val Loss: 0.11901302568627042 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 70/500 --- Train Loss: 1.0881396529075154 --- Val Loss: 0.2782273769712706 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 80/500 --- Train Loss: 1.199621167174534 --- Val Loss: 0.37780524000553767 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Epoch 90/500 --- Train Loss: 0.9231565992193187 --- Val Loss: 0.3133115882562096 --- Train Acc: 0.87 --- Val Acc: 0.89\n",
      "Epoch 100/500 --- Train Loss: 1.3955353084745041 --- Val Loss: 0.953036886831187 --- Train Acc: 0.63 --- Val Acc: 0.67\n",
      "Epoch 110/500 --- Train Loss: 1.3373710706366728 --- Val Loss: 1.0522096959364675 --- Train Acc: 0.59 --- Val Acc: 0.59\n",
      "Epoch 120/500 --- Train Loss: 1.5936682870679226 --- Val Loss: 1.3983175451072396 --- Train Acc: 0.48 --- Val Acc: 0.46\n",
      "Epoch 130/500 --- Train Loss: 1.5789569847225111 --- Val Loss: 1.553501634973432 --- Train Acc: 0.44 --- Val Acc: 0.41\n",
      "Epoch 140/500 --- Train Loss: 1.8330886257624692 --- Val Loss: 1.627364093071682 --- Train Acc: 0.36 --- Val Acc: 0.36\n",
      "Epoch 150/500 --- Train Loss: 1.8633352026857701 --- Val Loss: 1.8315269394072866 --- Train Acc: 0.29 --- Val Acc: 0.26\n",
      "Epoch 160/500 --- Train Loss: 1.8159890070920748 --- Val Loss: 1.869217128057033 --- Train Acc: 0.28 --- Val Acc: 0.25\n",
      "Epoch 170/500 --- Train Loss: 2.19586970094673 --- Val Loss: 2.2352072081657126 --- Train Acc: 0.14 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.140340327909687 --- Val Loss: 2.190002217190156 --- Train Acc: 0.16 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.1788529559713443 --- Val Loss: 2.2396150302395545 --- Train Acc: 0.14 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.251750765797911 --- Val Loss: 2.2700300681958705 --- Train Acc: 0.12 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: 2.275280906585637 --- Val Loss: 2.287307661093786 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 220/500 --- Train Loss: 2.290838315754965 --- Val Loss: 2.3037098587679483 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 230/500 --- Train Loss: 2.2704573186307595 --- Val Loss: 2.303404962631115 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 240/500 --- Train Loss: 2.2619743716679523 --- Val Loss: 2.2950249123401285 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 250/500 --- Train Loss: 2.2828150721493827 --- Val Loss: 2.303732527173161 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 260/500 --- Train Loss: 2.2887300880260697 --- Val Loss: 2.3035577836635266 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 270/500 --- Train Loss: 2.245701340027657 --- Val Loss: 2.2383689591181564 --- Train Acc: 0.13 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.307296541949217 --- Val Loss: 2.3046033963915153 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 290/500 --- Train Loss: 2.2972234909091593 --- Val Loss: 2.304033440632556 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 300/500 --- Train Loss: 2.3011564564624667 --- Val Loss: 2.3041832003087066 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 310/500 --- Train Loss: 2.299174889406778 --- Val Loss: 2.30403349245492 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 320/500 --- Train Loss: 2.301156185636599 --- Val Loss: 2.304093901354487 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 330/500 --- Train Loss: 2.2991253714854722 --- Val Loss: 2.304071330312927 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 340/500 --- Train Loss: 2.2930618747591622 --- Val Loss: 2.303989085842929 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 350/500 --- Train Loss: 2.2991184535045726 --- Val Loss: 2.3039702812190517 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 360/500 --- Train Loss: 2.2972493809097307 --- Val Loss: 2.3040478287218034 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 370/500 --- Train Loss: 2.2991869809266094 --- Val Loss: 2.304197331709018 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 380/500 --- Train Loss: 2.299188863124468 --- Val Loss: 2.304006412464775 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 390/500 --- Train Loss: 2.30115687922358 --- Val Loss: 2.3040970721093306 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 400/500 --- Train Loss: 2.2992043544421064 --- Val Loss: 2.3040093422100654 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 410/500 --- Train Loss: 2.297233110899683 --- Val Loss: 2.303960627437774 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 420/500 --- Train Loss: 2.2932490117419277 --- Val Loss: 2.304006803957451 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 430/500 --- Train Loss: 2.291185384666605 --- Val Loss: 2.3039912907293347 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 440/500 --- Train Loss: 2.293293785960293 --- Val Loss: 2.3038511250832943 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 450/500 --- Train Loss: 2.301156626533102 --- Val Loss: 2.304044155029538 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 460/500 --- Train Loss: 2.297249876977549 --- Val Loss: 2.304090716545551 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 470/500 --- Train Loss: 2.293286236346208 --- Val Loss: 2.304130381241619 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 480/500 --- Train Loss: 2.2992045387356748 --- Val Loss: 2.3040182634132154 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 490/500 --- Train Loss: 2.301161888536531 --- Val Loss: 2.3038984076952986 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3247224880140824 --- Val Loss: 2.3280607964036064 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: 2.3617517790059295 --- Val Loss: 2.340825418877483 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.33220145636272 --- Val Loss: 2.349845685870402 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/500 --- Train Loss: 2.3521568130652013 --- Val Loss: 2.358586503168645 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 40/500 --- Train Loss: 2.3935948821537196 --- Val Loss: 2.4280152287025634 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: 2.3902987107803004 --- Val Loss: 2.4175614332032285 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 60/500 --- Train Loss: 2.341390715815036 --- Val Loss: 2.3644910838085798 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3827725760587195 --- Val Loss: 2.3343537186470096 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/500 --- Train Loss: 2.334529760174647 --- Val Loss: 2.331781201249714 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/500 --- Train Loss: 2.367723037535881 --- Val Loss: 2.3345341924946372 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 100/500 --- Train Loss: 2.3327665136988753 --- Val Loss: 2.3431373871372783 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: 2.3256198806713457 --- Val Loss: 2.323685258696802 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/500 --- Train Loss: 2.38204598270632 --- Val Loss: 2.3877228934652956 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.3590702305360987 --- Val Loss: 2.325746091857614 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3505185636437456 --- Val Loss: 2.3506702295929145 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/500 --- Train Loss: 2.41267741770492 --- Val Loss: 2.42829250696364 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 160/500 --- Train Loss: 2.353621405405398 --- Val Loss: 2.3713932473050616 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: 2.3577148855854286 --- Val Loss: 2.354470937874655 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: 2.3408675576050006 --- Val Loss: 2.3300290346914094 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: 2.360597307731667 --- Val Loss: 2.3742075711444373 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: 2.3830105865277407 --- Val Loss: 2.3849888485092583 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: 2.33806219144596 --- Val Loss: 2.311530340246008 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/500 --- Train Loss: 2.3753313461230374 --- Val Loss: 2.335206633409062 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.321455882163785 --- Val Loss: 2.3287567626689167 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 240/500 --- Train Loss: 2.3470471399968704 --- Val Loss: 2.322259351018816 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/500 --- Train Loss: 2.367107723048503 --- Val Loss: 2.416413945052563 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 260/500 --- Train Loss: 2.4075381763000014 --- Val Loss: 2.401481402585526 --- Train Acc: 0.10 --- Val Acc: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_21800\\2185905466.py:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  return input_data * self.mask\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3015670628039193 --- Val Loss: 2.2993778599509045 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.3221835777972695 --- Val Loss: 2.237375344932319 --- Train Acc: 0.12 --- Val Acc: 0.15\n",
      "Epoch 20/100 --- Train Loss: 2.3213842871098636 --- Val Loss: 2.2777973921265664 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 30/100 --- Train Loss: 2.3019418648764582 --- Val Loss: 2.29956982054536 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3018906842943925 --- Val Loss: 2.2986707648615763 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 50/100 --- Train Loss: 2.3018626399810604 --- Val Loss: 2.2989773997307488 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.301843894489345 --- Val Loss: 2.298944467598783 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.3020018315357227 --- Val Loss: 2.299384272900986 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: 2.301852676538306 --- Val Loss: 2.298887587575894 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3018564480265975 --- Val Loss: 2.2985591524570363 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3029041995341455 --- Val Loss: 2.3025611039933698 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 0.9880836309547848 --- Val Loss: 0.887796755593949 --- Train Acc: 0.74 --- Val Acc: 0.71\n",
      "Epoch 20/500 --- Train Loss: 2.526287129261762 --- Val Loss: 2.4047068840024903 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: 2.3735362875125694 --- Val Loss: 2.2929876139893186 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/500 --- Train Loss: 2.299449434411721 --- Val Loss: 2.2962467003639357 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/500 --- Train Loss: 2.301883068079861 --- Val Loss: 2.295292111931687 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/500 --- Train Loss: 2.301847309889492 --- Val Loss: 2.295956772651925 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 70/500 --- Train Loss: 2.301409089900661 --- Val Loss: 2.2958812726087614 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/500 --- Train Loss: 2.314589884611893 --- Val Loss: 2.2970780882776967 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/500 --- Train Loss: 2.3020029344585047 --- Val Loss: 2.2932489636255355 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/500 --- Train Loss: 2.301885426176465 --- Val Loss: 2.294898301112249 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/500 --- Train Loss: 2.3010661990734493 --- Val Loss: 2.294416708976121 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/500 --- Train Loss: 2.3016607188856306 --- Val Loss: 2.297628124519455 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.3010763215762244 --- Val Loss: 2.2953932193738353 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/500 --- Train Loss: 2.3014126415203697 --- Val Loss: 2.295541155640451 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/500 --- Train Loss: 2.301941806615904 --- Val Loss: 2.295675777507001 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 160/500 --- Train Loss: 2.3003240353040746 --- Val Loss: 2.2928193710627873 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 170/500 --- Train Loss: 2.2989868586406184 --- Val Loss: 2.2980339620346837 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/500 --- Train Loss: 2.3005211406473065 --- Val Loss: 2.2954274176299085 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 190/500 --- Train Loss: 2.30014785841651 --- Val Loss: 2.2971855201318574 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: 2.302742067497524 --- Val Loss: 2.2972623301457062 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/500 --- Train Loss: 2.2998032273782285 --- Val Loss: 2.2965742530025635 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/500 --- Train Loss: 2.299791438618704 --- Val Loss: 2.2952333609347875 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.300561874933161 --- Val Loss: 2.2971697827476185 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/500 --- Train Loss: 2.3007311516268887 --- Val Loss: 2.295876281583591 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/500 --- Train Loss: 2.300032140942952 --- Val Loss: 2.295785395092606 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 260/500 --- Train Loss: 2.3020330841219008 --- Val Loss: 2.2969879355678775 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/500 --- Train Loss: 2.3003644320675267 --- Val Loss: 2.298077833442956 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/500 --- Train Loss: 2.2989394275068107 --- Val Loss: 2.295079118560379 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/500 --- Train Loss: 2.2990996161780943 --- Val Loss: 2.2954513677418236 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/500 --- Train Loss: 2.299961895918212 --- Val Loss: 2.2945378572425845 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/500 --- Train Loss: 2.299600148117818 --- Val Loss: 2.296966338859187 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/500 --- Train Loss: 2.2971891467388907 --- Val Loss: 2.295400978571472 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/500 --- Train Loss: 2.299143629598972 --- Val Loss: 2.296191854591773 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 340/500 --- Train Loss: 2.301030898269961 --- Val Loss: 2.296253768398748 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 350/500 --- Train Loss: 2.2948302080263088 --- Val Loss: 2.296877410661288 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.29994480442917 --- Val Loss: 2.294637363291325 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/500 --- Train Loss: 2.300351517066653 --- Val Loss: 2.2933636295932036 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/500 --- Train Loss: 2.299536173821919 --- Val Loss: 2.297904729414321 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.2973918192974927 --- Val Loss: 2.2965834957953115 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/500 --- Train Loss: 2.2953104505793274 --- Val Loss: 2.295341218018394 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/500 --- Train Loss: 2.297257373080803 --- Val Loss: 2.294353590589245 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/500 --- Train Loss: 2.301072075743329 --- Val Loss: 2.296406322228213 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/500 --- Train Loss: 2.2961904499618613 --- Val Loss: 2.2961880594475725 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/500 --- Train Loss: 2.296896419169659 --- Val Loss: 2.297713393357412 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 450/500 --- Train Loss: 2.2983097187225567 --- Val Loss: 2.2953683080805747 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/500 --- Train Loss: 2.301618824884767 --- Val Loss: 2.294829267731653 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 470/500 --- Train Loss: 2.296199908361519 --- Val Loss: 2.296284040827372 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 480/500 --- Train Loss: 2.297038637949104 --- Val Loss: 2.2988445653159126 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 490/500 --- Train Loss: 2.2979182099846054 --- Val Loss: 2.2970971575482326 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3023594853013747 --- Val Loss: 2.3021261937040562 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/1000 --- Train Loss: 2.3016652141846294 --- Val Loss: 2.300052213625987 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/1000 --- Train Loss: 2.3016201987998195 --- Val Loss: 2.299634615569156 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/1000 --- Train Loss: 2.301617849122199 --- Val Loss: 2.2995766575154724 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/1000 --- Train Loss: 2.3016163631268713 --- Val Loss: 2.2995350865487887 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: 2.301614054175657 --- Val Loss: 2.2995636392884764 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.3016099091782904 --- Val Loss: 2.2995447587406073 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: 2.3016038399765213 --- Val Loss: 2.2996015054650756 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/1000 --- Train Loss: 2.3015911847014845 --- Val Loss: 2.299513856725721 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.3015600664414104 --- Val Loss: 2.299530442018765 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/1000 --- Train Loss: 2.301468885295269 --- Val Loss: 2.2995131236638144 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: 2.301070775915906 --- Val Loss: 2.299013105173864 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/1000 --- Train Loss: 2.2965834029607612 --- Val Loss: 2.294280822406223 --- Train Acc: 0.21 --- Val Acc: 0.24\n",
      "Epoch 130/1000 --- Train Loss: 1.8831491648980014 --- Val Loss: 1.868417127776864 --- Train Acc: 0.34 --- Val Acc: 0.33\n",
      "Epoch 140/1000 --- Train Loss: 1.1837478868140832 --- Val Loss: 1.2029046248897597 --- Train Acc: 0.52 --- Val Acc: 0.51\n",
      "Epoch 150/1000 --- Train Loss: 0.5959462684189025 --- Val Loss: 0.6007326835899525 --- Train Acc: 0.82 --- Val Acc: 0.82\n",
      "Epoch 160/1000 --- Train Loss: 0.24306469615226148 --- Val Loss: 0.2575757079678177 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 170/1000 --- Train Loss: 0.12103048740963815 --- Val Loss: 0.11462500666523653 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 180/1000 --- Train Loss: 0.06288465426485505 --- Val Loss: 0.06090332023325525 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 190/1000 --- Train Loss: 0.04204922718348218 --- Val Loss: 0.035804450184802515 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 200/1000 --- Train Loss: 0.031050845224567695 --- Val Loss: 0.0228120536364211 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/1000 --- Train Loss: 0.02035633058377148 --- Val Loss: 0.014027779196244766 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/1000 --- Train Loss: 0.021093830005231153 --- Val Loss: 0.007457274647987969 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/1000 --- Train Loss: 0.009056137650512011 --- Val Loss: 0.004406525469558801 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.0178328984579659 --- Val Loss: 0.0020691837951729115 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.011574745367621082 --- Val Loss: 0.0013626871534837038 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.009179475597743136 --- Val Loss: 0.0007452416727609912 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.00609366201694261 --- Val Loss: 0.0004954540172861377 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.013105007479022353 --- Val Loss: 0.00020500938741528047 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.01755900146628491 --- Val Loss: 0.00013630425968366592 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.010950475155440295 --- Val Loss: 0.0006226984326286864 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.01249185524673758 --- Val Loss: 0.00027653139790956657 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.011764052570768844 --- Val Loss: 3.5141966748591636e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.01842204565581157 --- Val Loss: 3.7990111785358416e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.06441575678512965 --- Val Loss: 0.0002997903494764759 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.28853571855880367 --- Val Loss: 0.1283078082347762 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 360/1000 --- Train Loss: 0.03825401926858251 --- Val Loss: 6.079045932271632e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.0570824030751391 --- Val Loss: 6.722204055186586e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.09370908648758586 --- Val Loss: 0.038341231731262776 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.05186067142347234 --- Val Loss: 0.002346100776813849 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.37565560102888823 --- Val Loss: 0.002279541823190889 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.37770866366797046 --- Val Loss: 0.05616780170665742 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.09766376026404466 --- Val Loss: 1.4998109291282397e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.1548718105951184 --- Val Loss: 0.12729784743825562 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 440/1000 --- Train Loss: 1.0361355774723282 --- Val Loss: 1.1583611740878197 --- Train Acc: 0.87 --- Val Acc: 0.84\n",
      "Epoch 450/1000 --- Train Loss: 0.38002728537991703 --- Val Loss: 0.16786312702913467 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 460/1000 --- Train Loss: 0.4592550097600374 --- Val Loss: 0.3060851516123461 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 470/1000 --- Train Loss: 0.2295970727646774 --- Val Loss: 0.0765303042301187 --- Train Acc: 0.96 --- Val Acc: 0.98\n",
      "Epoch 480/1000 --- Train Loss: 0.48553899908477866 --- Val Loss: 0.36948404068259316 --- Train Acc: 0.91 --- Val Acc: 0.92\n",
      "Epoch 490/1000 --- Train Loss: 0.7339010480093577 --- Val Loss: 0.6463941337307446 --- Train Acc: 0.81 --- Val Acc: 0.78\n",
      "Epoch 500/1000 --- Train Loss: 1.1058243425072725 --- Val Loss: 1.097750392151609 --- Train Acc: 0.58 --- Val Acc: 0.56\n",
      "Epoch 510/1000 --- Train Loss: 1.164874400456109 --- Val Loss: 1.1644608265661978 --- Train Acc: 0.56 --- Val Acc: 0.55\n",
      "Epoch 520/1000 --- Train Loss: 1.2953749047101706 --- Val Loss: 1.3631163887585005 --- Train Acc: 0.49 --- Val Acc: 0.47\n",
      "Epoch 530/1000 --- Train Loss: 1.3697952117401762 --- Val Loss: 1.3800030866968829 --- Train Acc: 0.48 --- Val Acc: 0.47\n",
      "Epoch 540/1000 --- Train Loss: 1.3390343239018436 --- Val Loss: 1.3059628153892697 --- Train Acc: 0.48 --- Val Acc: 0.48\n",
      "Epoch 550/1000 --- Train Loss: 1.8752857468344637 --- Val Loss: 1.9523515439809893 --- Train Acc: 0.27 --- Val Acc: 0.27\n",
      "Epoch 560/1000 --- Train Loss: 1.8236479236373164 --- Val Loss: 1.9046353373614415 --- Train Acc: 0.27 --- Val Acc: 0.28\n",
      "Epoch 570/1000 --- Train Loss: 1.908098107047212 --- Val Loss: 1.8891792085324217 --- Train Acc: 0.26 --- Val Acc: 0.27\n",
      "Epoch 580/1000 --- Train Loss: 1.9986338999929791 --- Val Loss: 2.1496932098210464 --- Train Acc: 0.19 --- Val Acc: 0.20\n",
      "Epoch 590/1000 --- Train Loss: 2.058435841853249 --- Val Loss: 2.102234807987581 --- Train Acc: 0.19 --- Val Acc: 0.20\n",
      "Epoch 600/1000 --- Train Loss: 2.215647383576262 --- Val Loss: 2.2947456580335692 --- Train Acc: 0.14 --- Val Acc: 0.14\n",
      "Epoch 610/1000 --- Train Loss: 2.0065921207660655 --- Val Loss: 2.0434888709793633 --- Train Acc: 0.21 --- Val Acc: 0.21\n",
      "Epoch 620/1000 --- Train Loss: 2.2177440960842687 --- Val Loss: 2.277948267284919 --- Train Acc: 0.14 --- Val Acc: 0.14\n",
      "Epoch 630/1000 --- Train Loss: 2.0714651652668366 --- Val Loss: 2.141134290981308 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 640/1000 --- Train Loss: 2.289811053775726 --- Val Loss: 2.2977512164027964 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 650/1000 --- Train Loss: 2.263416794259414 --- Val Loss: 2.2867891686664894 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: 2.2823753734914165 --- Val Loss: 2.293480226594751 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 670/1000 --- Train Loss: 2.2857423060540785 --- Val Loss: 2.292334960204863 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.271481435015602 --- Val Loss: 2.292227787904038 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 690/1000 --- Train Loss: 2.27538980001981 --- Val Loss: 2.284410858541472 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.285807635235525 --- Val Loss: 2.284351498742892 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.297786030069132 --- Val Loss: 2.284335924770257 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.285670185076566 --- Val Loss: 2.2841920513880862 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: 2.282870082871885 --- Val Loss: 2.2996913765701015 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 740/1000 --- Train Loss: 2.2908432752469525 --- Val Loss: 2.299717204071008 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: 2.297672878890195 --- Val Loss: 2.2997707576240303 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 760/1000 --- Train Loss: 2.295646006632303 --- Val Loss: 2.299516953541502 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.30162261890476 --- Val Loss: 2.299518733832773 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: 2.297616831807204 --- Val Loss: 2.299520742013227 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 790/1000 --- Train Loss: 2.291635455540962 --- Val Loss: 2.2994617860230737 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 800/1000 --- Train Loss: 2.3016204044504347 --- Val Loss: 2.2995601406674924 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: 2.301621461015549 --- Val Loss: 2.2994270621200807 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 820/1000 --- Train Loss: 2.299590545692564 --- Val Loss: 2.2994514678275415 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 830/1000 --- Train Loss: 2.2956338885173144 --- Val Loss: 2.2994759216032024 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: 2.299590944224713 --- Val Loss: 2.299413391117885 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.3116588339187323 --- Val Loss: 2.2994061692619523 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 860/1000 --- Train Loss: 2.2995919826263136 --- Val Loss: 2.2994410282460125 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 870/1000 --- Train Loss: 2.2996385285144574 --- Val Loss: 2.2994225113963895 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 880/1000 --- Train Loss: 2.2916822300482678 --- Val Loss: 2.2994417318463753 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 890/1000 --- Train Loss: 2.3016194611354077 --- Val Loss: 2.2995060992064547 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: 2.2995904034968557 --- Val Loss: 2.2994884412929446 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 910/1000 --- Train Loss: 2.2937114972975667 --- Val Loss: 2.2994938368725584 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.2996490855523697 --- Val Loss: 2.2995099137987682 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 930/1000 --- Train Loss: 2.2976134429302606 --- Val Loss: 2.2994290984259513 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 940/1000 --- Train Loss: 2.291680828604366 --- Val Loss: 2.299425905133157 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 950/1000 --- Train Loss: 2.299594664779095 --- Val Loss: 2.2994539477970557 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 960/1000 --- Train Loss: 2.2975602973137352 --- Val Loss: 2.2992767950919255 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 970/1000 --- Train Loss: 2.299649056826861 --- Val Loss: 2.2993708660306984 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: 2.297608433821261 --- Val Loss: 2.299403970162254 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 990/1000 --- Train Loss: 2.3016241503060058 --- Val Loss: 2.299289616153506 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.029994746601858 --- Val Loss: 1.9723350057687832 --- Train Acc: 0.37 --- Val Acc: 0.44\n",
      "Epoch 10/100 --- Train Loss: 2.2436920470439934 --- Val Loss: 2.2708627663258163 --- Train Acc: 0.13 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.3028613048311417 --- Val Loss: 2.306335739051291 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 30/100 --- Train Loss: 2.302027980266485 --- Val Loss: 2.307956323867759 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/100 --- Train Loss: 2.302306549453235 --- Val Loss: 2.3087441570953535 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/100 --- Train Loss: 2.3006098379264364 --- Val Loss: 2.309994298391179 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/100 --- Train Loss: 2.3011940988991038 --- Val Loss: 2.3110074207087288 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.3029930493536424 --- Val Loss: 2.3061564125769696 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.301902597175438 --- Val Loss: 2.308502527310619 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/100 --- Train Loss: 2.3040379185776505 --- Val Loss: 2.3028110431510433 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3019057116002783 --- Val Loss: 2.3019768979834665 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 0.250971908863033 --- Val Loss: 0.10712517142459341 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 20/1000 --- Train Loss: 0.6408894768124632 --- Val Loss: 0.1809830392009048 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 30/1000 --- Train Loss: 0.8612158146170258 --- Val Loss: 0.11450664557781477 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 40/1000 --- Train Loss: 0.8556974667359798 --- Val Loss: 0.33696377005488265 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 50/1000 --- Train Loss: 0.5446714491464811 --- Val Loss: 0.05616086374471466 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 1.2500033082203599 --- Val Loss: 0.6177680749003237 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 70/1000 --- Train Loss: 1.3191403154929235 --- Val Loss: 0.8985698868130124 --- Train Acc: 0.93 --- Val Acc: 0.94\n",
      "Epoch 80/1000 --- Train Loss: 1.644445908496405 --- Val Loss: 1.67182292959612 --- Train Acc: 0.39 --- Val Acc: 0.37\n",
      "Epoch 90/1000 --- Train Loss: 1.8745380943082004 --- Val Loss: 1.89151343950939 --- Train Acc: 0.28 --- Val Acc: 0.26\n",
      "Epoch 100/1000 --- Train Loss: 1.9026282378266282 --- Val Loss: 1.9630458476910422 --- Train Acc: 0.25 --- Val Acc: 0.23\n",
      "Epoch 110/1000 --- Train Loss: 1.9339109128520517 --- Val Loss: 1.954762738628801 --- Train Acc: 0.25 --- Val Acc: 0.23\n",
      "Epoch 120/1000 --- Train Loss: 1.944611235027985 --- Val Loss: 1.9276342507010376 --- Train Acc: 0.26 --- Val Acc: 0.24\n",
      "Epoch 130/1000 --- Train Loss: 1.9782646103283819 --- Val Loss: 2.01789052492716 --- Train Acc: 0.22 --- Val Acc: 0.21\n",
      "Epoch 140/1000 --- Train Loss: 2.0448460297583657 --- Val Loss: 2.0304311650703313 --- Train Acc: 0.22 --- Val Acc: 0.21\n",
      "Epoch 150/1000 --- Train Loss: 2.160677533209473 --- Val Loss: 2.1908700529116105 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 160/1000 --- Train Loss: 2.1552106723235864 --- Val Loss: 2.18884002783147 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 170/1000 --- Train Loss: 2.1456743321735288 --- Val Loss: 2.181128663790882 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 180/1000 --- Train Loss: 2.1381355261279977 --- Val Loss: 2.1808658299197203 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 190/1000 --- Train Loss: 2.1559914479512594 --- Val Loss: 2.189254297377615 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 200/1000 --- Train Loss: 2.153775618262548 --- Val Loss: 2.1969931983671054 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 210/1000 --- Train Loss: 2.1503846326701432 --- Val Loss: 2.196364929665296 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 220/1000 --- Train Loss: 2.1446496533149144 --- Val Loss: 2.1966555177517053 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 230/1000 --- Train Loss: 2.2022746839159772 --- Val Loss: 2.213452955712432 --- Train Acc: 0.14 --- Val Acc: 0.14\n",
      "Epoch 240/1000 --- Train Loss: 2.19435355440056 --- Val Loss: 2.2329992917025314 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: 2.1765536295665666 --- Val Loss: 2.234061731436412 --- Train Acc: 0.15 --- Val Acc: 0.13\n",
      "Epoch 260/1000 --- Train Loss: 2.206753742332638 --- Val Loss: 2.2341625732030783 --- Train Acc: 0.15 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.249930703314001 --- Val Loss: 2.2675215306834597 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.2909061221792024 --- Val Loss: 2.2676002298606073 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.2685342908786246 --- Val Loss: 2.2592350485651616 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 300/1000 --- Train Loss: 2.2018436950767146 --- Val Loss: 2.188037522860288 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 310/1000 --- Train Loss: 2.2145064782877033 --- Val Loss: 2.185482781388087 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 320/1000 --- Train Loss: 2.1797355625590016 --- Val Loss: 2.185082264913894 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 330/1000 --- Train Loss: 2.191184894474687 --- Val Loss: 2.1747075687335324 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 340/1000 --- Train Loss: 2.1938949705009434 --- Val Loss: 2.174994691327912 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 350/1000 --- Train Loss: 2.2170370585780272 --- Val Loss: 2.1940284540885524 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 360/1000 --- Train Loss: 2.303734089478703 --- Val Loss: 2.3018811508939474 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.312655224319672 --- Val Loss: 2.300696289405233 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.3487157683561812 --- Val Loss: 2.3007614285958407 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.283372742343142 --- Val Loss: 2.2818100701277317 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.25885945785173 --- Val Loss: 2.223823510720256 --- Train Acc: 0.15 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.3108448200194887 --- Val Loss: 2.3052369025017994 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 30/1000 --- Train Loss: 2.3108254835693174 --- Val Loss: 2.3047947205747006 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.2987548338145807 --- Val Loss: 2.303519930595905 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.296889608950373 --- Val Loss: 2.303518669061116 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: 2.3028771478893084 --- Val Loss: 2.301538778905882 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.302814298902505 --- Val Loss: 2.302610956089993 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.3034218844066534 --- Val Loss: 2.3052424870009625 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.3030083672429233 --- Val Loss: 2.301579880558477 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: 2.3029986955054063 --- Val Loss: 2.3041829439718153 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.303069949242213 --- Val Loss: 2.3011577757037096 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.3027574609483987 --- Val Loss: 2.301305403638985 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.3007106111388222 --- Val Loss: 2.305268260221397 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 140/1000 --- Train Loss: 2.3029857146394668 --- Val Loss: 2.3016392961053382 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.302647725690499 --- Val Loss: 2.3025151279595866 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.302985896527069 --- Val Loss: 2.3026002204965286 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.302542278832538 --- Val Loss: 2.3031970743244736 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.30264622507411 --- Val Loss: 2.3020111331838686 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.302561011698315 --- Val Loss: 2.304827188574472 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 200/1000 --- Train Loss: 2.3028850756929855 --- Val Loss: 2.3041451066389125 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.302715407898421 --- Val Loss: 2.3016068742144835 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.3034964037607573 --- Val Loss: 2.306522971080176 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.302749587063174 --- Val Loss: 2.3015692133959034 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.303575446600341 --- Val Loss: 2.305348341981458 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: 2.302857130687107 --- Val Loss: 2.300061943569824 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.3028857943053214 --- Val Loss: 2.305316194211383 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.302868287928853 --- Val Loss: 2.3041995237614907 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.3028882678917166 --- Val Loss: 2.3047840718889834 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.300660594772393 --- Val Loss: 2.302731899158016 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.300684713774296 --- Val Loss: 2.305418091211913 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 310/1000 --- Train Loss: 2.302808206014553 --- Val Loss: 2.303958304404966 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.302743434831258 --- Val Loss: 2.304702113552816 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.302903152261895 --- Val Loss: 2.3049083188594373 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 340/1000 --- Train Loss: 2.302609528555113 --- Val Loss: 2.302590667281624 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3024622265039643 --- Val Loss: 2.3042647642417933 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.302920753243011 --- Val Loss: 2.3045684543082574 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.302953163040699 --- Val Loss: 2.3050493281383972 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.2992767852708877 --- Val Loss: 2.307141681035301 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 390/1000 --- Train Loss: 2.301430380849642 --- Val Loss: 2.3047407743566004 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.300691158208915 --- Val Loss: 2.304794152141317 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 410/1000 --- Train Loss: 2.3030054626668663 --- Val Loss: 2.3039885446956445 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.302880638950969 --- Val Loss: 2.3030662262466133 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.302622234321951 --- Val Loss: 2.303069488318909 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3036660593157 --- Val Loss: 2.308042121652727 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 450/1000 --- Train Loss: 2.3006655863168817 --- Val Loss: 2.3036278108163635 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3026067338599856 --- Val Loss: 2.3049955433626406 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.302514921946199 --- Val Loss: 2.3016534158853292 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.3025747924425954 --- Val Loss: 2.304196771366679 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.3013096084108215 --- Val Loss: 2.3010461594902485 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.302700771082346 --- Val Loss: 2.3032549662990562 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.302661675960117 --- Val Loss: 2.3054307094984554 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 520/1000 --- Train Loss: 2.299446175752938 --- Val Loss: 2.3027737621724573 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.297024467571211 --- Val Loss: 2.3064027645099974 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 540/1000 --- Train Loss: 2.302668352902889 --- Val Loss: 2.301544724720444 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.303249438549305 --- Val Loss: 2.3021724262514214 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.30265080471452 --- Val Loss: 2.304687768806125 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.303130685586641 --- Val Loss: 2.303586170558365 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.3027629005271315 --- Val Loss: 2.302928049875345 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.302997864307496 --- Val Loss: 2.3037924965314605 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.3032527046631244 --- Val Loss: 2.3042686127362293 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.303129684315167 --- Val Loss: 2.303377598045949 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.302497094998411 --- Val Loss: 2.3036512639510316 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.302703608332314 --- Val Loss: 2.3018555522720625 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3029957973129362 --- Val Loss: 2.302595227587397 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.3026160554161783 --- Val Loss: 2.304934975929201 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 660/1000 --- Train Loss: 2.3024350796514748 --- Val Loss: 2.3040830842218476 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3030394595682107 --- Val Loss: 2.304326360591047 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.3026412884100993 --- Val Loss: 2.305360789010498 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 690/1000 --- Train Loss: 2.3030687916292982 --- Val Loss: 2.3040483488496193 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3029067319149767 --- Val Loss: 2.304750569439807 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 710/1000 --- Train Loss: 2.3027275909556786 --- Val Loss: 2.301433028496552 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.303019441216936 --- Val Loss: 2.3065516288644003 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 730/1000 --- Train Loss: 2.3027391014108005 --- Val Loss: 2.3018233001855846 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.3029873459289667 --- Val Loss: 2.3027387720610144 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.303050904409927 --- Val Loss: 2.305432801273667 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 760/1000 --- Train Loss: 2.30276868532896 --- Val Loss: 2.302668599161616 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.3032512004637042 --- Val Loss: 2.306732250546922 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.302775521772545 --- Val Loss: 2.303076451718811 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.3030187374190945 --- Val Loss: 2.304482604311935 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.302789091957999 --- Val Loss: 2.306410126486383 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 810/1000 --- Train Loss: 2.3028412693506377 --- Val Loss: 2.3048438015948394 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.302950248497651 --- Val Loss: 2.303000540101434 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.3032329258164967 --- Val Loss: 2.2996081163481157 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3027508142310067 --- Val Loss: 2.3017687336084873 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.302639429942786 --- Val Loss: 2.303482518539782 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.3026845437565173 --- Val Loss: 2.3019135464569347 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.3030159084536166 --- Val Loss: 2.304177860487943 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3026340548790767 --- Val Loss: 2.303957492956035 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3031386366521662 --- Val Loss: 2.3043139874955987 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.3029697074898334 --- Val Loss: 2.302727919185053 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.302888022159289 --- Val Loss: 2.3032930824448723 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.302600673155667 --- Val Loss: 2.3050076722308184 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 930/1000 --- Train Loss: 2.3025809360489666 --- Val Loss: 2.303251284153942 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3025964360167026 --- Val Loss: 2.3025350502624393 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.3028096585988336 --- Val Loss: 2.3057333132571913 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 960/1000 --- Train Loss: 2.302985358962431 --- Val Loss: 2.303941244395088 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.30245620449788 --- Val Loss: 2.3037047168690385 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.302996886621387 --- Val Loss: 2.3068084955236787 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 990/1000 --- Train Loss: 2.302685971107078 --- Val Loss: 2.3006169177590254 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.2570979344339936 --- Val Loss: 2.2542469766792217 --- Train Acc: 0.20 --- Val Acc: 0.20\n",
      "Epoch 10/1000 --- Train Loss: 1.0517265962267688 --- Val Loss: 0.4617377850749008 --- Train Acc: 0.91 --- Val Acc: 0.90\n",
      "Epoch 20/1000 --- Train Loss: 1.6362776365885376 --- Val Loss: 1.4277672716816792 --- Train Acc: 0.44 --- Val Acc: 0.44\n",
      "Epoch 30/1000 --- Train Loss: 1.9919013709906412 --- Val Loss: 1.676810016222148 --- Train Acc: 0.31 --- Val Acc: 0.35\n",
      "Epoch 40/1000 --- Train Loss: 2.2137589110258626 --- Val Loss: 2.2340022805536037 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 50/1000 --- Train Loss: 2.2890240853601473 --- Val Loss: 2.2855044933421156 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.282524431613242 --- Val Loss: 2.284939810131082 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: 2.2791220389488456 --- Val Loss: 2.29585233347191 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/1000 --- Train Loss: 2.3015978474284386 --- Val Loss: 2.292745925754271 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.3139260186696866 --- Val Loss: 2.2937100782850366 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/1000 --- Train Loss: 2.2875925186658055 --- Val Loss: 2.2924685812085097 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: 2.279132564933467 --- Val Loss: 2.291726148149355 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 120/1000 --- Train Loss: 2.311906239491314 --- Val Loss: 2.294472174000589 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/1000 --- Train Loss: 2.299902067703891 --- Val Loss: 2.293692832376005 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/1000 --- Train Loss: 2.2999255892266266 --- Val Loss: 2.2932194574012907 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/1000 --- Train Loss: 2.29993591171731 --- Val Loss: 2.293478652421848 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/1000 --- Train Loss: 2.311994331742728 --- Val Loss: 2.292863639304273 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/1000 --- Train Loss: 2.2998771756653484 --- Val Loss: 2.2941848908884217 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: 2.2999034974423536 --- Val Loss: 2.2934711581188814 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.2998972908910233 --- Val Loss: 2.294541891021222 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.299945547006646 --- Val Loss: 2.293775516506586 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: 2.2999410685196624 --- Val Loss: 2.293827386340315 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: 2.323833614910621 --- Val Loss: 2.294458116256151 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/1000 --- Train Loss: 2.299898663356067 --- Val Loss: 2.294763845645433 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.299921514103507 --- Val Loss: 2.2931880637894957 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: 2.3119315026372167 --- Val Loss: 2.293796858206793 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.29992165176194 --- Val Loss: 2.2937303758725647 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.2999134706600226 --- Val Loss: 2.294142342467289 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.2999444729945733 --- Val Loss: 2.2940286676530888 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/1000 --- Train Loss: 2.2999169154770187 --- Val Loss: 2.294344889959049 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/1000 --- Train Loss: 2.31178107943062 --- Val Loss: 2.294999221752499 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/1000 --- Train Loss: 2.2999255166677823 --- Val Loss: 2.2938747601110903 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/1000 --- Train Loss: 2.2999219653863188 --- Val Loss: 2.2928449682071896 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: 2.299902592636154 --- Val Loss: 2.293980771974019 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: 2.2998982375685944 --- Val Loss: 2.2939000036390977 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/1000 --- Train Loss: 2.299930107236831 --- Val Loss: 2.2930216701878234 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/1000 --- Train Loss: 2.2998948016920417 --- Val Loss: 2.2938075598766967 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/1000 --- Train Loss: 2.299893894729392 --- Val Loss: 2.2936913690775764 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/1000 --- Train Loss: 2.2998949498158168 --- Val Loss: 2.293945753390661 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.3120611708439704 --- Val Loss: 2.2940494932399615 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/1000 --- Train Loss: 2.299932550841549 --- Val Loss: 2.29396182048268 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/1000 --- Train Loss: 2.3119402001869167 --- Val Loss: 2.292363639900189 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/1000 --- Train Loss: 2.2999009278572062 --- Val Loss: 2.2936250937890357 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/1000 --- Train Loss: 2.2999343026597923 --- Val Loss: 2.2930937791669384 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/1000 --- Train Loss: 2.299895120937585 --- Val Loss: 2.2936922435445983 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.2999218986077232 --- Val Loss: 2.293226269538701 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: 2.3000185277589478 --- Val Loss: 2.293254689919702 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/1000 --- Train Loss: 2.312028854077225 --- Val Loss: 2.292088640112119 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.2999402653028342 --- Val Loss: 2.293465373895557 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/1000 --- Train Loss: 2.299902026209444 --- Val Loss: 2.293197748062601 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 500/1000 --- Train Loss: 2.2999368275124756 --- Val Loss: 2.2933421767345243 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.2999136292243354 --- Val Loss: 2.293886803241456 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 520/1000 --- Train Loss: 2.2999147700275273 --- Val Loss: 2.293073525363551 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 530/1000 --- Train Loss: 2.2999156506619802 --- Val Loss: 2.294292587171159 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.2999540468621325 --- Val Loss: 2.292827220482539 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 550/1000 --- Train Loss: 2.299919590196751 --- Val Loss: 2.2945449248163508 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: 2.2999826711828457 --- Val Loss: 2.2929420338100446 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 570/1000 --- Train Loss: 2.299944087996394 --- Val Loss: 2.2923578025913036 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 580/1000 --- Train Loss: 2.2998718177076793 --- Val Loss: 2.2937047857964523 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 590/1000 --- Train Loss: 2.2999547224871115 --- Val Loss: 2.2925392934727693 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.299930648247928 --- Val Loss: 2.2950658317099313 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 610/1000 --- Train Loss: 2.2999447672140336 --- Val Loss: 2.2938946824864668 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.2999027344895238 --- Val Loss: 2.293403913494168 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: 2.2998985624188437 --- Val Loss: 2.293885415162533 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 640/1000 --- Train Loss: 2.2999111037782534 --- Val Loss: 2.293248110025281 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: 2.2999362948583384 --- Val Loss: 2.2941530983124165 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 660/1000 --- Train Loss: 2.299916094672495 --- Val Loss: 2.2944904702866933 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 670/1000 --- Train Loss: 2.2998868655391616 --- Val Loss: 2.2937953940930442 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.299953715309101 --- Val Loss: 2.292985164765536 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 690/1000 --- Train Loss: 2.2999276746050024 --- Val Loss: 2.2930535872496716 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 700/1000 --- Train Loss: 2.2998935170511117 --- Val Loss: 2.293382488524448 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 710/1000 --- Train Loss: 2.299915358332099 --- Val Loss: 2.2929455521254285 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 720/1000 --- Train Loss: 2.299889793500499 --- Val Loss: 2.293171481353508 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 730/1000 --- Train Loss: 2.299913571288603 --- Val Loss: 2.292468722117996 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 740/1000 --- Train Loss: 2.2999110889944987 --- Val Loss: 2.2932025720435965 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: 2.299941181742378 --- Val Loss: 2.293393274023007 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 760/1000 --- Train Loss: 2.2998995983852244 --- Val Loss: 2.2943080171728165 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.299919667349471 --- Val Loss: 2.293090023905157 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: 2.299904000796233 --- Val Loss: 2.293466826118044 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 790/1000 --- Train Loss: 2.299916583594011 --- Val Loss: 2.294473811475109 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 800/1000 --- Train Loss: 2.2999887068374116 --- Val Loss: 2.295256529016556 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: 2.299909623557352 --- Val Loss: 2.2939871885497265 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 820/1000 --- Train Loss: 2.299947007125269 --- Val Loss: 2.294252830711173 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 830/1000 --- Train Loss: 2.2999195894286117 --- Val Loss: 2.293463096697629 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: 2.299990606991944 --- Val Loss: 2.29422579393193 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.2999025413092906 --- Val Loss: 2.2929541485072646 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 860/1000 --- Train Loss: 2.2999041443427917 --- Val Loss: 2.293076484379427 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 870/1000 --- Train Loss: 2.299938579065735 --- Val Loss: 2.293495578216185 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 880/1000 --- Train Loss: 2.2999127139979487 --- Val Loss: 2.2942744502656214 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 890/1000 --- Train Loss: 2.2998877608092516 --- Val Loss: 2.2931505168501887 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: 2.299960747594871 --- Val Loss: 2.2938801218464584 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 910/1000 --- Train Loss: 2.299891392027798 --- Val Loss: 2.294251342091295 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.3000143039918446 --- Val Loss: 2.294017512749288 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 930/1000 --- Train Loss: 2.2998941145876564 --- Val Loss: 2.2943701265625545 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 940/1000 --- Train Loss: 2.299965585142665 --- Val Loss: 2.293370901043988 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 950/1000 --- Train Loss: 2.29994825982991 --- Val Loss: 2.2938968879277337 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 960/1000 --- Train Loss: 2.299901817761114 --- Val Loss: 2.2942413561809785 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 970/1000 --- Train Loss: 2.3000517299909586 --- Val Loss: 2.293369879547002 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: 2.2999334402454 --- Val Loss: 2.2950287068902986 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 990/1000 --- Train Loss: 2.2998896080782374 --- Val Loss: 2.293939554602098 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.4860430217089418 --- Val Loss: 2.355761800194552 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3384438857388323 --- Val Loss: 2.3747454453820356 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 20/500 --- Train Loss: 2.3305561447293304 --- Val Loss: 2.312636727205177 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.3134946010287614 --- Val Loss: 2.3112061859282496 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 40/500 --- Train Loss: 2.3623490394384463 --- Val Loss: 2.3161955089962225 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: 2.314490222518227 --- Val Loss: 2.314950974236763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3198797396368716 --- Val Loss: 2.325310550679615 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3906941276577487 --- Val Loss: 2.3964569009165375 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.3287901658561156 --- Val Loss: 2.3194013029110456 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: 2.3189272518580695 --- Val Loss: 2.3338628637164702 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/500 --- Train Loss: 2.332105811036286 --- Val Loss: 2.3493769840005787 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 110/500 --- Train Loss: 2.339084171274687 --- Val Loss: 2.336415183454203 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.302559662811521 --- Val Loss: 2.3025134115029813 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/1000 --- Train Loss: 2.3023551190102327 --- Val Loss: 2.3018945641712656 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/1000 --- Train Loss: 2.302211611989896 --- Val Loss: 2.3014039182142043 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/1000 --- Train Loss: 2.3021109386821217 --- Val Loss: 2.3010130491137564 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/1000 --- Train Loss: 2.3020408902298866 --- Val Loss: 2.300700526008779 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: 2.3019918454256674 --- Val Loss: 2.3004511021232 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.301957317398335 --- Val Loss: 2.300247549051237 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: 2.3019335109132313 --- Val Loss: 2.3000833946421766 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/1000 --- Train Loss: 2.3019165279488485 --- Val Loss: 2.299948299228867 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.3019046696390295 --- Val Loss: 2.2998410234740265 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/1000 --- Train Loss: 2.3018964521237644 --- Val Loss: 2.2997500526100842 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: 2.3018905242184973 --- Val Loss: 2.299675230012149 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/1000 --- Train Loss: 2.301886357656189 --- Val Loss: 2.299614489716043 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/1000 --- Train Loss: 2.3018834813118474 --- Val Loss: 2.2995637803348847 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/1000 --- Train Loss: 2.3018813814534256 --- Val Loss: 2.2995243640220955 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/1000 --- Train Loss: 2.301879964732714 --- Val Loss: 2.299490435439127 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/1000 --- Train Loss: 2.301878981209104 --- Val Loss: 2.2994619789096054 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/1000 --- Train Loss: 2.3018782073573614 --- Val Loss: 2.299438308488401 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: 2.301877698301504 --- Val Loss: 2.2994187235041648 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.3018773434317956 --- Val Loss: 2.2994019242807693 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.301877030253987 --- Val Loss: 2.2993856217557576 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: 2.301876820647501 --- Val Loss: 2.2993749024919565 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: 2.3018766614531847 --- Val Loss: 2.299363848006561 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/1000 --- Train Loss: 2.3018765612418246 --- Val Loss: 2.299357838748922 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.3018764590765377 --- Val Loss: 2.299353411416523 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: 2.3018763320246984 --- Val Loss: 2.2993483493053657 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.301876244841018 --- Val Loss: 2.2993424809791834 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.3018761888321673 --- Val Loss: 2.299338912943072 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.3018761468771967 --- Val Loss: 2.299335749064721 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/1000 --- Train Loss: 2.3018760804461857 --- Val Loss: 2.29933381087804 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/1000 --- Train Loss: 2.3018759908439588 --- Val Loss: 2.29933130814074 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/1000 --- Train Loss: 2.301875995930532 --- Val Loss: 2.2993254606947366 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/1000 --- Train Loss: 2.301875867524173 --- Val Loss: 2.299321790512948 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: 2.3018758593062967 --- Val Loss: 2.2993188200318815 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: 2.3018758001016764 --- Val Loss: 2.2993183848501757 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/1000 --- Train Loss: 2.3018757141970725 --- Val Loss: 2.2993180678935343 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/1000 --- Train Loss: 2.3018756103623157 --- Val Loss: 2.2993178744800256 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/1000 --- Train Loss: 2.3018755693876782 --- Val Loss: 2.299319530954005 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/1000 --- Train Loss: 2.301875555580948 --- Val Loss: 2.299318903375606 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.3018754664234584 --- Val Loss: 2.299318249813839 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/1000 --- Train Loss: 2.301875353189162 --- Val Loss: 2.299316461916861 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/1000 --- Train Loss: 2.3018753006464654 --- Val Loss: 2.2993157154726216 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/1000 --- Train Loss: 2.3018752129094513 --- Val Loss: 2.2993122805328 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/1000 --- Train Loss: 2.301875091818174 --- Val Loss: 2.299312346637985 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/1000 --- Train Loss: 2.301875067484392 --- Val Loss: 2.2993128560453093 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.3018749358388586 --- Val Loss: 2.299311228184806 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: 2.3018748798214763 --- Val Loss: 2.299310059096902 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/1000 --- Train Loss: 2.3018747296624196 --- Val Loss: 2.299311099039708 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.3018746435871664 --- Val Loss: 2.29931201220469 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/1000 --- Train Loss: 2.3018746107334516 --- Val Loss: 2.299312510602757 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 500/1000 --- Train Loss: 2.3018744978726335 --- Val Loss: 2.2993135124187787 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.3018743805417192 --- Val Loss: 2.299313162182428 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 520/1000 --- Train Loss: 2.3018742428947707 --- Val Loss: 2.2993123747145012 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 530/1000 --- Train Loss: 2.301874134635729 --- Val Loss: 2.299309840812239 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.301873936498404 --- Val Loss: 2.2993092398237462 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 550/1000 --- Train Loss: 2.3018738997728247 --- Val Loss: 2.2993092114498235 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: 2.301873720374563 --- Val Loss: 2.2993098303832507 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 570/1000 --- Train Loss: 2.3018736627488625 --- Val Loss: 2.2993109385047323 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 580/1000 --- Train Loss: 2.3018735633477556 --- Val Loss: 2.299312605921575 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 590/1000 --- Train Loss: 2.3018732928485424 --- Val Loss: 2.2993118637270813 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.3018731973900395 --- Val Loss: 2.2993110197306588 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 610/1000 --- Train Loss: 2.3018729678492726 --- Val Loss: 2.299310137849072 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.301872860944855 --- Val Loss: 2.2993091179126472 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: 2.301872676426584 --- Val Loss: 2.2993079740823723 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 640/1000 --- Train Loss: 2.3018724164601836 --- Val Loss: 2.2993079473430558 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: 2.3018721829264925 --- Val Loss: 2.2993067725790635 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 660/1000 --- Train Loss: 2.3018720281142704 --- Val Loss: 2.2993084599474582 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 670/1000 --- Train Loss: 2.301871831609809 --- Val Loss: 2.299308147553344 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.301871564047282 --- Val Loss: 2.2993077344918453 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 690/1000 --- Train Loss: 2.301871218556134 --- Val Loss: 2.299307721915855 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 700/1000 --- Train Loss: 2.3018709799786703 --- Val Loss: 2.2993084508073665 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 710/1000 --- Train Loss: 2.3018707511672827 --- Val Loss: 2.2993077293566095 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 720/1000 --- Train Loss: 2.301870342930208 --- Val Loss: 2.299307555222248 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 730/1000 --- Train Loss: 2.3018700975420128 --- Val Loss: 2.2993089045395623 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 740/1000 --- Train Loss: 2.3018697912890103 --- Val Loss: 2.29930934665945 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: 2.301869416335162 --- Val Loss: 2.299309341492478 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 760/1000 --- Train Loss: 2.3018689682338143 --- Val Loss: 2.2993081995472227 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.301868634911432 --- Val Loss: 2.299308050859227 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: 2.301868157397724 --- Val Loss: 2.299307762488467 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 790/1000 --- Train Loss: 2.301867611804247 --- Val Loss: 2.2993081880295727 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 800/1000 --- Train Loss: 2.3018669526323543 --- Val Loss: 2.2993070412374417 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: 2.301866343484419 --- Val Loss: 2.2993069669973294 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 820/1000 --- Train Loss: 2.301866121805487 --- Val Loss: 2.299304923137356 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 830/1000 --- Train Loss: 2.3018653716211195 --- Val Loss: 2.299304316427394 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: 2.301864824309002 --- Val Loss: 2.299299726578452 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.301863634164956 --- Val Loss: 2.299299055251086 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 860/1000 --- Train Loss: 2.301863193968431 --- Val Loss: 2.299299104742733 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 870/1000 --- Train Loss: 2.3018622600906946 --- Val Loss: 2.2992976069819955 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 880/1000 --- Train Loss: 2.301861600183692 --- Val Loss: 2.299294118957772 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 890/1000 --- Train Loss: 2.3018603650250227 --- Val Loss: 2.2992931434284096 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: 2.3018593189271654 --- Val Loss: 2.2992915505445652 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 910/1000 --- Train Loss: 2.301857866688875 --- Val Loss: 2.299290606136626 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.3018564396322185 --- Val Loss: 2.299289663772369 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 930/1000 --- Train Loss: 2.301855355944172 --- Val Loss: 2.299290298519839 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 940/1000 --- Train Loss: 2.301853329880334 --- Val Loss: 2.299287699766217 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 950/1000 --- Train Loss: 2.301851545408708 --- Val Loss: 2.2992838335666703 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 960/1000 --- Train Loss: 2.301849441005965 --- Val Loss: 2.299283456954094 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 970/1000 --- Train Loss: 2.30184798251796 --- Val Loss: 2.299282996486356 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: 2.301844930396911 --- Val Loss: 2.299282510378403 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 990/1000 --- Train Loss: 2.3018419997101467 --- Val Loss: 2.299278387975423 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.297451494670716 --- Val Loss: 2.2977060640172446 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 10/100 --- Train Loss: 2.189960346265456 --- Val Loss: 2.060061456463595 --- Train Acc: 0.39 --- Val Acc: 0.38\n",
      "Epoch 20/100 --- Train Loss: 2.2473165847889613 --- Val Loss: 2.2912560178642543 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/100 --- Train Loss: 2.2976266426317395 --- Val Loss: 2.299998630978645 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/100 --- Train Loss: 2.3389381164863026 --- Val Loss: 2.31443500126526 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.3016586485368404 --- Val Loss: 2.301656823060072 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/100 --- Train Loss: 2.3018064454049534 --- Val Loss: 2.303673423125172 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.301660531993374 --- Val Loss: 2.3018609590183443 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/100 --- Train Loss: 2.301815372656872 --- Val Loss: 2.3015534808540963 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.301831260228119 --- Val Loss: 2.301386788661936 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.301561317425029 --- Val Loss: 2.301288861917417 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 10/500 --- Train Loss: 0.6374516653724801 --- Val Loss: 0.32448128671066295 --- Train Acc: 0.90 --- Val Acc: 0.91\n",
      "Epoch 20/500 --- Train Loss: 1.2577396406354329 --- Val Loss: 0.7726480453032557 --- Train Acc: 0.75 --- Val Acc: 0.75\n",
      "Epoch 30/500 --- Train Loss: 1.9349444563494616 --- Val Loss: 1.8930702105211696 --- Train Acc: 0.28 --- Val Acc: 0.29\n",
      "Epoch 40/500 --- Train Loss: 2.295552925218056 --- Val Loss: 2.286692196954334 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 50/500 --- Train Loss: 2.281292346225588 --- Val Loss: 2.3014812819053447 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 60/500 --- Train Loss: 2.3311936647340668 --- Val Loss: 2.299691002788292 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 70/500 --- Train Loss: 2.309302421037965 --- Val Loss: 2.2974598913143787 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 80/500 --- Train Loss: 2.297338142518184 --- Val Loss: 2.3008060228728033 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 90/500 --- Train Loss: 2.3052282287468553 --- Val Loss: 2.299398438431448 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 100/500 --- Train Loss: 2.299255115334543 --- Val Loss: 2.3006001275185235 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 110/500 --- Train Loss: 2.3492406024685315 --- Val Loss: 2.298887918093262 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 120/500 --- Train Loss: 2.3011692789219356 --- Val Loss: 2.2998880401613246 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 130/500 --- Train Loss: 2.301226132307994 --- Val Loss: 2.2995007026686443 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 140/500 --- Train Loss: 2.30121530811212 --- Val Loss: 2.3006053473837955 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 150/500 --- Train Loss: 2.3011817596191997 --- Val Loss: 2.2997141712560105 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 160/500 --- Train Loss: 2.3131631369864865 --- Val Loss: 2.2987322039886355 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 170/500 --- Train Loss: 2.3012138298994764 --- Val Loss: 2.300261433429883 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 180/500 --- Train Loss: 2.3012354815282055 --- Val Loss: 2.299127841924712 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 190/500 --- Train Loss: 2.3012175999400237 --- Val Loss: 2.298924167084063 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 200/500 --- Train Loss: 2.301190562286338 --- Val Loss: 2.2985709886702947 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 210/500 --- Train Loss: 2.301224273136845 --- Val Loss: 2.3002701773533434 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 220/500 --- Train Loss: 2.301245390108527 --- Val Loss: 2.2992658902051377 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 230/500 --- Train Loss: 2.3011862314497327 --- Val Loss: 2.2992918549883012 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 240/500 --- Train Loss: 2.301235145827592 --- Val Loss: 2.2994731330024654 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 250/500 --- Train Loss: 2.3012715179817187 --- Val Loss: 2.2998841346800147 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 260/500 --- Train Loss: 2.30117817634648 --- Val Loss: 2.3000955365600384 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 270/500 --- Train Loss: 2.3132000615269934 --- Val Loss: 2.3000887186189325 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 280/500 --- Train Loss: 2.3011803770772463 --- Val Loss: 2.2992271105712727 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 290/500 --- Train Loss: 2.3012629922914525 --- Val Loss: 2.301921469959753 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 300/500 --- Train Loss: 2.3012373336661303 --- Val Loss: 2.3005420589311054 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 310/500 --- Train Loss: 2.301202227098891 --- Val Loss: 2.299765663557601 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 320/500 --- Train Loss: 2.3012960985913096 --- Val Loss: 2.300487851837199 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 330/500 --- Train Loss: 2.301197661501674 --- Val Loss: 2.299346094811151 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 340/500 --- Train Loss: 2.3012169688234816 --- Val Loss: 2.2999696838724506 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 350/500 --- Train Loss: 2.301238257384046 --- Val Loss: 2.2991308718944525 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 360/500 --- Train Loss: 2.3012130581569963 --- Val Loss: 2.2994358965374433 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 370/500 --- Train Loss: 2.3013050363408576 --- Val Loss: 2.298046954729345 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 380/500 --- Train Loss: 2.3011896164563628 --- Val Loss: 2.300627873815192 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 390/500 --- Train Loss: 2.3011994389073855 --- Val Loss: 2.299463061013674 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 400/500 --- Train Loss: 2.301186346388767 --- Val Loss: 2.3005013750880843 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 410/500 --- Train Loss: 2.301224284935014 --- Val Loss: 2.3004843628676572 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 420/500 --- Train Loss: 2.3012210038108583 --- Val Loss: 2.3003274051530664 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 430/500 --- Train Loss: 2.3012227705516906 --- Val Loss: 2.3013794635496536 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 440/500 --- Train Loss: 2.3132874597986097 --- Val Loss: 2.2990913022825086 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 450/500 --- Train Loss: 2.3012275457276923 --- Val Loss: 2.29853932649162 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 460/500 --- Train Loss: 2.3012089474652884 --- Val Loss: 2.298764874652215 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 470/500 --- Train Loss: 2.3012120249787023 --- Val Loss: 2.298933743130292 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 480/500 --- Train Loss: 2.3012525665376415 --- Val Loss: 2.2999084259253655 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 490/500 --- Train Loss: 2.301250654662931 --- Val Loss: 2.2984011763771064 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 4.970984114925508 --- Val Loss: 3.6797752603550222 --- Train Acc: 0.35 --- Val Acc: 0.34\n",
      "Epoch 10/1000 --- Train Loss: 2.3109276260435907 --- Val Loss: 2.316754283525755 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.3172980689950298 --- Val Loss: 2.3277886822053655 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.309769582997355 --- Val Loss: 2.31213987188948 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.305385369279887 --- Val Loss: 2.300218070774505 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.3086309586067824 --- Val Loss: 2.3128351137030343 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.3063019897027024 --- Val Loss: 2.300193495866233 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 70/1000 --- Train Loss: 2.3246270568813356 --- Val Loss: 2.320542403704459 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.3083662590859393 --- Val Loss: 2.3170114343186254 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: 2.3093951126213237 --- Val Loss: 2.3246010173565606 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.315593112117329 --- Val Loss: 2.3069571121574777 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3086865133213945 --- Val Loss: 2.302023358273934 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.303528096240577 --- Val Loss: 2.302051725044184 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.310711168859768 --- Val Loss: 2.3084416537545267 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: 2.3119793878717667 --- Val Loss: 2.3304585946420824 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: 2.308914030299746 --- Val Loss: 2.317393670808162 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.319616738585151 --- Val Loss: 2.3019320837049078 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: 2.327484222119704 --- Val Loss: 2.308854519177583 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: 2.306520863343492 --- Val Loss: 2.313270318965983 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: 2.3055275237546584 --- Val Loss: 2.317388418625475 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: 2.3133594335698087 --- Val Loss: 2.307375623355289 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.312627311710752 --- Val Loss: 2.313523449959954 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.3134961216837615 --- Val Loss: 2.322415877856353 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.309769857866151 --- Val Loss: 2.3179993898113618 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: 2.3177569240417673 --- Val Loss: 2.317731736728082 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3044743486531107 --- Val Loss: 2.3185739857993726 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: 2.305702426219225 --- Val Loss: 2.3049843762155278 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.313770845601765 --- Val Loss: 2.3253167506618224 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.325831675738089 --- Val Loss: 2.3501138851171532 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.312442023263496 --- Val Loss: 2.298956621563109 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: 2.309000046381434 --- Val Loss: 2.31866897096039 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.3055026321956866 --- Val Loss: 2.3095624065253237 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.3054810736114733 --- Val Loss: 2.3154723797831664 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: 2.3072629379242566 --- Val Loss: 2.300420322721889 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3082017865254 --- Val Loss: 2.3046003864530262 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.312008475919673 --- Val Loss: 2.3167945581912255 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: 2.306350326558739 --- Val Loss: 2.2942516725296893 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 370/1000 --- Train Loss: 2.3124043567319923 --- Val Loss: 2.307711417226494 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.308795744649444 --- Val Loss: 2.3261755952738485 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: 2.3142998248962634 --- Val Loss: 2.3338094045559563 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 400/1000 --- Train Loss: 2.324982860063039 --- Val Loss: 2.3238011244768506 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3121914751930728 --- Val Loss: 2.31995890520714 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 420/1000 --- Train Loss: 2.328188697498784 --- Val Loss: 2.3415643714462795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3146682360702613 --- Val Loss: 2.3117903117458867 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: 2.311271461373903 --- Val Loss: 2.3196083877619063 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3133697694827244 --- Val Loss: 2.322022254289846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3084905010803842 --- Val Loss: 2.296095897092079 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 470/1000 --- Train Loss: 2.307471220173741 --- Val Loss: 2.31080900766995 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.310110053626757 --- Val Loss: 2.300309231575084 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.30803551443953 --- Val Loss: 2.3044044172998484 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3087288885808857 --- Val Loss: 2.3037783529287266 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.305929840254562 --- Val Loss: 2.31215165860611 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.3040663397814387 --- Val Loss: 2.3024460045037216 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 530/1000 --- Train Loss: 2.3144841504280023 --- Val Loss: 2.318420770872494 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3121324142523805 --- Val Loss: 2.304550102586792 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: 2.3071116349309557 --- Val Loss: 2.312632820990259 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: 2.3136688470225724 --- Val Loss: 2.3169397340337645 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: 2.3126347232255067 --- Val Loss: 2.316965706523308 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.306808999359715 --- Val Loss: 2.312758348034106 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.316369363934037 --- Val Loss: 2.309626689346294 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.310726961648184 --- Val Loss: 2.3104006988039116 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.306431129961557 --- Val Loss: 2.309102769789279 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3101752784070424 --- Val Loss: 2.3023918915136496 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.3081475067908617 --- Val Loss: 2.305720084676446 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.321685171655924 --- Val Loss: 2.3311066946975036 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: 2.3128660839202784 --- Val Loss: 2.3242318297801403 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3151322297541994 --- Val Loss: 2.315401188285782 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3178647131153123 --- Val Loss: 2.3106455172963405 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.3142912464147742 --- Val Loss: 2.3060777943384085 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.3095827702425864 --- Val Loss: 2.304692350919289 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.308018226767647 --- Val Loss: 2.3144642142462013 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: 2.3117386204583887 --- Val Loss: 2.2981778020132806 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3089300910266255 --- Val Loss: 2.3113149680797442 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3166335314305986 --- Val Loss: 2.328992824680664 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: 2.3264696089864554 --- Val Loss: 2.31422147577594 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.309739644254916 --- Val Loss: 2.318165649528756 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.317079670857682 --- Val Loss: 2.3150245382977857 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: 2.3162991432883486 --- Val Loss: 2.3345913565976137 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.317934527306216 --- Val Loss: 2.306627378635541 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: 2.3169976356921884 --- Val Loss: 2.3040173518812814 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.311243866719851 --- Val Loss: 2.3082955389809747 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.309725856432609 --- Val Loss: 2.3218116745267974 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 820/1000 --- Train Loss: 2.3090847052607644 --- Val Loss: 2.3158447794632107 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: 2.3130796365929953 --- Val Loss: 2.3322030900423365 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: 2.31614634450221 --- Val Loss: 2.314267819952481 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.310311359190083 --- Val Loss: 2.311028379972061 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.313283383263378 --- Val Loss: 2.308063486310193 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.313737562176537 --- Val Loss: 2.3105271594529286 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.3101126782505275 --- Val Loss: 2.3088248584638387 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: 2.3090919113258948 --- Val Loss: 2.307192929777024 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3112320253295713 --- Val Loss: 2.299216102365029 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: 2.317096745874204 --- Val Loss: 2.3191368001354964 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.33109201235066 --- Val Loss: 2.3125328342568103 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.316286496695146 --- Val Loss: 2.323293443185519 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: 2.310214520414024 --- Val Loss: 2.3105476110874656 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3150089266486824 --- Val Loss: 2.30720471707207 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.3082762536616985 --- Val Loss: 2.3246640181432827 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 970/1000 --- Train Loss: 2.307405581587193 --- Val Loss: 2.3026399306311065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3147931978324565 --- Val Loss: 2.319709606473619 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.313809817639501 --- Val Loss: 2.3358429115441957 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.08333333333333333\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.313615418503331 --- Val Loss: 2.3225112963832064 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.3097976603434818 --- Val Loss: 2.32357585365431 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Epoch 20/100 --- Train Loss: 2.3072125127388956 --- Val Loss: 2.322365812369329 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/100 --- Train Loss: 2.3126928727667018 --- Val Loss: 2.3091061599815212 --- Train Acc: 0.11 --- Val Acc: 0.16\n",
      "Epoch 40/100 --- Train Loss: 2.3029637026317706 --- Val Loss: 2.3108770631331166 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/100 --- Train Loss: 2.313788704964507 --- Val Loss: 2.294893690969553 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.319514406531372 --- Val Loss: 2.3080111743857623 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.3053134067600554 --- Val Loss: 2.298079735558233 --- Train Acc: 0.11 --- Val Acc: 0.16\n",
      "Epoch 80/100 --- Train Loss: 2.316323258320741 --- Val Loss: 2.3217466233696493 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/100 --- Train Loss: 2.3077158071065718 --- Val Loss: 2.309003007347074 --- Train Acc: 0.10 --- Val Acc: 0.06\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3025166110676043 --- Val Loss: 2.302407767716306 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.30204783342047 --- Val Loss: 2.301070331461587 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.3018200432930067 --- Val Loss: 2.300235502783349 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: 2.30170710960478 --- Val Loss: 2.2996955068964104 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/1000 --- Train Loss: 2.3016532119096564 --- Val Loss: 2.299349748011391 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/1000 --- Train Loss: 2.3016264282366525 --- Val Loss: 2.299104554176726 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/1000 --- Train Loss: 2.3016139832903932 --- Val Loss: 2.298977575043646 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 70/1000 --- Train Loss: 2.30160738890583 --- Val Loss: 2.2988718172612526 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.301604378594361 --- Val Loss: 2.2987919992787442 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/1000 --- Train Loss: 2.3016026657200834 --- Val Loss: 2.2987477424071825 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/1000 --- Train Loss: 2.301601809240594 --- Val Loss: 2.29873013908195 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.3016010827887396 --- Val Loss: 2.2986965169166744 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.3016006760237513 --- Val Loss: 2.298682332318663 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: 2.301600315069163 --- Val Loss: 2.2986785021980247 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: 2.3015998840340504 --- Val Loss: 2.2986598795361957 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: 2.301599493922405 --- Val Loss: 2.2986565957402005 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 160/1000 --- Train Loss: 2.3015990501094654 --- Val Loss: 2.29865569009275 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: 2.301598557765046 --- Val Loss: 2.2986539458420117 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: 2.301597985667102 --- Val Loss: 2.2986562991201795 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 190/1000 --- Train Loss: 2.301597322434925 --- Val Loss: 2.2986611291618932 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: 2.3015965369393276 --- Val Loss: 2.29865594614919 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/1000 --- Train Loss: 2.301595718783912 --- Val Loss: 2.2986697055967618 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/1000 --- Train Loss: 2.3015947119984252 --- Val Loss: 2.298646960277158 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 230/1000 --- Train Loss: 2.301593470402702 --- Val Loss: 2.2986631837819362 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/1000 --- Train Loss: 2.3015922179079706 --- Val Loss: 2.298657853512809 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: 2.301590433632233 --- Val Loss: 2.2986517231931773 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 260/1000 --- Train Loss: 2.3015886253477107 --- Val Loss: 2.2986582582316957 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.3015861552300776 --- Val Loss: 2.29865614718711 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/1000 --- Train Loss: 2.3015834406057083 --- Val Loss: 2.2986555331697494 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: 2.3015798268745384 --- Val Loss: 2.2986522000222878 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: 2.3015751373477618 --- Val Loss: 2.29863090766361 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: 2.3015700515334307 --- Val Loss: 2.2986476217897094 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/1000 --- Train Loss: 2.3015625443804524 --- Val Loss: 2.2986526272082464 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/1000 --- Train Loss: 2.3015539845886246 --- Val Loss: 2.298633957924966 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 340/1000 --- Train Loss: 2.301542297672588 --- Val Loss: 2.298616478054394 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 350/1000 --- Train Loss: 2.301526345514658 --- Val Loss: 2.298603672127815 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 360/1000 --- Train Loss: 2.301506605377616 --- Val Loss: 2.29857229300128 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/1000 --- Train Loss: 2.3014763403280063 --- Val Loss: 2.298538540386461 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/1000 --- Train Loss: 2.3014318994944083 --- Val Loss: 2.2984853927709255 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 390/1000 --- Train Loss: 2.301365540183558 --- Val Loss: 2.2984146878044287 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/1000 --- Train Loss: 2.3012599078279017 --- Val Loss: 2.2983186146138106 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/1000 --- Train Loss: 2.30109032402052 --- Val Loss: 2.298147609258566 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: 2.300765485470469 --- Val Loss: 2.2978299213127924 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: 2.3000840605065487 --- Val Loss: 2.297147456898817 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/1000 --- Train Loss: 2.2984291019381304 --- Val Loss: 2.2954582279577256 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Epoch 450/1000 --- Train Loss: 2.292761170417574 --- Val Loss: 2.289855390211355 --- Train Acc: 0.21 --- Val Acc: 0.23\n",
      "Epoch 460/1000 --- Train Loss: 2.2591521149302403 --- Val Loss: 2.2562111617938596 --- Train Acc: 0.28 --- Val Acc: 0.29\n",
      "Epoch 470/1000 --- Train Loss: 2.0227240616709383 --- Val Loss: 2.011777810581883 --- Train Acc: 0.20 --- Val Acc: 0.25\n",
      "Epoch 480/1000 --- Train Loss: 1.8980608055007147 --- Val Loss: 1.8906790186079367 --- Train Acc: 0.21 --- Val Acc: 0.25\n",
      "Epoch 490/1000 --- Train Loss: 1.8260289220334673 --- Val Loss: 1.8187577535777741 --- Train Acc: 0.21 --- Val Acc: 0.25\n",
      "Epoch 500/1000 --- Train Loss: 1.748812631536042 --- Val Loss: 1.7395102195453738 --- Train Acc: 0.21 --- Val Acc: 0.24\n",
      "Epoch 510/1000 --- Train Loss: 1.651195725091572 --- Val Loss: 1.6368464787127124 --- Train Acc: 0.23 --- Val Acc: 0.26\n",
      "Epoch 520/1000 --- Train Loss: 1.5704109522037084 --- Val Loss: 1.5542226422747953 --- Train Acc: 0.31 --- Val Acc: 0.37\n",
      "Epoch 530/1000 --- Train Loss: 1.4846860755787032 --- Val Loss: 1.47159592790506 --- Train Acc: 0.42 --- Val Acc: 0.43\n",
      "Epoch 540/1000 --- Train Loss: 1.324858566007945 --- Val Loss: 1.3199561728493683 --- Train Acc: 0.50 --- Val Acc: 0.50\n",
      "Epoch 550/1000 --- Train Loss: 1.1545804234570558 --- Val Loss: 1.140280321047187 --- Train Acc: 0.63 --- Val Acc: 0.65\n",
      "Epoch 560/1000 --- Train Loss: 0.9418676032289915 --- Val Loss: 0.913527572881364 --- Train Acc: 0.74 --- Val Acc: 0.76\n",
      "Epoch 570/1000 --- Train Loss: 0.7561715317338699 --- Val Loss: 0.7085782377052626 --- Train Acc: 0.77 --- Val Acc: 0.79\n",
      "Epoch 580/1000 --- Train Loss: 0.6091818415913228 --- Val Loss: 0.548022783752137 --- Train Acc: 0.81 --- Val Acc: 0.83\n",
      "Epoch 590/1000 --- Train Loss: 0.5228998973134841 --- Val Loss: 0.44586443247691177 --- Train Acc: 0.85 --- Val Acc: 0.87\n",
      "Epoch 600/1000 --- Train Loss: 0.4458893150321892 --- Val Loss: 0.37087318065441577 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 610/1000 --- Train Loss: 0.3654858181509328 --- Val Loss: 0.30606666159128676 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 620/1000 --- Train Loss: 0.2994091947225754 --- Val Loss: 0.24372091482753142 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 630/1000 --- Train Loss: 0.24300021302010158 --- Val Loss: 0.18997082231883353 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 640/1000 --- Train Loss: 0.18871611619060258 --- Val Loss: 0.1520729090628999 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 650/1000 --- Train Loss: 0.15602578429525193 --- Val Loss: 0.12435559021149528 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 660/1000 --- Train Loss: 0.14646943018414507 --- Val Loss: 0.10493802604601819 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 670/1000 --- Train Loss: 0.11498582764529923 --- Val Loss: 0.08925840937551462 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 680/1000 --- Train Loss: 0.11026540019261774 --- Val Loss: 0.07827282064033543 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 690/1000 --- Train Loss: 0.08958711797857599 --- Val Loss: 0.06853937100129068 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 700/1000 --- Train Loss: 0.07787957945853304 --- Val Loss: 0.06094132160978646 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 710/1000 --- Train Loss: 0.06780690144906522 --- Val Loss: 0.05313431945428477 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 720/1000 --- Train Loss: 0.06283070999834893 --- Val Loss: 0.046691545629858534 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 730/1000 --- Train Loss: 0.053862473831147915 --- Val Loss: 0.04158702859929169 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 740/1000 --- Train Loss: 0.0468167385789554 --- Val Loss: 0.03599486108140855 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 750/1000 --- Train Loss: 0.05068371361042328 --- Val Loss: 0.03076012687025942 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 760/1000 --- Train Loss: 0.04438527876575687 --- Val Loss: 0.02553182599739841 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 770/1000 --- Train Loss: 0.03722178922972489 --- Val Loss: 0.021737211934181246 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 780/1000 --- Train Loss: 0.03624717741106512 --- Val Loss: 0.018677032461625664 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 790/1000 --- Train Loss: 0.03661305316874167 --- Val Loss: 0.015349935697508728 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.023259061196669574 --- Val Loss: 0.012994075147648434 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.030946722509114635 --- Val Loss: 0.01106325802260786 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.017007585862094853 --- Val Loss: 0.008989983287938428 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.02233720904603431 --- Val Loss: 0.008044848071067832 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.018986820489834605 --- Val Loss: 0.006705932927802418 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.020041533738312828 --- Val Loss: 0.005236961629427154 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.01668865514576229 --- Val Loss: 0.004713827620041307 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.015642137736360237 --- Val Loss: 0.004178797258029948 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.00836753224424255 --- Val Loss: 0.003640737024621462 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.019977446031931655 --- Val Loss: 0.00279498368704121 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.01765499068769581 --- Val Loss: 0.0026527720096007857 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.010715151689574412 --- Val Loss: 0.0020698979793199114 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.013783558426157985 --- Val Loss: 0.0016719240941748743 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.012613274127862381 --- Val Loss: 0.0016531228590747407 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.012609561734459028 --- Val Loss: 0.0014466687117590724 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.007705057683715058 --- Val Loss: 0.0011816670834609933 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.012818038360280429 --- Val Loss: 0.000983234815762859 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.007628904870504463 --- Val Loss: 0.0009489893798676838 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.007907585375420486 --- Val Loss: 0.0007618789738836569 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.012847866091031345 --- Val Loss: 0.000660036607334914 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.9722222222222222\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3013346911425834 --- Val Loss: 2.30657185918381 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/100 --- Train Loss: 1.1365681855178442 --- Val Loss: 1.0353638735398363 --- Train Acc: 0.55 --- Val Acc: 0.60\n",
      "Epoch 20/100 --- Train Loss: 2.301834370312089 --- Val Loss: 2.3059233968943045 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 30/100 --- Train Loss: 2.2996712231085303 --- Val Loss: 2.3061989992975427 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/100 --- Train Loss: 2.301703932645512 --- Val Loss: 2.3057238316260267 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/100 --- Train Loss: 2.299782910424652 --- Val Loss: 2.3061192362568526 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/100 --- Train Loss: 2.2957362158178305 --- Val Loss: 2.305372085377249 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.299652474305515 --- Val Loss: 2.306484758203912 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/100 --- Train Loss: 2.3017579578253335 --- Val Loss: 2.305689948269405 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/100 --- Train Loss: 2.301679177926871 --- Val Loss: 2.305722876590885 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3024644003216057 --- Val Loss: 2.3022609988373173 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 10/100 --- Train Loss: 2.3016111375972406 --- Val Loss: 2.299691508048761 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 20/100 --- Train Loss: 2.3011740721139944 --- Val Loss: 2.2980830572510493 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 30/100 --- Train Loss: 2.3008160990648374 --- Val Loss: 2.2968245176022415 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 40/100 --- Train Loss: 2.2993365273917084 --- Val Loss: 2.2949885354708326 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 50/100 --- Train Loss: 2.2662288417600465 --- Val Loss: 2.26628165285674 --- Train Acc: 0.28 --- Val Acc: 0.29\n",
      "Epoch 60/100 --- Train Loss: 1.9381728501444282 --- Val Loss: 1.9556409981722078 --- Train Acc: 0.40 --- Val Acc: 0.43\n",
      "Epoch 70/100 --- Train Loss: 1.3779569814532269 --- Val Loss: 1.3455076465332616 --- Train Acc: 0.65 --- Val Acc: 0.69\n",
      "Epoch 80/100 --- Train Loss: 0.6984840015604433 --- Val Loss: 0.5859669450928363 --- Train Acc: 0.85 --- Val Acc: 0.85\n",
      "Epoch 90/100 --- Train Loss: 0.43419804333380674 --- Val Loss: 0.3174341729939369 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.9333333333333333\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.2993190530731153 --- Val Loss: 2.298619043367813 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 1.58074841885818 --- Val Loss: 0.5993260280278974 --- Train Acc: 0.81 --- Val Acc: 0.86\n",
      "Epoch 20/100 --- Train Loss: 1.6938382702059578 --- Val Loss: 1.54357822907796 --- Train Acc: 0.39 --- Val Acc: 0.38\n",
      "Epoch 30/100 --- Train Loss: 2.2618492275880997 --- Val Loss: 2.234757638336086 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3078736097222943 --- Val Loss: 2.302273001044533 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.301876674343855 --- Val Loss: 2.301166779524206 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.3019085109204696 --- Val Loss: 2.300075235759617 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.3137914397062542 --- Val Loss: 2.3009755311027607 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: 2.3018714238414018 --- Val Loss: 2.300778598183223 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3018710214814093 --- Val Loss: 2.300155852161028 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.301777386826865 --- Val Loss: 2.3022081928507396 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 0.07184851686677454 --- Val Loss: 0.05241431586623281 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 20/100 --- Train Loss: 0.0070977535537738706 --- Val Loss: 0.0006525647479471373 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 30/100 --- Train Loss: 0.09896962775917843 --- Val Loss: 0.004360521372221571 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 40/100 --- Train Loss: 0.13828221138711938 --- Val Loss: 0.03460096455164087 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 50/100 --- Train Loss: 1.0700011077321465 --- Val Loss: 0.6504919390969719 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 60/100 --- Train Loss: 0.8141430072507747 --- Val Loss: 0.3616158061626836 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 70/100 --- Train Loss: 0.7825636017064264 --- Val Loss: 0.43309226948888463 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 80/100 --- Train Loss: 0.2616845461023603 --- Val Loss: 0.12179867350524706 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 90/100 --- Train Loss: 1.217176119263896 --- Val Loss: 0.6920400546911544 --- Train Acc: 0.90 --- Val Acc: 0.93\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.4861111111111111\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3008198980254706 --- Val Loss: 2.3020493008528784 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.300024723971557 --- Val Loss: 2.30319543267493 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.2611470509388147 --- Val Loss: 2.2642474925287166 --- Train Acc: 0.21 --- Val Acc: 0.20\n",
      "Epoch 30/500 --- Train Loss: 0.2514079201228878 --- Val Loss: 0.17615514958886724 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 40/500 --- Train Loss: 0.33771069740655896 --- Val Loss: 0.19986429149265653 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 50/500 --- Train Loss: 1.3172505955150902 --- Val Loss: 0.6787034741092652 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 60/500 --- Train Loss: 0.7919373462995615 --- Val Loss: 0.2779881391861247 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 70/500 --- Train Loss: 1.2205540893263522 --- Val Loss: 0.7561393707242403 --- Train Acc: 0.71 --- Val Acc: 0.77\n",
      "Epoch 80/500 --- Train Loss: 1.840771507015787 --- Val Loss: 1.8358254901710431 --- Train Acc: 0.31 --- Val Acc: 0.32\n",
      "Epoch 90/500 --- Train Loss: 2.2905804999743555 --- Val Loss: 2.237881381881897 --- Train Acc: 0.14 --- Val Acc: 0.14\n",
      "Epoch 100/500 --- Train Loss: 2.253499072505704 --- Val Loss: 2.289253749566429 --- Train Acc: 0.13 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: 2.3439357973543653 --- Val Loss: 2.3032203358765106 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.297989875508352 --- Val Loss: 2.3031776845464345 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/500 --- Train Loss: 2.3000548221538684 --- Val Loss: 2.3029188622066177 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3119687513407032 --- Val Loss: 2.302811714077063 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/500 --- Train Loss: 2.3000555796478883 --- Val Loss: 2.303203376804465 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: 2.3000845577913265 --- Val Loss: 2.3031698527542255 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/500 --- Train Loss: 2.300023748358807 --- Val Loss: 2.302850343263413 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: 2.3000622892749507 --- Val Loss: 2.3029086028664625 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/500 --- Train Loss: 2.300001254551964 --- Val Loss: 2.3026229353127037 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.299988980101067 --- Val Loss: 2.3026301865255947 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/500 --- Train Loss: 2.3000589977385912 --- Val Loss: 2.3028069421593513 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: 2.300135766317955 --- Val Loss: 2.3031695580085416 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.3000089920521574 --- Val Loss: 2.3023515385276663 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/500 --- Train Loss: 2.3000100166647597 --- Val Loss: 2.3028917210561066 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: 2.3000621449250094 --- Val Loss: 2.3022502893455403 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: 2.300016334261046 --- Val Loss: 2.3029966524898597 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: 2.300025529949907 --- Val Loss: 2.302809330163413 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: 2.3000205191686076 --- Val Loss: 2.3022909098658433 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/500 --- Train Loss: 2.30003486257282 --- Val Loss: 2.3030933768905952 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: 2.300015547658944 --- Val Loss: 2.3024506458256453 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/500 --- Train Loss: 2.2999847728068947 --- Val Loss: 2.302836737851736 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: 2.3000123488243185 --- Val Loss: 2.3024656547677043 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/500 --- Train Loss: 2.300004214310614 --- Val Loss: 2.3027624499692965 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/500 --- Train Loss: 2.3000076956597746 --- Val Loss: 2.302299060907328 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/500 --- Train Loss: 2.3000596083570475 --- Val Loss: 2.303302759938731 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/500 --- Train Loss: 2.3000456591875005 --- Val Loss: 2.3034290109457434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: 2.303227828129571 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3009963356501872 --- Val Loss: 2.3005143270326767 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/100 --- Train Loss: 0.6799376874263682 --- Val Loss: 0.521746381838577 --- Train Acc: 0.86 --- Val Acc: 0.84\n",
      "Epoch 20/100 --- Train Loss: 0.3902635012300391 --- Val Loss: 0.16724532952990662 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 30/100 --- Train Loss: 0.559775822812575 --- Val Loss: 0.18663880019172757 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 40/100 --- Train Loss: 0.5530729956132546 --- Val Loss: 0.3340350272258748 --- Train Acc: 0.92 --- Val Acc: 0.90\n",
      "Epoch 50/100 --- Train Loss: 1.008178466836464 --- Val Loss: 0.8277931652228203 --- Train Acc: 0.75 --- Val Acc: 0.74\n",
      "Epoch 60/100 --- Train Loss: 1.5953026946214883 --- Val Loss: 1.173004905108481 --- Train Acc: 0.61 --- Val Acc: 0.61\n",
      "Epoch 70/100 --- Train Loss: 2.0980413094229755 --- Val Loss: 2.0693364469334785 --- Train Acc: 0.21 --- Val Acc: 0.19\n",
      "Epoch 80/100 --- Train Loss: 2.3339114061932236 --- Val Loss: 2.2897033201076455 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 90/100 --- Train Loss: 2.3157889004453454 --- Val Loss: 2.2972884848115336 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.301692158996658 --- Val Loss: 2.3015079118972497 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 0.2893839900507066 --- Val Loss: 0.2009044627378885 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 20/500 --- Train Loss: 0.5265474766203304 --- Val Loss: 0.21006796719214155 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 30/500 --- Train Loss: 0.6625151018946212 --- Val Loss: 0.18762318018743113 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 40/500 --- Train Loss: 0.7512693694646726 --- Val Loss: 0.10358006822434164 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 50/500 --- Train Loss: 0.6803202848933322 --- Val Loss: 0.2762144891061385 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 60/500 --- Train Loss: 0.7108946985264633 --- Val Loss: 0.11232132335163082 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 70/500 --- Train Loss: 1.054318999220342 --- Val Loss: 0.111287252440789 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 80/500 --- Train Loss: 0.9696480300680588 --- Val Loss: 0.4735850412318146 --- Train Acc: 0.91 --- Val Acc: 0.93\n",
      "Epoch 90/500 --- Train Loss: 0.48611007472914125 --- Val Loss: 0.29744892547927737 --- Train Acc: 0.89 --- Val Acc: 0.91\n",
      "Epoch 100/500 --- Train Loss: 0.6569221458718215 --- Val Loss: 0.5065685772313185 --- Train Acc: 0.81 --- Val Acc: 0.81\n",
      "Epoch 110/500 --- Train Loss: 0.6902548671093841 --- Val Loss: 0.5936096321868644 --- Train Acc: 0.77 --- Val Acc: 0.79\n",
      "Epoch 120/500 --- Train Loss: 1.066745012611901 --- Val Loss: 0.8625357877856632 --- Train Acc: 0.67 --- Val Acc: 0.68\n",
      "Epoch 130/500 --- Train Loss: 1.1625057868482298 --- Val Loss: 1.125517255655476 --- Train Acc: 0.58 --- Val Acc: 0.56\n",
      "Epoch 140/500 --- Train Loss: 1.3051804393658013 --- Val Loss: 1.1644411863103987 --- Train Acc: 0.55 --- Val Acc: 0.55\n",
      "Epoch 150/500 --- Train Loss: 1.8632482704455928 --- Val Loss: 1.7994990254395544 --- Train Acc: 0.29 --- Val Acc: 0.30\n",
      "Epoch 160/500 --- Train Loss: 1.8345325835451358 --- Val Loss: 1.7618640215862331 --- Train Acc: 0.29 --- Val Acc: 0.34\n",
      "Epoch 170/500 --- Train Loss: 1.8331134578327408 --- Val Loss: 1.8080005806788615 --- Train Acc: 0.27 --- Val Acc: 0.32\n",
      "Epoch 180/500 --- Train Loss: 1.97354651143015 --- Val Loss: 1.9671222409917992 --- Train Acc: 0.23 --- Val Acc: 0.24\n",
      "Epoch 190/500 --- Train Loss: 2.068000557600501 --- Val Loss: 2.0570576139447954 --- Train Acc: 0.20 --- Val Acc: 0.20\n",
      "Epoch 200/500 --- Train Loss: 2.148362936746502 --- Val Loss: 2.1170226656259823 --- Train Acc: 0.18 --- Val Acc: 0.17\n",
      "Epoch 210/500 --- Train Loss: 2.1887845462584212 --- Val Loss: 2.2046637126832724 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 220/500 --- Train Loss: 2.16047394753388 --- Val Loss: 2.1897375710480516 --- Train Acc: 0.16 --- Val Acc: 0.14\n",
      "Epoch 230/500 --- Train Loss: 2.1482253783637284 --- Val Loss: 2.1805013753741442 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 240/500 --- Train Loss: 2.242057400330053 --- Val Loss: 2.2514380038551858 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 250/500 --- Train Loss: 2.2304055703624237 --- Val Loss: 2.2847612593770443 --- Train Acc: 0.13 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.266158008718574 --- Val Loss: 2.2720917822741393 --- Train Acc: 0.13 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: 2.2408051275536134 --- Val Loss: 2.2717161764751572 --- Train Acc: 0.13 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: 2.2113481548715934 --- Val Loss: 2.2064698343483733 --- Train Acc: 0.15 --- Val Acc: 0.13\n",
      "Epoch 290/500 --- Train Loss: 2.192165534448821 --- Val Loss: 2.203955594399995 --- Train Acc: 0.15 --- Val Acc: 0.13\n",
      "Epoch 300/500 --- Train Loss: 2.1987841457313317 --- Val Loss: 2.1929109645161233 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 310/500 --- Train Loss: 2.2021783671958253 --- Val Loss: 2.182396694040183 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 320/500 --- Train Loss: 2.173639929132337 --- Val Loss: 2.1821299184414533 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 330/500 --- Train Loss: 2.1677163493531277 --- Val Loss: 2.1819837141945793 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 340/500 --- Train Loss: 2.208472485556793 --- Val Loss: 2.181720907618109 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 350/500 --- Train Loss: 2.287336254053861 --- Val Loss: 2.299009384045343 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.287757408565697 --- Val Loss: 2.2989058708533303 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.2792252409733034 --- Val Loss: 2.2990165437922303 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.2878638213904234 --- Val Loss: 2.29906162952606 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.2898851358568786 --- Val Loss: 2.2991372463702846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: 2.2918918150723138 --- Val Loss: 2.299218083901382 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.285856135892427 --- Val Loss: 2.2992503230744084 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.2855407875006106 --- Val Loss: 2.299234242932889 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.283596685067848 --- Val Loss: 2.299296672183967 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.2816358643362085 --- Val Loss: 2.299111437109832 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.2855456001368246 --- Val Loss: 2.2991049706007165 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: 2.2751039404363067 --- Val Loss: 2.299152773819274 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: 2.281947716174501 --- Val Loss: 2.2992389506101323 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.2812866977235675 --- Val Loss: 2.299139450113479 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.271534325310959 --- Val Loss: 2.2991714418064375 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.08333333333333333\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302191872748493 --- Val Loss: 2.3022899176177343 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/100 --- Train Loss: 2.301789001327274 --- Val Loss: 2.3018384480379113 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 2.3017859086554995 --- Val Loss: 2.302020303595207 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: 2.30178820883572 --- Val Loss: 2.301952519987183 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/100 --- Train Loss: 2.301770024021003 --- Val Loss: 2.3019366941485804 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/100 --- Train Loss: 2.301681121324067 --- Val Loss: 2.3016105556022093 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.2942128710231717 --- Val Loss: 2.2939879770517124 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/100 --- Train Loss: 0.8990943117473144 --- Val Loss: 0.858074528657105 --- Train Acc: 0.68 --- Val Acc: 0.66\n",
      "Epoch 80/100 --- Train Loss: 0.21597365374795474 --- Val Loss: 0.15031914460633256 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 90/100 --- Train Loss: 0.12063866712369828 --- Val Loss: 0.03861371461713472 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.9555555555555556\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302011741387507 --- Val Loss: 2.3016813576627566 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.1628205579317967 --- Val Loss: 2.162537359745114 --- Train Acc: 0.35 --- Val Acc: 0.36\n",
      "Epoch 20/500 --- Train Loss: 0.5751268381499746 --- Val Loss: 0.41012696863646697 --- Train Acc: 0.89 --- Val Acc: 0.87\n",
      "Epoch 30/500 --- Train Loss: 0.41582022674355323 --- Val Loss: 0.14896675104976612 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 40/500 --- Train Loss: 0.3413223747658832 --- Val Loss: 0.07494244659081963 --- Train Acc: 0.97 --- Val Acc: 0.99\n",
      "Epoch 50/500 --- Train Loss: 0.362600456285288 --- Val Loss: 0.11229044453769928 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 60/500 --- Train Loss: 0.36747642689422366 --- Val Loss: 0.11071977036023337 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 70/500 --- Train Loss: 0.4458283579555376 --- Val Loss: 0.13347121235925305 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 80/500 --- Train Loss: 0.5087543119602876 --- Val Loss: 0.1271228523708431 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 90/500 --- Train Loss: 0.6035716284791067 --- Val Loss: 0.2846060281622734 --- Train Acc: 0.90 --- Val Acc: 0.90\n",
      "Epoch 100/500 --- Train Loss: 0.8802694780627334 --- Val Loss: 0.4296278851848972 --- Train Acc: 0.87 --- Val Acc: 0.90\n",
      "Epoch 110/500 --- Train Loss: 0.9032128631861044 --- Val Loss: 0.5322945906263106 --- Train Acc: 0.80 --- Val Acc: 0.83\n",
      "Epoch 120/500 --- Train Loss: 1.3251497901315654 --- Val Loss: 1.0853973243009245 --- Train Acc: 0.59 --- Val Acc: 0.56\n",
      "Epoch 130/500 --- Train Loss: 2.4216353726693707 --- Val Loss: 2.1484482273181 --- Train Acc: 0.40 --- Val Acc: 0.43\n",
      "Epoch 140/500 --- Train Loss: 1.9830381654959677 --- Val Loss: 1.9820929123142563 --- Train Acc: 0.25 --- Val Acc: 0.25\n",
      "Epoch 150/500 --- Train Loss: 2.2649072093288196 --- Val Loss: 2.2640141340169593 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 160/500 --- Train Loss: 2.0710516621165698 --- Val Loss: 1.9597456231515527 --- Train Acc: 0.26 --- Val Acc: 0.25\n",
      "Epoch 170/500 --- Train Loss: 2.301411567512028 --- Val Loss: 2.29945256694179 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.301315779798689 --- Val Loss: 2.298540420929059 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/500 --- Train Loss: 2.3007048051040977 --- Val Loss: 2.2984208948394427 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.30069577624509 --- Val Loss: 2.2983687842262763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3006948616471976 --- Val Loss: 2.298385663686306 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.3006952319194407 --- Val Loss: 2.298442027096511 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.3006939685655476 --- Val Loss: 2.298471995313537 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.300546535713105 --- Val Loss: 2.2983797391251253 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.312643004572622 --- Val Loss: 2.298381247163597 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.300693796849957 --- Val Loss: 2.298460826833928 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.3006945703622863 --- Val Loss: 2.2984565567576065 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.300694794632887 --- Val Loss: 2.2985146102870537 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.3006951271857083 --- Val Loss: 2.298506723766132 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.30069397785179 --- Val Loss: 2.2984781214634213 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.300694509498284 --- Val Loss: 2.298478411476919 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: 2.298629949851852 --- Val Loss: 2.298439568077816 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.300694409768093 --- Val Loss: 2.2984175297522285 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.2987077930226034 --- Val Loss: 2.298395697536961 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: 2.300694690575682 --- Val Loss: 2.2984643109822134 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.300694830766368 --- Val Loss: 2.2984264097648883 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.300695110548752 --- Val Loss: 2.2984237158787075 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.3006942196170166 --- Val Loss: 2.298437970816969 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.3006943031348555 --- Val Loss: 2.2984954192730354 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: 2.3006941056828745 --- Val Loss: 2.2984389516738135 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.3006942146271503 --- Val Loss: 2.2984822398867366 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.3006941766744586 --- Val Loss: 2.2984006824434733 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.30069363180309 --- Val Loss: 2.298475150228126 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.300693825409366 --- Val Loss: 2.2984631580644246 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.3006944865218215 --- Val Loss: 2.29849925071796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: 2.298629544135174 --- Val Loss: 2.2984103615824996 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: 2.3006938237475514 --- Val Loss: 2.2984994013973874 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.300694175348421 --- Val Loss: 2.298404788907194 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.300693719296105 --- Val Loss: 2.2984681403201446 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3018629734686997 --- Val Loss: 2.3015030743094442 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 0.7270506828021592 --- Val Loss: 0.35408770192563277 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 20/1000 --- Train Loss: 1.378211641634087 --- Val Loss: 0.5568533348665033 --- Train Acc: 0.87 --- Val Acc: 0.86\n",
      "Epoch 30/1000 --- Train Loss: 1.9192059566432933 --- Val Loss: 1.3241006982038168 --- Train Acc: 0.66 --- Val Acc: 0.64\n",
      "Epoch 40/1000 --- Train Loss: 1.3254269152412559 --- Val Loss: 1.010576478237522 --- Train Acc: 0.66 --- Val Acc: 0.63\n",
      "Epoch 50/1000 --- Train Loss: 1.7000690523585618 --- Val Loss: 1.5762991192400957 --- Train Acc: 0.48 --- Val Acc: 0.43\n",
      "Epoch 60/1000 --- Train Loss: 1.91165501514144 --- Val Loss: 1.942415626334587 --- Train Acc: 0.26 --- Val Acc: 0.24\n",
      "Epoch 70/1000 --- Train Loss: 2.119766334608233 --- Val Loss: 2.150249123610564 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 80/1000 --- Train Loss: 2.3052736696964753 --- Val Loss: 2.2918848687971565 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.2787292046121594 --- Val Loss: 2.2911089599623677 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 100/1000 --- Train Loss: 2.2734058241358204 --- Val Loss: 2.29172747304343 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: 2.2835750106794586 --- Val Loss: 2.299946545932757 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3016673388819484 --- Val Loss: 2.300601450401401 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.299609268460201 --- Val Loss: 2.300594702256074 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.3016684615767433 --- Val Loss: 2.3006783459088527 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.2954906090214053 --- Val Loss: 2.300869361703345 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.301695833571413 --- Val Loss: 2.300334641041669 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.313665036963791 --- Val Loss: 2.3004725095313883 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.2996116816249677 --- Val Loss: 2.3006768607906887 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.2996135610304087 --- Val Loss: 2.3004640000725005 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.301677707354681 --- Val Loss: 2.3001144200999084 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.301669201870351 --- Val Loss: 2.300817036546425 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.301670511961722 --- Val Loss: 2.3005640418330002 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.3016784801037167 --- Val Loss: 2.300293752471883 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.3016712284061405 --- Val Loss: 2.300557946359594 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3016752590281975 --- Val Loss: 2.300608645167798 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.3016741009000157 --- Val Loss: 2.300580818607274 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.3016743337908667 --- Val Loss: 2.3007241515251797 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.30166833093723 --- Val Loss: 2.3006330338288428 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.3016788848354883 --- Val Loss: 2.300889624408587 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3016668722974534 --- Val Loss: 2.300680836720034 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3016748642319276 --- Val Loss: 2.300525005496387 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.301670680594691 --- Val Loss: 2.300503203649889 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.3016683155316575 --- Val Loss: 2.3007524630507974 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.301681799849217 --- Val Loss: 2.300869808046078 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.301672314434613 --- Val Loss: 2.3005509980466137 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.301669667119154 --- Val Loss: 2.3006895286266675 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3016703611874965 --- Val Loss: 2.300820116423498 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.301673208316936 --- Val Loss: 2.3004932633063553 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.301672391314337 --- Val Loss: 2.3007448225343388 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3016707749802654 --- Val Loss: 2.3005955863445324 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.3016743362050684 --- Val Loss: 2.3008530379589827 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.301674575914997 --- Val Loss: 2.3010802038768414 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.301681702889762 --- Val Loss: 2.3005449367895965 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3016755428580353 --- Val Loss: 2.3001824058287275 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3016690607514656 --- Val Loss: 2.30076201468988 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3016682704195275 --- Val Loss: 2.3006295747333194 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3016772571896857 --- Val Loss: 2.3004178541573714 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.301669263352759 --- Val Loss: 2.3005838260280065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.301673122740732 --- Val Loss: 2.300570911790934 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.301676039667856 --- Val Loss: 2.3005736462213573 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.3016690521011895 --- Val Loss: 2.300742144910061 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.301673037805196 --- Val Loss: 2.3007503740565296 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.301669019836747 --- Val Loss: 2.30071676596134 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3016757684542886 --- Val Loss: 2.3001863596169936 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.3016746582880367 --- Val Loss: 2.300484562777311 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.301671589175952 --- Val Loss: 2.3006945954401785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.301669569485776 --- Val Loss: 2.3004870519017753 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.3016767896389103 --- Val Loss: 2.300486400312675 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.3016728084841054 --- Val Loss: 2.300721033595298 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.301672518214896 --- Val Loss: 2.300361041432983 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.301668458417653 --- Val Loss: 2.3007415855602926 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3016708250995386 --- Val Loss: 2.3002752103565456 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.3016700231006673 --- Val Loss: 2.30074375793107 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.30167037293508 --- Val Loss: 2.3007193849348755 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.3016714133482865 --- Val Loss: 2.3008805443455285 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3016713252821557 --- Val Loss: 2.30036835924087 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3016700351215986 --- Val Loss: 2.300661470440238 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.3016766596374683 --- Val Loss: 2.300293835830646 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.301675016414339 --- Val Loss: 2.300508514492391 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.301672380825828 --- Val Loss: 2.300513502569263 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.301678013296387 --- Val Loss: 2.3008724524789415 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.301675122103358 --- Val Loss: 2.3007501168880085 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3016662848333205 --- Val Loss: 2.3007084088376306 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.3016723963263743 --- Val Loss: 2.3007094069818663 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.301673295367344 --- Val Loss: 2.3009226236345586 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3016759064204733 --- Val Loss: 2.3009799919069365 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.3016716394305092 --- Val Loss: 2.3007885905096725 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.301670487000575 --- Val Loss: 2.3007816318059766 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.30166742392415 --- Val Loss: 2.3005216080082396 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.301682645236316 --- Val Loss: 2.3008510177840935 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.301672147217427 --- Val Loss: 2.3008447777180088 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3016723601860187 --- Val Loss: 2.300637871890346 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.301676220134262 --- Val Loss: 2.3007569380323196 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3016677233950347 --- Val Loss: 2.3005942907771924 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.3016836120591533 --- Val Loss: 2.300448000530886 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.3016734045051668 --- Val Loss: 2.300440192242865 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.301680774233039 --- Val Loss: 2.3009886237530788 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.3016702143259082 --- Val Loss: 2.3008665993141855 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3016700517247344 --- Val Loss: 2.3005021830595047 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3016679339255193 --- Val Loss: 2.300809965383841 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.301668496267084 --- Val Loss: 2.3005126311166095 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.3016702669575944 --- Val Loss: 2.300790452760111 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.3016751570337495 --- Val Loss: 2.30040362349852 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3016703480308482 --- Val Loss: 2.3008615899106175 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3016771193105936 --- Val Loss: 2.300352469044604 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.301674828083535 --- Val Loss: 2.3006343058200964 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: 2.3016747169492837 --- Val Loss: 2.300653347991167 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3016691873540482 --- Val Loss: 2.3004664760270455 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.301675069831951 --- Val Loss: 2.3005677377922082 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.08333333333333333\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3018787664788944 --- Val Loss: 2.3012765045262418 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/500 --- Train Loss: 0.592471727360687 --- Val Loss: 0.35293529635938814 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 20/500 --- Train Loss: 0.6578508674776532 --- Val Loss: 0.2126582172703392 --- Train Acc: 0.91 --- Val Acc: 0.93\n",
      "Epoch 30/500 --- Train Loss: 0.6035532413339562 --- Val Loss: 0.27658853671962624 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 40/500 --- Train Loss: 0.7907545411270515 --- Val Loss: 0.4989005840420522 --- Train Acc: 0.84 --- Val Acc: 0.83\n",
      "Epoch 50/500 --- Train Loss: 1.256539398950899 --- Val Loss: 0.8843824081036176 --- Train Acc: 0.67 --- Val Acc: 0.68\n",
      "Epoch 60/500 --- Train Loss: 1.7459493152845562 --- Val Loss: 1.6420478080602359 --- Train Acc: 0.38 --- Val Acc: 0.36\n",
      "Epoch 70/500 --- Train Loss: 2.2091950428053764 --- Val Loss: 2.2436668958083033 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.2673800878048147 --- Val Loss: 2.294730563697587 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/500 --- Train Loss: 2.325114482765555 --- Val Loss: 2.2994507854201016 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/500 --- Train Loss: 2.289083473005209 --- Val Loss: 2.298868406519753 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: 2.2950244377851163 --- Val Loss: 2.298806458116084 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: 2.301117287781593 --- Val Loss: 2.2986717829635634 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/500 --- Train Loss: 2.2991303610870832 --- Val Loss: 2.298643133027712 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/500 --- Train Loss: 2.297126330296781 --- Val Loss: 2.2985819094476825 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/500 --- Train Loss: 2.3131301763248695 --- Val Loss: 2.2986734416813923 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/500 --- Train Loss: 2.3011152804956976 --- Val Loss: 2.2989463556353495 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/500 --- Train Loss: 2.3011156444773557 --- Val Loss: 2.2988675601897843 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/500 --- Train Loss: 2.301118339854488 --- Val Loss: 2.2990223244271255 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.299107649325694 --- Val Loss: 2.298275973377504 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/500 --- Train Loss: 2.297114746247566 --- Val Loss: 2.2981999745256525 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/500 --- Train Loss: 2.3011184612002524 --- Val Loss: 2.298471384211891 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/500 --- Train Loss: 2.299108890297051 --- Val Loss: 2.2983530406485144 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.3011125714225447 --- Val Loss: 2.2991188883385103 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/500 --- Train Loss: 2.2991368952517175 --- Val Loss: 2.298960628306099 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/500 --- Train Loss: 2.3011202520186154 --- Val Loss: 2.2988161968157383 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/500 --- Train Loss: 2.299104482818059 --- Val Loss: 2.298610794802538 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/500 --- Train Loss: 2.3011201475485525 --- Val Loss: 2.2988159693595644 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/500 --- Train Loss: 2.3011151528553953 --- Val Loss: 2.2986073894299572 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.297143798184476 --- Val Loss: 2.2982833774194145 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/500 --- Train Loss: 2.2991099611370887 --- Val Loss: 2.2985421274453457 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/500 --- Train Loss: 2.301116418823542 --- Val Loss: 2.298794384738341 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/500 --- Train Loss: 2.3011175865743296 --- Val Loss: 2.298603623226789 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/500 --- Train Loss: 2.3011214078426745 --- Val Loss: 2.2987018027467627 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/500 --- Train Loss: 2.301115838487351 --- Val Loss: 2.298853231101769 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.2991077727376226 --- Val Loss: 2.2985259123860255 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/500 --- Train Loss: 2.2991091143986213 --- Val Loss: 2.2986332000985756 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/500 --- Train Loss: 2.2991025660082975 --- Val Loss: 2.298513905338534 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/500 --- Train Loss: 2.3011286038262937 --- Val Loss: 2.298582248132555 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/500 --- Train Loss: 2.299107729866565 --- Val Loss: 2.298655316209592 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: 2.3011188887091474 --- Val Loss: 2.298738595027967 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/500 --- Train Loss: 2.301112517956569 --- Val Loss: 2.298894452983909 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/500 --- Train Loss: 2.299125877781635 --- Val Loss: 2.2987055546364616 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/500 --- Train Loss: 2.3011263924221463 --- Val Loss: 2.2987534450936153 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/500 --- Train Loss: 2.301117771113349 --- Val Loss: 2.2983377589226204 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/500 --- Train Loss: 2.3011227612712273 --- Val Loss: 2.298975023859408 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/500 --- Train Loss: 2.3011120319446063 --- Val Loss: 2.2989375473467506 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/500 --- Train Loss: 2.2990995361291517 --- Val Loss: 2.298627436998698 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/500 --- Train Loss: 2.3011106999154056 --- Val Loss: 2.298600504952486 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/500 --- Train Loss: 2.2991004038207166 --- Val Loss: 2.2982405611117205 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.2984190249490966 --- Val Loss: 2.3007623380747875 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.308542775905636 --- Val Loss: 2.309974138322395 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 20/100 --- Train Loss: 2.313786172528936 --- Val Loss: 2.3066926271543426 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.3028862541501227 --- Val Loss: 2.3155653096252093 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3017583605470526 --- Val Loss: 2.306401139828357 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.3023397862818573 --- Val Loss: 2.305594950493544 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/100 --- Train Loss: 2.301328004151472 --- Val Loss: 2.3062097519909526 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/100 --- Train Loss: 2.301598504224205 --- Val Loss: 2.3095730886694876 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/100 --- Train Loss: 2.3015533797877405 --- Val Loss: 2.303763361124179 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/100 --- Train Loss: 2.303259432412284 --- Val Loss: 2.312792916785542 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.286803260158649 --- Val Loss: 2.286203728732371 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.4639688343844526 --- Val Loss: 2.305831144080569 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.302478931395849 --- Val Loss: 2.3025640229505195 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: 2.30174515631542 --- Val Loss: 2.3024889628304632 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 20/500 --- Train Loss: 2.3013972005405248 --- Val Loss: 2.3025777394492177 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 30/500 --- Train Loss: 2.301227283549968 --- Val Loss: 2.302700578721671 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/500 --- Train Loss: 2.3011455738952535 --- Val Loss: 2.302816913719993 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: 2.301106174911301 --- Val Loss: 2.3029105087555144 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/500 --- Train Loss: 2.30108768155359 --- Val Loss: 2.3029821475358863 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: 2.3010785327974808 --- Val Loss: 2.3030375259399616 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/500 --- Train Loss: 2.3010738985657917 --- Val Loss: 2.3030725797481537 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: 2.301071577708458 --- Val Loss: 2.303101456257577 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/500 --- Train Loss: 2.301070280960546 --- Val Loss: 2.3031170199651023 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: 2.3010695631037903 --- Val Loss: 2.303138359039518 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 120/500 --- Train Loss: 2.301069111967308 --- Val Loss: 2.3031379921601203 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 130/500 --- Train Loss: 2.3010688724093784 --- Val Loss: 2.3031318081931436 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 140/500 --- Train Loss: 2.3010686320206903 --- Val Loss: 2.30314319163282 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 150/500 --- Train Loss: 2.30106831332324 --- Val Loss: 2.3031399421310614 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: 2.3010680224506763 --- Val Loss: 2.3031368005945034 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: 2.3010677089777802 --- Val Loss: 2.303152388046323 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: 2.301067322873479 --- Val Loss: 2.303163673266477 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: 2.301066903595864 --- Val Loss: 2.3031612060872027 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: 2.301066457263799 --- Val Loss: 2.303148821813723 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: 2.3010659365981465 --- Val Loss: 2.3031555278551847 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: 2.3010652648906795 --- Val Loss: 2.303164462733404 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: 2.3010646173556206 --- Val Loss: 2.3031553851604065 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: 2.3010637007123305 --- Val Loss: 2.3031500402489593 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: 2.3010627120023557 --- Val Loss: 2.303151053057815 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: 2.301061471853525 --- Val Loss: 2.303139225709377 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: 2.301060345974046 --- Val Loss: 2.303136406597699 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: 2.3010585571358413 --- Val Loss: 2.3031326169532105 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: 2.301056750817423 --- Val Loss: 2.3031448167717707 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: 2.301054538111597 --- Val Loss: 2.3031342819741245 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: 2.3010513426684054 --- Val Loss: 2.3031271635969395 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: 2.301048014770521 --- Val Loss: 2.3031320403251248 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: 2.3010437638313386 --- Val Loss: 2.3031278521038048 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: 2.3010381705554654 --- Val Loss: 2.3031187767842396 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: 2.301031047730875 --- Val Loss: 2.3031100364900903 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: 2.3010226921391643 --- Val Loss: 2.3030963888570035 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: 2.301010816621222 --- Val Loss: 2.303080399392874 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: 2.300996662362953 --- Val Loss: 2.3030544560716955 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: 2.3009738065163656 --- Val Loss: 2.3030445613113657 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: 2.300947933918397 --- Val Loss: 2.3029939862851143 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: 2.3009079169124167 --- Val Loss: 2.302975769465896 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: 2.3008499592751783 --- Val Loss: 2.302917184653964 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: 2.3007676884739094 --- Val Loss: 2.3028346822879775 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: 2.300627478104486 --- Val Loss: 2.3026986823151523 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: 2.3004076709269428 --- Val Loss: 2.3024718938831654 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: 2.3000235469463073 --- Val Loss: 2.3020704834209265 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: 2.299277401337928 --- Val Loss: 2.3012581475581153 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: 2.2976440983988384 --- Val Loss: 2.2994874943917685 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: 2.2931741094972895 --- Val Loss: 2.294646034237175 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.15833333333333333\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.390505779430428 --- Val Loss: 2.375791362793735 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.3920878958721135 --- Val Loss: 2.381276861124815 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 20/1000 --- Train Loss: 2.348433365728962 --- Val Loss: 2.3387185337455354 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/1000 --- Train Loss: 2.4018762225787955 --- Val Loss: 2.436927380430962 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/1000 --- Train Loss: 2.4004374468318534 --- Val Loss: 2.412099731219719 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 50/1000 --- Train Loss: 2.332641694674542 --- Val Loss: 2.3200114611774483 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.3471699843509133 --- Val Loss: 2.326056773686859 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.4194297771286175 --- Val Loss: 2.4328707244340078 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.342842440972326 --- Val Loss: 2.3763244422043264 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 90/1000 --- Train Loss: 2.3604137257944666 --- Val Loss: 2.3208535253710205 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 100/1000 --- Train Loss: 2.342159493901069 --- Val Loss: 2.312793679858626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3349767896904443 --- Val Loss: 2.331338274699554 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/1000 --- Train Loss: 2.359534950398355 --- Val Loss: 2.3674816054198797 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/1000 --- Train Loss: 2.3936920795690058 --- Val Loss: 2.359599433250121 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: 2.327543206442943 --- Val Loss: 2.3317535401866167 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.435986271582282 --- Val Loss: 2.4595459792639285 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: 2.4171938741080594 --- Val Loss: 2.4183579113376514 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.331434316795419 --- Val Loss: 2.3163769285611853 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: 2.340593201285698 --- Val Loss: 2.346745739396441 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: 2.3823792461761575 --- Val Loss: 2.3732379651301803 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.3868961236314283 --- Val Loss: 2.350275086590981 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.3516452587325687 --- Val Loss: 2.3493957672419743 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.3731206258106914 --- Val Loss: 2.4021899538979556 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: 2.4183217886650046 --- Val Loss: 2.3921560223447784 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 240/1000 --- Train Loss: 2.3737508021952634 --- Val Loss: 2.3839367113123395 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 250/1000 --- Train Loss: 2.4218957322142747 --- Val Loss: 2.428858699946804 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.301630440968678 --- Val Loss: 2.303249016989709 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.3012055439219883 --- Val Loss: 2.302371615342255 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.301283161462564 --- Val Loss: 2.304200410150447 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.3011965061386093 --- Val Loss: 2.302632066408278 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 1.731340619913279 --- Val Loss: 1.7215703326748077 --- Train Acc: 0.22 --- Val Acc: 0.24\n",
      "Epoch 50/100 --- Train Loss: 0.2286041377476793 --- Val Loss: 0.1672287410222699 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 60/100 --- Train Loss: 0.051424622861908596 --- Val Loss: 0.022309094001821864 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 70/100 --- Train Loss: 0.024072556684042987 --- Val Loss: 0.008498708245008793 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.01759877027509941 --- Val Loss: 0.0041834275723764925 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.01949694601254192 --- Val Loss: 0.004053739272330736 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.975\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.741191881908767 --- Val Loss: 2.3633930824013136 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 10/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.3025317117809707 --- Val Loss: 2.3024968012003835 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/100 --- Train Loss: 2.302092297393514 --- Val Loss: 2.3017397333246303 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 2.301778125782458 --- Val Loss: 2.3011589125872423 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: 2.3015405572484284 --- Val Loss: 2.3006992651199156 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/100 --- Train Loss: 2.3013301988820336 --- Val Loss: 2.3003078464763163 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/100 --- Train Loss: 2.3010477422496938 --- Val Loss: 2.299859442289862 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.3002608266845854 --- Val Loss: 2.2989054080148756 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/100 --- Train Loss: 2.296398288041143 --- Val Loss: 2.2946797615944097 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/100 --- Train Loss: 2.268252843628729 --- Val Loss: 2.2657236177211586 --- Train Acc: 0.23 --- Val Acc: 0.24\n",
      "Epoch 90/100 --- Train Loss: 2.1144808904570858 --- Val Loss: 2.1098434812275366 --- Train Acc: 0.16 --- Val Acc: 0.16\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.3111111111111111\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.4304791832474586 --- Val Loss: 2.38629799397751 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 10/500 --- Train Loss: 2.3487680452951176 --- Val Loss: 2.350480904354274 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 20/500 --- Train Loss: 2.3394289927480014 --- Val Loss: 2.3315623955430653 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: 2.316824856364479 --- Val Loss: 2.3329735520984825 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 40/500 --- Train Loss: 2.3359002144029053 --- Val Loss: 2.3362565268282096 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/500 --- Train Loss: 2.3303924263610436 --- Val Loss: 2.3173667590968012 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/500 --- Train Loss: 2.3482819806011883 --- Val Loss: 2.353242497450048 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: 2.344460892272011 --- Val Loss: 2.346289592955529 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.3134565458252956 --- Val Loss: 2.303411321753175 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/500 --- Train Loss: 2.3286140817879635 --- Val Loss: 2.3328802067733854 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 100/500 --- Train Loss: 2.332042380148106 --- Val Loss: 2.317342659884302 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.3598039280224468 --- Val Loss: 2.35740465265684 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/500 --- Train Loss: 2.340528240906395 --- Val Loss: 2.3250795851263564 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/500 --- Train Loss: 2.325497161275728 --- Val Loss: 2.3199178757818246 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.341818212009928 --- Val Loss: 2.3666986894125754 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 150/500 --- Train Loss: 2.3555407022715436 --- Val Loss: 2.3642937356719096 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: 2.349355985710916 --- Val Loss: 2.3480276446375337 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 170/500 --- Train Loss: 2.3412454441998554 --- Val Loss: 2.309691988317989 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: 2.362297442371309 --- Val Loss: 2.33914855772736 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 190/500 --- Train Loss: 2.3243438163110626 --- Val Loss: 2.3260855249814494 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.3384740176876595 --- Val Loss: 2.3108975633017277 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/500 --- Train Loss: 2.3178856796348697 --- Val Loss: 2.3216255245461626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: 2.3428364645543454 --- Val Loss: 2.3549704314458983 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 230/500 --- Train Loss: 2.341139858028432 --- Val Loss: 2.317217846902961 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 240/500 --- Train Loss: 2.3361531805047777 --- Val Loss: 2.3411175488679397 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: 2.3400646432342422 --- Val Loss: 2.3211237551553294 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: 2.3434674960152826 --- Val Loss: 2.3175623203208104 --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: 2.3556470297958065 --- Val Loss: 2.3395329989921456 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/500 --- Train Loss: 2.350687554650784 --- Val Loss: 2.34816476690389 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 290/500 --- Train Loss: 2.3179417206736197 --- Val Loss: 2.3066354061008467 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: 2.3595291252284527 --- Val Loss: 2.3778024041001107 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.326583083534883 --- Val Loss: 2.33006454386802 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/500 --- Train Loss: 2.326625394341406 --- Val Loss: 2.3263047439537994 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/500 --- Train Loss: 2.343009100480618 --- Val Loss: 2.359866574556353 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 340/500 --- Train Loss: 2.3214589933357583 --- Val Loss: 2.3347795529683073 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 350/500 --- Train Loss: 2.3083767427723605 --- Val Loss: 2.3208920663620534 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 360/500 --- Train Loss: 2.3182245421609675 --- Val Loss: 2.3373798544325526 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 370/500 --- Train Loss: 2.330496512575895 --- Val Loss: 2.3228737405948223 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/500 --- Train Loss: 2.3318452018109954 --- Val Loss: 2.3426144664144157 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.317645180465285 --- Val Loss: 2.3099434112712314 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/500 --- Train Loss: 2.355638010961134 --- Val Loss: 2.3391806066744976 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.328506838679211 --- Val Loss: 2.3225283965383943 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/500 --- Train Loss: 2.347501852127497 --- Val Loss: 2.344182925146643 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 430/500 --- Train Loss: 2.3123720889045614 --- Val Loss: 2.31262808962285 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/500 --- Train Loss: 2.319550262886253 --- Val Loss: 2.3199994648244138 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 450/500 --- Train Loss: 2.323827334536885 --- Val Loss: 2.316993093469905 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 460/500 --- Train Loss: 2.3608737835022198 --- Val Loss: 2.3789726774057267 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 470/500 --- Train Loss: 2.3286641391754985 --- Val Loss: 2.336962043961986 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 480/500 --- Train Loss: 2.3245953276931886 --- Val Loss: 2.3386172244991092 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 490/500 --- Train Loss: 2.3376949725290253 --- Val Loss: 2.3471904214867543 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 3.6519760590838954 --- Val Loss: 3.374999895831552 --- Train Acc: 0.53 --- Val Acc: 0.51\n",
      "Epoch 10/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.301758440585179 --- Val Loss: 2.3044706534528347 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/100 --- Train Loss: 0.39246434891191917 --- Val Loss: 0.22073952340878294 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 20/100 --- Train Loss: 0.8656721907682376 --- Val Loss: 0.35749232501832834 --- Train Acc: 0.92 --- Val Acc: 0.93\n",
      "Epoch 30/100 --- Train Loss: 2.0963368483511027 --- Val Loss: 2.0904034844260653 --- Train Acc: 0.21 --- Val Acc: 0.16\n",
      "Epoch 40/100 --- Train Loss: 2.052122235740049 --- Val Loss: 2.0750413748480714 --- Train Acc: 0.21 --- Val Acc: 0.16\n",
      "Epoch 50/100 --- Train Loss: 2.0223238580060228 --- Val Loss: 2.109334182160051 --- Train Acc: 0.20 --- Val Acc: 0.15\n",
      "Epoch 60/100 --- Train Loss: 2.360533171157815 --- Val Loss: 2.305306725420735 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.2936742521514133 --- Val Loss: 2.3072304066025797 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.313755462183798 --- Val Loss: 2.3068696307324132 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/100 --- Train Loss: 2.3138410681647543 --- Val Loss: 2.307821384342873 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.300995260094775 --- Val Loss: 2.298377558825104 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/500 --- Train Loss: 2.311628858725456 --- Val Loss: 2.303289031805626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.3202531613972677 --- Val Loss: 2.299030587042101 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: 2.2995480682701275 --- Val Loss: 2.2977579329604487 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.311969222140337 --- Val Loss: 2.3013813841154596 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/500 --- Train Loss: 2.3134740296847447 --- Val Loss: 2.301237106911916 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/500 --- Train Loss: 2.3132719051841875 --- Val Loss: 2.2977158565160405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/500 --- Train Loss: 2.3255609645521784 --- Val Loss: 2.2993273356194477 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/500 --- Train Loss: 2.3011931773749077 --- Val Loss: 2.2990908026921617 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/500 --- Train Loss: 2.299221499957159 --- Val Loss: 2.2989621341133657 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/500 --- Train Loss: 2.31354086639327 --- Val Loss: 2.298244755185919 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/500 --- Train Loss: 2.2991449563686936 --- Val Loss: 2.299244236187838 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/500 --- Train Loss: 2.299268775004431 --- Val Loss: 2.298688800701975 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/500 --- Train Loss: 2.3014002724364677 --- Val Loss: 2.2993361588885963 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/500 --- Train Loss: 2.297237036515353 --- Val Loss: 2.300928182292247 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/500 --- Train Loss: 2.301702955158226 --- Val Loss: 2.3000330598558736 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: 2.2999823428374446 --- Val Loss: 2.3014080872038902 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/500 --- Train Loss: 2.3017483297809385 --- Val Loss: 2.2990925884535662 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/500 --- Train Loss: 2.3014217743786105 --- Val Loss: 2.299777641513185 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/500 --- Train Loss: 2.3016206788730154 --- Val Loss: 2.2984092277783876 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.3013953834258927 --- Val Loss: 2.2998758587637904 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/500 --- Train Loss: 2.301133976832978 --- Val Loss: 2.2983591783512725 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: 2.3012580040071366 --- Val Loss: 2.298814532137572 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/500 --- Train Loss: 2.3015404235670345 --- Val Loss: 2.298110354272951 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/500 --- Train Loss: 2.3013631293308747 --- Val Loss: 2.300557073389001 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: 2.3019987062352745 --- Val Loss: 2.2954543547172883 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: 2.301297570143007 --- Val Loss: 2.2970755718599785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: 2.3012209886333093 --- Val Loss: 2.2972516776320013 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: 2.3013162231881226 --- Val Loss: 2.3011061335105403 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/500 --- Train Loss: 2.301514659014083 --- Val Loss: 2.297905199532009 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: 2.3015702200458192 --- Val Loss: 2.298482826825429 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/500 --- Train Loss: 2.301172467102185 --- Val Loss: 2.298161292613245 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: 2.301625228397605 --- Val Loss: 2.2979376365330286 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 330/500 --- Train Loss: 2.3019191908425034 --- Val Loss: 2.2955553461257927 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 340/500 --- Train Loss: 2.3139561132047755 --- Val Loss: 2.3005432670179036 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/500 --- Train Loss: 2.301453954352278 --- Val Loss: 2.3014417519817547 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/500 --- Train Loss: 2.3015646972923163 --- Val Loss: 2.298607770966293 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/500 --- Train Loss: 2.3012057967488277 --- Val Loss: 2.3003928426290536 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/500 --- Train Loss: 2.301272644589215 --- Val Loss: 2.301333807115617 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/500 --- Train Loss: 2.301206474555338 --- Val Loss: 2.2991619397756686 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: 2.301574158013757 --- Val Loss: 2.298971496282591 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/500 --- Train Loss: 2.301615923254228 --- Val Loss: 2.3004165156189034 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/500 --- Train Loss: 2.301284497602166 --- Val Loss: 2.2979979730196174 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/500 --- Train Loss: 2.301861700681576 --- Val Loss: 2.296870762716425 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 440/500 --- Train Loss: 2.3022183072677707 --- Val Loss: 2.297176268601988 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/500 --- Train Loss: 2.3015252210556216 --- Val Loss: 2.3015725563621414 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/500 --- Train Loss: 2.3014652367151878 --- Val Loss: 2.2968662838620397 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/500 --- Train Loss: 2.3013752334897957 --- Val Loss: 2.300417283674413 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/500 --- Train Loss: 2.3013570504224083 --- Val Loss: 2.2984385584779377 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 490/500 --- Train Loss: 2.301165071053521 --- Val Loss: 2.298711034493179 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 3.7856910931270504 --- Val Loss: 3.864842835550077 --- Train Acc: 0.46 --- Val Acc: 0.45\n",
      "Epoch 10/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 40/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.302242479680644 --- Val Loss: 2.302357326170117 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.3015350730990796 --- Val Loss: 2.302148770288842 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/1000 --- Train Loss: 2.301512918453205 --- Val Loss: 2.3021863256773814 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: 2.30151077092814 --- Val Loss: 2.3022149793791202 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.3015096336787058 --- Val Loss: 2.3020569200412853 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/1000 --- Train Loss: 2.3015053450812593 --- Val Loss: 2.3021602723643393 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/1000 --- Train Loss: 2.3014958876900597 --- Val Loss: 2.302212270565559 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.3014733919831105 --- Val Loss: 2.302181626413524 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.3013961007546855 --- Val Loss: 2.3021008840235964 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: 2.300952099149878 --- Val Loss: 2.3015320158532115 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 100/1000 --- Train Loss: 2.28044982023833 --- Val Loss: 2.2810241221967615 --- Train Acc: 0.21 --- Val Acc: 0.23\n",
      "Epoch 110/1000 --- Train Loss: 1.5073075003667753 --- Val Loss: 1.4841560282064448 --- Train Acc: 0.44 --- Val Acc: 0.45\n",
      "Epoch 120/1000 --- Train Loss: 0.6158364599232508 --- Val Loss: 0.5625113180834405 --- Train Acc: 0.82 --- Val Acc: 0.84\n",
      "Epoch 130/1000 --- Train Loss: 0.2682385862431832 --- Val Loss: 0.23539912226864487 --- Train Acc: 0.93 --- Val Acc: 0.92\n",
      "Epoch 140/1000 --- Train Loss: 0.14109640499422826 --- Val Loss: 0.09710879265288147 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 150/1000 --- Train Loss: 0.07909700291511122 --- Val Loss: 0.04466985350792752 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 160/1000 --- Train Loss: 0.06193709649434794 --- Val Loss: 0.027041452717866545 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 170/1000 --- Train Loss: 0.04234517304125427 --- Val Loss: 0.018807738164391893 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 180/1000 --- Train Loss: 0.036323414933017245 --- Val Loss: 0.013022865134617818 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/1000 --- Train Loss: 0.036112099031303864 --- Val Loss: 0.004878985211580677 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/1000 --- Train Loss: 0.03522262439470189 --- Val Loss: 0.005475162887892307 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/1000 --- Train Loss: 0.016713699722919067 --- Val Loss: 0.0009788611541664558 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/1000 --- Train Loss: 0.03876872272118618 --- Val Loss: 0.0025786277333602775 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/1000 --- Train Loss: 0.04089443384801273 --- Val Loss: 0.017989968115856753 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.043689069890483455 --- Val Loss: 0.009331847949845602 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.07865641366858458 --- Val Loss: 0.010733828675839314 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.04011965195255047 --- Val Loss: 0.0005939774479812565 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.08281789297435296 --- Val Loss: 0.0015132165542443197 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.40805325220252825 --- Val Loss: 0.3525106251442013 --- Train Acc: 0.90 --- Val Acc: 0.90\n",
      "Epoch 290/1000 --- Train Loss: 0.06204404935341198 --- Val Loss: 0.005374751755962017 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.018537741231551195 --- Val Loss: 0.00459729060432344 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.1250691107264501 --- Val Loss: 0.014095713765807557 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 320/1000 --- Train Loss: 0.12041270058848108 --- Val Loss: 0.02932811165651316 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 330/1000 --- Train Loss: 0.8145416143330833 --- Val Loss: 0.7099197403142249 --- Train Acc: 0.75 --- Val Acc: 0.76\n",
      "Epoch 340/1000 --- Train Loss: 1.2637657306958048 --- Val Loss: 1.270891220231836 --- Train Acc: 0.54 --- Val Acc: 0.50\n",
      "Epoch 350/1000 --- Train Loss: 0.8465986448263172 --- Val Loss: 0.8654835352307017 --- Train Acc: 0.69 --- Val Acc: 0.68\n",
      "Epoch 360/1000 --- Train Loss: 1.5295712299228668 --- Val Loss: 1.5616509828283494 --- Train Acc: 0.45 --- Val Acc: 0.43\n",
      "Epoch 370/1000 --- Train Loss: 1.3766781212370096 --- Val Loss: 1.3242424343172856 --- Train Acc: 0.50 --- Val Acc: 0.48\n",
      "Epoch 380/1000 --- Train Loss: 1.3038185208397781 --- Val Loss: 1.3064548531728604 --- Train Acc: 0.49 --- Val Acc: 0.48\n",
      "Epoch 390/1000 --- Train Loss: 1.654547008253237 --- Val Loss: 1.7027066779529199 --- Train Acc: 0.38 --- Val Acc: 0.33\n",
      "Epoch 400/1000 --- Train Loss: 1.8120933354009934 --- Val Loss: 1.6716800256057096 --- Train Acc: 0.35 --- Val Acc: 0.34\n",
      "Epoch 410/1000 --- Train Loss: 2.245438762145287 --- Val Loss: 2.2216972553360437 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: 2.1015570429312853 --- Val Loss: 2.1089025642393233 --- Train Acc: 0.19 --- Val Acc: 0.21\n",
      "Epoch 430/1000 --- Train Loss: 2.046464129953981 --- Val Loss: 2.0865836711564687 --- Train Acc: 0.25 --- Val Acc: 0.22\n",
      "Epoch 440/1000 --- Train Loss: 2.2653828173363784 --- Val Loss: 2.25602837199822 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.2482296308833667 --- Val Loss: 2.242312940475998 --- Train Acc: 0.13 --- Val Acc: 0.16\n",
      "Epoch 460/1000 --- Train Loss: 2.224562860978271 --- Val Loss: 2.2423941846334756 --- Train Acc: 0.14 --- Val Acc: 0.16\n",
      "Epoch 470/1000 --- Train Loss: 2.243852022492405 --- Val Loss: 2.242343634312837 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.240239638418723 --- Val Loss: 2.2425836017007996 --- Train Acc: 0.14 --- Val Acc: 0.16\n",
      "Epoch 490/1000 --- Train Loss: 2.241904667070247 --- Val Loss: 2.2419135681796676 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 500/1000 --- Train Loss: 2.241440037899527 --- Val Loss: 2.2433852335262077 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.293523814021598 --- Val Loss: 2.302624599071795 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 520/1000 --- Train Loss: 2.297390817896952 --- Val Loss: 2.302802953161828 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.2994900605113426 --- Val Loss: 2.302439660524546 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.301517819614831 --- Val Loss: 2.302440337528278 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 550/1000 --- Train Loss: 2.2933534953175507 --- Val Loss: 2.302259202292401 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: 2.297394532096593 --- Val Loss: 2.302185140592157 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: 2.30151667643939 --- Val Loss: 2.302428846199105 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 580/1000 --- Train Loss: 2.297392634128367 --- Val Loss: 2.3024550670686303 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3015185374919516 --- Val Loss: 2.3024379130234207 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 600/1000 --- Train Loss: 2.301517447488741 --- Val Loss: 2.3026011506021185 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.29945411187172 --- Val Loss: 2.3024966210604987 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 620/1000 --- Train Loss: 2.2994535600215094 --- Val Loss: 2.302469680440101 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 630/1000 --- Train Loss: 2.2973883649522118 --- Val Loss: 2.302460136478019 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 640/1000 --- Train Loss: 2.2974828395528624 --- Val Loss: 2.302356099527224 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.2973881634471254 --- Val Loss: 2.3023789476139864 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.295387573548114 --- Val Loss: 2.3023549438786226 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.2994536017751157 --- Val Loss: 2.3026397610172973 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: 2.301514905585235 --- Val Loss: 2.3024387574900227 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.2953278224798304 --- Val Loss: 2.3026418940902778 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.2995121895146813 --- Val Loss: 2.3022297893537798 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.299539896025689 --- Val Loss: 2.3022102331714462 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.301514401531792 --- Val Loss: 2.302200740139173 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.2994563899299894 --- Val Loss: 2.302161407743013 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.3015135618185343 --- Val Loss: 2.3022236061513026 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: 2.3015132384434773 --- Val Loss: 2.3023419334260233 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.2994570301943034 --- Val Loss: 2.302163105098017 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.2994555725086663 --- Val Loss: 2.3023780564146765 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.3015149735598053 --- Val Loss: 2.3021100685128792 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.3015140674372008 --- Val Loss: 2.3022744520881213 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.30151336293766 --- Val Loss: 2.3023199248020823 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3015136674136576 --- Val Loss: 2.302397214883396 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.3015134094838197 --- Val Loss: 2.3023151533290376 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.301514284708499 --- Val Loss: 2.3023456218387186 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.3015145517619677 --- Val Loss: 2.302209657506023 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: 2.29756577732679 --- Val Loss: 2.3022648417607847 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.3015146565940428 --- Val Loss: 2.302195446584713 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.297537071824083 --- Val Loss: 2.3020381139717268 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: 2.3015149440248552 --- Val Loss: 2.302415917832238 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.3015137832479002 --- Val Loss: 2.3022966111345666 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: 2.299510552733479 --- Val Loss: 2.30224319237955 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.3015140654498727 --- Val Loss: 2.30219583605413 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.301513989263003 --- Val Loss: 2.302165767692778 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.3015138786986657 --- Val Loss: 2.3021692526313364 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.3015142535970883 --- Val Loss: 2.3022368581259918 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.301515330663484 --- Val Loss: 2.3020006533171977 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.301514332030037 --- Val Loss: 2.302247402718916 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.3015140395223943 --- Val Loss: 2.3020911724684856 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.3015140532754965 --- Val Loss: 2.302074371994963 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: 2.2995384863480046 --- Val Loss: 2.30221963042071 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3008971227068105 --- Val Loss: 2.2981918743225473 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.300364757783457 --- Val Loss: 2.2953887229175667 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 0.2534636822216033 --- Val Loss: 0.20638267102181665 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 30/1000 --- Train Loss: 0.06881627927645012 --- Val Loss: 0.012135411429022625 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/1000 --- Train Loss: 0.17589254233241644 --- Val Loss: 0.03514317867850415 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 50/1000 --- Train Loss: 0.9883828046616706 --- Val Loss: 0.866709964606039 --- Train Acc: 0.71 --- Val Acc: 0.70\n",
      "Epoch 60/1000 --- Train Loss: 1.633486792399691 --- Val Loss: 1.6307627212563103 --- Train Acc: 0.38 --- Val Acc: 0.39\n",
      "Epoch 70/1000 --- Train Loss: 1.9035487672591895 --- Val Loss: 1.9611523312039654 --- Train Acc: 0.26 --- Val Acc: 0.26\n",
      "Epoch 80/1000 --- Train Loss: 1.8538913660293526 --- Val Loss: 1.9625300362619438 --- Train Acc: 0.26 --- Val Acc: 0.26\n",
      "Epoch 90/1000 --- Train Loss: 2.29657420774137 --- Val Loss: 2.2862240822567608 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 100/1000 --- Train Loss: 2.2965256625144588 --- Val Loss: 2.2940026814506647 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.2945255042142065 --- Val Loss: 2.294185542536736 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.300571063991677 --- Val Loss: 2.2942673304449746 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.301498073664479 --- Val Loss: 2.3016008312290834 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 0.3060029007467902 --- Val Loss: 0.1932449504444969 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 20/100 --- Train Loss: 0.21854199743720423 --- Val Loss: 0.06684561320008613 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 30/100 --- Train Loss: 0.2216353689244668 --- Val Loss: 0.06224987267429095 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/100 --- Train Loss: 0.4549598059294846 --- Val Loss: 0.1151895232773093 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 50/100 --- Train Loss: 0.4526456822686198 --- Val Loss: 0.14126335260215034 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 60/100 --- Train Loss: 0.4685483505547965 --- Val Loss: 0.16331939577992202 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 70/100 --- Train Loss: 0.44003284088802547 --- Val Loss: 0.39200166792149055 --- Train Acc: 0.94 --- Val Acc: 0.91\n",
      "Epoch 80/100 --- Train Loss: 0.6192307544638915 --- Val Loss: 0.2759921141796536 --- Train Acc: 0.93 --- Val Acc: 0.92\n",
      "Epoch 90/100 --- Train Loss: 0.8358515576765194 --- Val Loss: 0.7078110518687031 --- Train Acc: 0.82 --- Val Acc: 0.82\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.7388888888888889\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.2987117622993547 --- Val Loss: 2.2989646209635692 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 10/1000 --- Train Loss: 1.163038422218523 --- Val Loss: 0.514785604729238 --- Train Acc: 0.87 --- Val Acc: 0.87\n",
      "Epoch 20/1000 --- Train Loss: 1.2685038923581078 --- Val Loss: 0.8304463534938435 --- Train Acc: 0.66 --- Val Acc: 0.68\n",
      "Epoch 30/1000 --- Train Loss: 2.127878391988838 --- Val Loss: 2.117581908630368 --- Train Acc: 0.21 --- Val Acc: 0.17\n",
      "Epoch 40/1000 --- Train Loss: 2.30441705856665 --- Val Loss: 2.3063405998443565 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.3622631555444626 --- Val Loss: 2.304815040446671 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: 2.302244012944454 --- Val Loss: 2.3042495873997644 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.326296016027507 --- Val Loss: 2.3056309699481727 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: 2.302253861322282 --- Val Loss: 2.3043848689141697 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: 2.302251971559126 --- Val Loss: 2.305100810868305 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: 2.3022199446526206 --- Val Loss: 2.304398902265015 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: 2.300339018976618 --- Val Loss: 2.303581496765417 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.302196077727257 --- Val Loss: 2.3051186811259656 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.302247034061856 --- Val Loss: 2.30577568816192 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.302236160920226 --- Val Loss: 2.305504705854069 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: 2.3022140362445294 --- Val Loss: 2.3041113601313454 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 160/1000 --- Train Loss: 2.3021982471451796 --- Val Loss: 2.3036171166188444 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 170/1000 --- Train Loss: 2.302209024986015 --- Val Loss: 2.305539271645001 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: 2.3021755207707786 --- Val Loss: 2.3040905987165288 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 190/1000 --- Train Loss: 2.302241309653177 --- Val Loss: 2.3044883247676173 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 200/1000 --- Train Loss: 2.302273064652689 --- Val Loss: 2.305929654037776 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 210/1000 --- Train Loss: 2.3022610464010222 --- Val Loss: 2.3059384617241925 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 220/1000 --- Train Loss: 2.3021974296089067 --- Val Loss: 2.3041581127003385 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 230/1000 --- Train Loss: 2.3022324915312744 --- Val Loss: 2.3036173631616275 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 240/1000 --- Train Loss: 2.302201939071932 --- Val Loss: 2.30448668712438 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 250/1000 --- Train Loss: 2.302218816013716 --- Val Loss: 2.3049983166098826 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 260/1000 --- Train Loss: 2.302184865950192 --- Val Loss: 2.305085607527928 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 270/1000 --- Train Loss: 2.3022010085420272 --- Val Loss: 2.304290737458383 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 280/1000 --- Train Loss: 2.3022287047198575 --- Val Loss: 2.3049879944640663 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: 2.302191354889789 --- Val Loss: 2.304131575343969 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 300/1000 --- Train Loss: 2.302249538431111 --- Val Loss: 2.3041641034053 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 310/1000 --- Train Loss: 2.3022134687525613 --- Val Loss: 2.3044610479011345 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: 2.3022309350495647 --- Val Loss: 2.3046188054443792 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 330/1000 --- Train Loss: 2.302206091801394 --- Val Loss: 2.3058545461642335 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: 2.302234654822042 --- Val Loss: 2.3039363277757974 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 350/1000 --- Train Loss: 2.302182456655205 --- Val Loss: 2.3049870274859083 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 360/1000 --- Train Loss: 2.3022818267899567 --- Val Loss: 2.3057847361956205 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 370/1000 --- Train Loss: 2.302209772975689 --- Val Loss: 2.3051737215012196 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 380/1000 --- Train Loss: 2.302205347482018 --- Val Loss: 2.30461516470159 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 390/1000 --- Train Loss: 2.3022317376819164 --- Val Loss: 2.304846298646741 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 400/1000 --- Train Loss: 2.302188366053857 --- Val Loss: 2.3049190594473923 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 410/1000 --- Train Loss: 2.302176500112267 --- Val Loss: 2.3045960429015797 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 420/1000 --- Train Loss: 2.3022004161944847 --- Val Loss: 2.3039713761315137 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 430/1000 --- Train Loss: 2.302216868899087 --- Val Loss: 2.3053273703261836 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 440/1000 --- Train Loss: 2.3023402995675237 --- Val Loss: 2.306338479066141 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 450/1000 --- Train Loss: 2.302258525062846 --- Val Loss: 2.305670533358884 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 460/1000 --- Train Loss: 2.3021949142911096 --- Val Loss: 2.3052666212008304 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 470/1000 --- Train Loss: 2.3022117255561 --- Val Loss: 2.303494736474152 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 480/1000 --- Train Loss: 2.3022175217728607 --- Val Loss: 2.3046849470502186 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 490/1000 --- Train Loss: 2.302197473436249 --- Val Loss: 2.305519182757694 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 500/1000 --- Train Loss: 2.3022410918565583 --- Val Loss: 2.3057211049217443 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 510/1000 --- Train Loss: 2.302194297619502 --- Val Loss: 2.304638395591796 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 520/1000 --- Train Loss: 2.3022142009817985 --- Val Loss: 2.3056572577173227 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 530/1000 --- Train Loss: 2.302201770942243 --- Val Loss: 2.304671410553336 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 540/1000 --- Train Loss: 2.3022013854193513 --- Val Loss: 2.3042560112157164 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: 2.3022236439911037 --- Val Loss: 2.30302360588858 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: 2.3022182210376054 --- Val Loss: 2.305224783212112 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 570/1000 --- Train Loss: 2.302184494483668 --- Val Loss: 2.305157609736717 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 580/1000 --- Train Loss: 2.302213475134386 --- Val Loss: 2.304811293627598 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 590/1000 --- Train Loss: 2.3022094135534714 --- Val Loss: 2.3043353121469567 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 600/1000 --- Train Loss: 2.3021892653034364 --- Val Loss: 2.3044195033862476 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 610/1000 --- Train Loss: 2.3021644825635934 --- Val Loss: 2.3041839304561615 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 620/1000 --- Train Loss: 2.3022345981494623 --- Val Loss: 2.3052381643764606 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 630/1000 --- Train Loss: 2.3022631855122366 --- Val Loss: 2.303555966110827 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 640/1000 --- Train Loss: 2.3022370057073474 --- Val Loss: 2.3043957195860427 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 650/1000 --- Train Loss: 2.302222968721648 --- Val Loss: 2.30443744398313 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: 2.3023008696646876 --- Val Loss: 2.305463772203317 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 670/1000 --- Train Loss: 2.302231509700585 --- Val Loss: 2.3041819232457823 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 680/1000 --- Train Loss: 2.3021961638185022 --- Val Loss: 2.3048015528532506 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 690/1000 --- Train Loss: 2.3022058965790246 --- Val Loss: 2.3048128606537226 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 700/1000 --- Train Loss: 2.3021892694433963 --- Val Loss: 2.304002841728155 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 710/1000 --- Train Loss: 2.3022343810438897 --- Val Loss: 2.3044611042563536 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 720/1000 --- Train Loss: 2.30220437529255 --- Val Loss: 2.3044714401131268 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 730/1000 --- Train Loss: 2.3022648272143895 --- Val Loss: 2.304784956274244 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: 2.3022120751009654 --- Val Loss: 2.3042252502222516 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 750/1000 --- Train Loss: 2.302281626883998 --- Val Loss: 2.3038612705085417 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 760/1000 --- Train Loss: 2.3022263051218723 --- Val Loss: 2.3044400544224697 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 770/1000 --- Train Loss: 2.302234500138474 --- Val Loss: 2.3044721522002196 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 780/1000 --- Train Loss: 2.3022284220188496 --- Val Loss: 2.3062650476105477 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 790/1000 --- Train Loss: 2.3022239890584766 --- Val Loss: 2.305042212180638 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 800/1000 --- Train Loss: 2.302195091292237 --- Val Loss: 2.3046875140347463 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 810/1000 --- Train Loss: 2.30221209412155 --- Val Loss: 2.3058906521471068 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 820/1000 --- Train Loss: 2.302186943882441 --- Val Loss: 2.3056144443158786 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 830/1000 --- Train Loss: 2.3022323434411214 --- Val Loss: 2.3050675794077353 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 840/1000 --- Train Loss: 2.3021687299175313 --- Val Loss: 2.3044603705197115 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 850/1000 --- Train Loss: 2.30220150861764 --- Val Loss: 2.3040175305040087 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 860/1000 --- Train Loss: 2.3022434168785786 --- Val Loss: 2.304110567615089 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 870/1000 --- Train Loss: 2.3022489752691433 --- Val Loss: 2.3044460902507957 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 880/1000 --- Train Loss: 2.302196426091388 --- Val Loss: 2.30451318943302 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 890/1000 --- Train Loss: 2.302213330401586 --- Val Loss: 2.304039260520297 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 900/1000 --- Train Loss: 2.302237815137093 --- Val Loss: 2.3050007630439997 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 910/1000 --- Train Loss: 2.3021760190078884 --- Val Loss: 2.3040162242615896 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 920/1000 --- Train Loss: 2.302220325170588 --- Val Loss: 2.3041264604239604 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 930/1000 --- Train Loss: 2.3022622552604033 --- Val Loss: 2.3044446491156445 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 940/1000 --- Train Loss: 2.302220144942051 --- Val Loss: 2.3043800608246423 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 950/1000 --- Train Loss: 2.302257624762261 --- Val Loss: 2.303611966396565 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 960/1000 --- Train Loss: 2.302207971778225 --- Val Loss: 2.3046255772476507 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 970/1000 --- Train Loss: 2.3022164995474586 --- Val Loss: 2.3054827966921017 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 980/1000 --- Train Loss: 2.3022516921720646 --- Val Loss: 2.304716659834968 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 990/1000 --- Train Loss: 2.3022433711765027 --- Val Loss: 2.3034362620413567 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3019513334632484 --- Val Loss: 2.3032036122438515 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/500 --- Train Loss: 2.3013744786462866 --- Val Loss: 2.30581256818327 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/500 --- Train Loss: 2.3013713524395065 --- Val Loss: 2.305875995056095 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/500 --- Train Loss: 2.3013429070146687 --- Val Loss: 2.305669598442909 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.3010721943909775 --- Val Loss: 2.30547652846514 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/500 --- Train Loss: 1.844756151792891 --- Val Loss: 1.8387965723023374 --- Train Acc: 0.21 --- Val Acc: 0.19\n",
      "Epoch 60/500 --- Train Loss: 0.3070800268536707 --- Val Loss: 0.2654077965694362 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 70/500 --- Train Loss: 0.057831889922321654 --- Val Loss: 0.040279357371480466 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 80/500 --- Train Loss: 0.0342587318053114 --- Val Loss: 0.007189699170815924 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/500 --- Train Loss: 0.12386419972762634 --- Val Loss: 0.09439077625420732 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 100/500 --- Train Loss: 0.010634882684880853 --- Val Loss: 0.0004347622100600563 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/500 --- Train Loss: 0.01411755314510031 --- Val Loss: 0.0022653024006005495 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/500 --- Train Loss: 0.04404322103982694 --- Val Loss: 0.004968678753276888 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.2029455153034502 --- Val Loss: 0.078447808113546 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 140/500 --- Train Loss: 0.9895495727843807 --- Val Loss: 1.1769656468207088 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 150/500 --- Train Loss: 0.45501339039200855 --- Val Loss: 0.29996022805498773 --- Train Acc: 0.89 --- Val Acc: 0.92\n",
      "Epoch 160/500 --- Train Loss: 0.9138768026616423 --- Val Loss: 0.731751996695924 --- Train Acc: 0.74 --- Val Acc: 0.74\n",
      "Epoch 170/500 --- Train Loss: 1.205707981138063 --- Val Loss: 1.1464118935800964 --- Train Acc: 0.61 --- Val Acc: 0.58\n",
      "Epoch 180/500 --- Train Loss: 1.9105942474324302 --- Val Loss: 2.0883005390387863 --- Train Acc: 0.27 --- Val Acc: 0.23\n",
      "Epoch 190/500 --- Train Loss: 1.9311558223659402 --- Val Loss: 1.9667006626225247 --- Train Acc: 0.25 --- Val Acc: 0.20\n",
      "Epoch 200/500 --- Train Loss: 1.9654530273765078 --- Val Loss: 1.9397591316261151 --- Train Acc: 0.25 --- Val Acc: 0.24\n",
      "Epoch 210/500 --- Train Loss: 2.2967064105781376 --- Val Loss: 2.321863992473415 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/500 --- Train Loss: 2.2873271575826606 --- Val Loss: 2.3065944854045877 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/500 --- Train Loss: 2.293299856726798 --- Val Loss: 2.3068671910338883 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/500 --- Train Loss: 2.2830738375999426 --- Val Loss: 2.3067840213884314 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/500 --- Train Loss: 2.28314414672456 --- Val Loss: 2.3071479895528886 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/500 --- Train Loss: 2.289328177586934 --- Val Loss: 2.307262177903543 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/500 --- Train Loss: 2.2892166439635515 --- Val Loss: 2.307022541536595 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/500 --- Train Loss: 2.289195476257639 --- Val Loss: 2.3073777132491355 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/500 --- Train Loss: 2.285294119963975 --- Val Loss: 2.307511820937703 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/500 --- Train Loss: 2.299423935346296 --- Val Loss: 2.3075375332799895 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/500 --- Train Loss: 2.2892033753775904 --- Val Loss: 2.306516518615396 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: 2.2871774025025857 --- Val Loss: 2.3071061121906045 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/500 --- Train Loss: 2.2871515811736103 --- Val Loss: 2.307044248215434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/500 --- Train Loss: 2.2851022650220503 --- Val Loss: 2.307208756273079 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/500 --- Train Loss: 2.285105217026933 --- Val Loss: 2.3068652165885717 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/500 --- Train Loss: 2.285100632645336 --- Val Loss: 2.307122963062097 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/500 --- Train Loss: 2.2891925963500905 --- Val Loss: 2.3067368363968983 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/500 --- Train Loss: 2.2851013609416846 --- Val Loss: 2.3068782579538545 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/500 --- Train Loss: 2.297368552265832 --- Val Loss: 2.3070140090343676 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/500 --- Train Loss: 2.297310179069715 --- Val Loss: 2.305722605940301 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/500 --- Train Loss: 2.29323054972196 --- Val Loss: 2.305643589712892 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/500 --- Train Loss: 2.2932192596956993 --- Val Loss: 2.3056089006714604 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/500 --- Train Loss: 2.2953275333852994 --- Val Loss: 2.3059566002594263 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/500 --- Train Loss: 2.2952699449408502 --- Val Loss: 2.3066370456700143 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/500 --- Train Loss: 2.2993609814842086 --- Val Loss: 2.306244910604546 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/500 --- Train Loss: 2.2953039375710516 --- Val Loss: 2.3060040651262006 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/500 --- Train Loss: 2.293221997882142 --- Val Loss: 2.306283057553481 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/500 --- Train Loss: 2.2911699243545254 --- Val Loss: 2.3058943064332067 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/500 --- Train Loss: 2.3133962352086375 --- Val Loss: 2.3059516518241567 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302921842957794 --- Val Loss: 2.3042523650230056 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.302108697261613 --- Val Loss: 2.3035311511980967 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 1.7061538428482952 --- Val Loss: 1.6901764635339251 --- Train Acc: 0.31 --- Val Acc: 0.29\n",
      "Epoch 30/100 --- Train Loss: 0.17966609289869967 --- Val Loss: 0.03616245713052693 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 40/100 --- Train Loss: 0.14160188371772586 --- Val Loss: 0.00790729346509792 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 50/100 --- Train Loss: 0.18487914244637968 --- Val Loss: 0.07155041875024437 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 60/100 --- Train Loss: 0.07068428705880009 --- Val Loss: 0.004537893134299262 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/100 --- Train Loss: 0.045088756205443184 --- Val Loss: 0.009994170448711802 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.07388124128605793 --- Val Loss: 0.001883838715442928 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.06455909088536803 --- Val Loss: 0.0027482248679615243 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.9583333333333334\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3021540255981905 --- Val Loss: 2.3014618801000113 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 0.49217497800335325 --- Val Loss: 0.4107586863224265 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 20/1000 --- Train Loss: 0.2663312651772257 --- Val Loss: 0.09034236206809128 --- Train Acc: 0.96 --- Val Acc: 0.98\n",
      "Epoch 30/1000 --- Train Loss: 1.0743829751169145 --- Val Loss: 0.43961639991219803 --- Train Acc: 0.91 --- Val Acc: 0.92\n",
      "Epoch 40/1000 --- Train Loss: 0.7391120368730242 --- Val Loss: 0.10545473390448853 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 50/1000 --- Train Loss: 0.7820902363263579 --- Val Loss: 0.1427141373366931 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 60/1000 --- Train Loss: 0.7509628010223065 --- Val Loss: 0.15692516984909258 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 70/1000 --- Train Loss: 0.6744242838235897 --- Val Loss: 0.05638009283984507 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 80/1000 --- Train Loss: 0.43685539833317094 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/1000 --- Train Loss: 0.313129854516999 --- Val Loss: 0.16848193502744374 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 100/1000 --- Train Loss: 0.6672848672111815 --- Val Loss: 0.16848193502744385 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 110/1000 --- Train Loss: 1.0123625172962263 --- Val Loss: 0.5884933913672088 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 120/1000 --- Train Loss: 0.36113330995799287 --- Val Loss: 0.33978210413396864 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 130/1000 --- Train Loss: 0.6538579106886646 --- Val Loss: 0.38635873574907753 --- Train Acc: 0.86 --- Val Acc: 0.84\n",
      "Epoch 140/1000 --- Train Loss: 0.6781765200446458 --- Val Loss: 0.46479693906163483 --- Train Acc: 0.83 --- Val Acc: 0.81\n",
      "Epoch 150/1000 --- Train Loss: 0.6179296719460434 --- Val Loss: 0.5269559038817723 --- Train Acc: 0.80 --- Val Acc: 0.77\n",
      "Epoch 160/1000 --- Train Loss: 0.6633944164400268 --- Val Loss: 0.6537011009489591 --- Train Acc: 0.76 --- Val Acc: 0.73\n",
      "Epoch 170/1000 --- Train Loss: 0.7037842290498559 --- Val Loss: 0.7007628692386699 --- Train Acc: 0.76 --- Val Acc: 0.72\n",
      "Epoch 180/1000 --- Train Loss: 0.8336446216237849 --- Val Loss: 0.8234760327102372 --- Train Acc: 0.70 --- Val Acc: 0.67\n",
      "Epoch 190/1000 --- Train Loss: 0.9773766365935559 --- Val Loss: 0.8709667960628508 --- Train Acc: 0.66 --- Val Acc: 0.64\n",
      "Epoch 200/1000 --- Train Loss: 1.6486464911027972 --- Val Loss: 1.6116252604566932 --- Train Acc: 0.39 --- Val Acc: 0.39\n",
      "Epoch 210/1000 --- Train Loss: 1.4498929352876537 --- Val Loss: 1.430987065235879 --- Train Acc: 0.46 --- Val Acc: 0.45\n",
      "Epoch 220/1000 --- Train Loss: 1.5362502789768042 --- Val Loss: 1.5661891992075896 --- Train Acc: 0.41 --- Val Acc: 0.39\n",
      "Epoch 230/1000 --- Train Loss: 2.065346996246728 --- Val Loss: 2.0419180983207865 --- Train Acc: 0.21 --- Val Acc: 0.20\n",
      "Epoch 240/1000 --- Train Loss: 2.167971518241053 --- Val Loss: 2.12442894634131 --- Train Acc: 0.17 --- Val Acc: 0.17\n",
      "Epoch 250/1000 --- Train Loss: 2.2026979513743115 --- Val Loss: 2.2216719313578546 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 260/1000 --- Train Loss: 2.1871750271602193 --- Val Loss: 2.209265256587292 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 270/1000 --- Train Loss: 2.1840242362257642 --- Val Loss: 2.2071450315516032 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 280/1000 --- Train Loss: 2.2256269187605335 --- Val Loss: 2.2245813472997664 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: 2.223515761677721 --- Val Loss: 2.215895242398138 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 300/1000 --- Train Loss: 2.298420810938473 --- Val Loss: 2.2645659805472005 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.2841641821030483 --- Val Loss: 2.289153603640543 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.2869111909342483 --- Val Loss: 2.289019590391778 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.3230452418074106 --- Val Loss: 2.2972397575958317 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3009747259595645 --- Val Loss: 2.2968169157537055 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.300971116600706 --- Val Loss: 2.297068380003533 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.3009706262882523 --- Val Loss: 2.2970447968693497 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3009709636517632 --- Val Loss: 2.297159130350768 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.3009703893287345 --- Val Loss: 2.297015019419458 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.3009702872061415 --- Val Loss: 2.2968944816918895 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: 2.300971014359153 --- Val Loss: 2.2970096135819977 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.300970605796082 --- Val Loss: 2.2968117710709595 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.3009703959001033 --- Val Loss: 2.2970258570524504 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3009711169251483 --- Val Loss: 2.296804246913224 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.300971079595473 --- Val Loss: 2.2968304299800377 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.3009706469318774 --- Val Loss: 2.2968284121467977 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3009710866896613 --- Val Loss: 2.2967824407904147 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.300970707640266 --- Val Loss: 2.2969030243631208 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.3009703648356985 --- Val Loss: 2.2968501196965905 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.3009707691693784 --- Val Loss: 2.296852167835543 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3009702885839753 --- Val Loss: 2.2968227706709254 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.3009711335524012 --- Val Loss: 2.2970319786888473 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.300970940837985 --- Val Loss: 2.2970340363485793 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3009708669611952 --- Val Loss: 2.2970701752528857 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.3009702087375974 --- Val Loss: 2.2970044344637786 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.3009707360339218 --- Val Loss: 2.296852700029292 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3009703311381218 --- Val Loss: 2.2969428701837518 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.3009701760685464 --- Val Loss: 2.29699104386042 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.3009711804815187 --- Val Loss: 2.2968634147660842 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.300971621422132 --- Val Loss: 2.296816985183068 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.300971200885154 --- Val Loss: 2.2967891308324058 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.3009712173221915 --- Val Loss: 2.296739109603313 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.300970838140516 --- Val Loss: 2.296937385078806 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.3009704492682763 --- Val Loss: 2.296920213769064 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.300971792770764 --- Val Loss: 2.2969236290069075 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.3009703919483693 --- Val Loss: 2.29688135795246 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.300970820468538 --- Val Loss: 2.2969707854483388 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3009710432505055 --- Val Loss: 2.2969247744816426 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.3009713302521244 --- Val Loss: 2.2971214786265715 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.300971080842654 --- Val Loss: 2.297139563279906 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3009702207630505 --- Val Loss: 2.2968359084114205 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.300970288952451 --- Val Loss: 2.296857653657375 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.3009704858915163 --- Val Loss: 2.2968528574626177 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.300970362741148 --- Val Loss: 2.296878317959065 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.300970512452221 --- Val Loss: 2.296900385937249 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.300972399154033 --- Val Loss: 2.296860136358053 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.3009707034139373 --- Val Loss: 2.2969288042605127 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.3009712504687307 --- Val Loss: 2.2968142903494555 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.3009702879846965 --- Val Loss: 2.2969111911866 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.300970623870344 --- Val Loss: 2.2968911133977383 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: 2.300970754083877 --- Val Loss: 2.2968585604360214 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3009705158164957 --- Val Loss: 2.296969152016266 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.3009701793346236 --- Val Loss: 2.29687239344598 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.3009709741138265 --- Val Loss: 2.2970800570140426 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.300970800623994 --- Val Loss: 2.2967271926043344 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.300970437569352 --- Val Loss: 2.296796001146175 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3009713326753904 --- Val Loss: 2.296927199993847 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.300970149756343 --- Val Loss: 2.296876762662922 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3009710202553846 --- Val Loss: 2.2966728186181635 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.3009703272042836 --- Val Loss: 2.296947216068305 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.3009701159550597 --- Val Loss: 2.2968454380737513 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.300970621932978 --- Val Loss: 2.2968631665730603 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: 2.300970245736443 --- Val Loss: 2.296873958580808 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.300970366470661 --- Val Loss: 2.2969776596836757 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.3009707998580184 --- Val Loss: 2.2967897644260336 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.300970723193454 --- Val Loss: 2.296723988479801 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.3009705197523895 --- Val Loss: 2.296899528013069 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.3009708538007922 --- Val Loss: 2.2970293923839713 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.30097067041937 --- Val Loss: 2.2968656170542947 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: 2.3009704314553674 --- Val Loss: 2.2969267742445183 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.302209709585886 --- Val Loss: 2.302219974182938 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/1000 --- Train Loss: 1.465830459567533 --- Val Loss: 1.4033657651954805 --- Train Acc: 0.50 --- Val Acc: 0.49\n",
      "Epoch 20/1000 --- Train Loss: 0.09357905916222199 --- Val Loss: 0.05348465674801445 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 30/1000 --- Train Loss: 0.03470347939035578 --- Val Loss: 0.0033165251718098973 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 40/1000 --- Train Loss: 0.02427188791496324 --- Val Loss: 0.0016388073261021426 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/1000 --- Train Loss: 0.02214672057232061 --- Val Loss: 0.00024622411892184694 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 0.0553000651429807 --- Val Loss: 0.00017778929727215028 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/1000 --- Train Loss: 1.6492144363876389 --- Val Loss: 1.0427933806196386 --- Train Acc: 0.89 --- Val Acc: 0.90\n",
      "Epoch 80/1000 --- Train Loss: 0.5655866411017172 --- Val Loss: 0.03476310149935854 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 90/1000 --- Train Loss: 1.8688108358789648 --- Val Loss: 0.7262616915562111 --- Train Acc: 0.83 --- Val Acc: 0.86\n",
      "Epoch 100/1000 --- Train Loss: 1.3961370408353224 --- Val Loss: 0.7484851203983526 --- Train Acc: 0.72 --- Val Acc: 0.76\n",
      "Epoch 110/1000 --- Train Loss: 1.2651325788944465 --- Val Loss: 0.8269724893047239 --- Train Acc: 0.69 --- Val Acc: 0.69\n",
      "Epoch 120/1000 --- Train Loss: 1.169302204345568 --- Val Loss: 0.9594907179191051 --- Train Acc: 0.63 --- Val Acc: 0.62\n",
      "Epoch 130/1000 --- Train Loss: 1.3693936734517418 --- Val Loss: 0.9745720911001033 --- Train Acc: 0.60 --- Val Acc: 0.61\n",
      "Epoch 140/1000 --- Train Loss: 1.640461037407618 --- Val Loss: 1.0899248699038764 --- Train Acc: 0.54 --- Val Acc: 0.55\n",
      "Epoch 150/1000 --- Train Loss: 1.5034293688064482 --- Val Loss: 1.1917017579209739 --- Train Acc: 0.52 --- Val Acc: 0.52\n",
      "Epoch 160/1000 --- Train Loss: 1.6121387146544175 --- Val Loss: 1.354050645119802 --- Train Acc: 0.46 --- Val Acc: 0.45\n",
      "Epoch 170/1000 --- Train Loss: 2.3867749268029446 --- Val Loss: 2.388188620320733 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: 2.301666505664164 --- Val Loss: 2.306812316737375 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.2975407720109624 --- Val Loss: 2.301837692247172 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.313284575231862 --- Val Loss: 2.301600356340452 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: 2.3133919534753757 --- Val Loss: 2.301646659489133 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: 2.30132649515917 --- Val Loss: 2.3014737048360407 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/1000 --- Train Loss: 2.3013274476417984 --- Val Loss: 2.3014710939986514 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.3013274664243863 --- Val Loss: 2.3014198819570635 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: 2.301326784426163 --- Val Loss: 2.301526779966951 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.3013266554415295 --- Val Loss: 2.3014731647106674 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.3013279783663854 --- Val Loss: 2.301663800268257 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.31870708271434 --- Val Loss: 2.3270883360773933 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.3168781976473625 --- Val Loss: 2.3116577538403424 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 20/1000 --- Train Loss: 2.340095335805188 --- Val Loss: 2.3599721472017463 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.328743898939698 --- Val Loss: 2.331109276111929 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.3415610132293163 --- Val Loss: 2.3314515891349115 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: 2.328167025508528 --- Val Loss: 2.335663498617128 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.3371811750612705 --- Val Loss: 2.3535310025727276 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.3244631274409358 --- Val Loss: 2.315555226284691 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.3284721855703143 --- Val Loss: 2.3332505289116194 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: 2.3088084565048446 --- Val Loss: 2.3186645853566796 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.3340043366837984 --- Val Loss: 2.329491740546002 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.3320748368974504 --- Val Loss: 2.3401883565643997 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.337740753589398 --- Val Loss: 2.348038248634785 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.3357387535856065 --- Val Loss: 2.3311167327130273 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.3351337545075155 --- Val Loss: 2.328557755068419 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3194100462286857 --- Val Loss: 2.3172824028039205 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3477686075422675 --- Val Loss: 2.353438359269054 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: 2.326674181481468 --- Val Loss: 2.3387480509004095 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: 2.355013240959192 --- Val Loss: 2.3451010205712706 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.320637364466144 --- Val Loss: 2.3218042873936833 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: 2.326460204123323 --- Val Loss: 2.3129805740175735 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: 2.3711649866306774 --- Val Loss: 2.3474632547184213 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.328643564174921 --- Val Loss: 2.332237812523723 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.339403031200619 --- Val Loss: 2.328122611765255 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.33489041908049 --- Val Loss: 2.320772058071477 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3760752592155723 --- Val Loss: 2.3589125506853805 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.3189106226219267 --- Val Loss: 2.3190771473534664 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.3453846775146223 --- Val Loss: 2.341207472088867 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: 2.322017657501538 --- Val Loss: 2.328368903879966 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: 2.3361569598489953 --- Val Loss: 2.3285704409659065 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.3182095143219557 --- Val Loss: 2.3192892525619713 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.3110653408208126 --- Val Loss: 2.3097556601737943 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.3164479505055064 --- Val Loss: 2.3188025375780774 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.3464116746737336 --- Val Loss: 2.351283243354807 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: 2.338140740569061 --- Val Loss: 2.320643811774713 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 350/1000 --- Train Loss: 2.350331960020329 --- Val Loss: 2.358511099990499 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 360/1000 --- Train Loss: 2.331159593589657 --- Val Loss: 2.316741994238194 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 370/1000 --- Train Loss: 2.331383875631621 --- Val Loss: 2.337121471058759 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3232512852896345 --- Val Loss: 2.3233147009159913 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.335011427583255 --- Val Loss: 2.340488112782302 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.3542496235222745 --- Val Loss: 2.386892844419723 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3349169718619565 --- Val Loss: 2.3418486793437476 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: 2.3551230036452897 --- Val Loss: 2.351047057777235 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: 2.3415992451572105 --- Val Loss: 2.350845603381207 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 440/1000 --- Train Loss: 2.3584018324422527 --- Val Loss: 2.329129519583113 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.3509008281693453 --- Val Loss: 2.352272068039307 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 460/1000 --- Train Loss: 2.323354095150888 --- Val Loss: 2.316333991232699 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 470/1000 --- Train Loss: 2.3035963811690183 --- Val Loss: 2.305482999447359 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.3474612500994283 --- Val Loss: 2.361355665516889 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 490/1000 --- Train Loss: 2.3253944714775088 --- Val Loss: 2.33299052070186 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 500/1000 --- Train Loss: 2.3275070799750623 --- Val Loss: 2.340170563464809 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.346656765977536 --- Val Loss: 2.361868657629567 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 520/1000 --- Train Loss: 2.389767573853793 --- Val Loss: 2.3933836749147384 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: 2.3255258060880744 --- Val Loss: 2.320790995454882 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.3622165603650633 --- Val Loss: 2.367349391899466 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: 2.337615700837963 --- Val Loss: 2.318816706530901 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3430939799650363 --- Val Loss: 2.3504533538940846 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3086179755724117 --- Val Loss: 2.3124179716916697 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.332744910352483 --- Val Loss: 2.3407690148635294 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3614239862786914 --- Val Loss: 2.3734136968842092 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.3509002403526265 --- Val Loss: 2.3561944438228624 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: 2.3279375260450292 --- Val Loss: 2.318029525692 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.372478413028102 --- Val Loss: 2.3910664374894726 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 630/1000 --- Train Loss: 2.376966007846022 --- Val Loss: 2.3763065980292173 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3462961123064066 --- Val Loss: 2.324070774622774 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: 2.3434246636471734 --- Val Loss: 2.3634032307035713 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: 2.3312727392462462 --- Val Loss: 2.3411621700396608 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 670/1000 --- Train Loss: 2.3321551348165794 --- Val Loss: 2.324127130377994 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.370319180140247 --- Val Loss: 2.3871459582398096 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.321839445025383 --- Val Loss: 2.318519757326658 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.349874741825974 --- Val Loss: 2.3268479035187326 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.3525755786386457 --- Val Loss: 2.356721431881643 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.348992868578893 --- Val Loss: 2.360353618729743 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 730/1000 --- Train Loss: 2.3344921496873052 --- Val Loss: 2.360848687741494 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: 2.3591675731008834 --- Val Loss: 2.377736978179639 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 750/1000 --- Train Loss: 2.337430334915698 --- Val Loss: 2.3260459581517203 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.3348988902678984 --- Val Loss: 2.3259765965340202 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.3436760975032978 --- Val Loss: 2.3381911710445165 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.3284304523436417 --- Val Loss: 2.340820174229637 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.3200047782543494 --- Val Loss: 2.307037291976359 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 800/1000 --- Train Loss: 2.3423359219797297 --- Val Loss: 2.3573843794463465 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: 2.323811548415951 --- Val Loss: 2.3215896929972413 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.3338216098770252 --- Val Loss: 2.3436182991520806 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.331821730003396 --- Val Loss: 2.3178954157147236 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3519154354297545 --- Val Loss: 2.36164266521074 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: 2.3202608914503355 --- Val Loss: 2.3048971876119597 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 860/1000 --- Train Loss: 2.3324968745655537 --- Val Loss: 2.3302488780316546 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: 2.322714720680809 --- Val Loss: 2.322554485713799 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3663523031720257 --- Val Loss: 2.3785814070692335 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: 2.3269903938469296 --- Val Loss: 2.3156096025454644 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.332470839172619 --- Val Loss: 2.3414492098541455 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 910/1000 --- Train Loss: 2.3245429863491753 --- Val Loss: 2.313180944828109 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.3357160483149797 --- Val Loss: 2.329100910028868 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 930/1000 --- Train Loss: 2.3377319538773462 --- Val Loss: 2.352343991422974 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: 2.3209669910764776 --- Val Loss: 2.315812340862876 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3417744864509173 --- Val Loss: 2.344295064488391 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.3327480503491413 --- Val Loss: 2.3202815250180526 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.3364546011075205 --- Val Loss: 2.35009893546714 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.340210767567764 --- Val Loss: 2.3578059969634735 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 990/1000 --- Train Loss: 2.3446332409534687 --- Val Loss: 2.3580625030142297 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09444444444444444\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3018120718507813 --- Val Loss: 2.301050998809334 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/100 --- Train Loss: 2.3019549541184783 --- Val Loss: 2.3003780561960796 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 0.7317164984523877 --- Val Loss: 0.6107664999064795 --- Train Acc: 0.74 --- Val Acc: 0.75\n",
      "Epoch 30/100 --- Train Loss: 0.1429626805912457 --- Val Loss: 0.03502479344048091 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/100 --- Train Loss: 0.08965586833026833 --- Val Loss: 0.0049802448542301 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/100 --- Train Loss: 0.06449260508946911 --- Val Loss: 0.0035783795702772894 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/100 --- Train Loss: 0.07352592055214419 --- Val Loss: 0.004685146702492657 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/100 --- Train Loss: 0.057442489858066284 --- Val Loss: 0.0016391034886293662 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.08951011567529943 --- Val Loss: 0.005935024284316065 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.0776543632355493 --- Val Loss: 0.0017002255458726913 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.9583333333333334\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3018833978067583 --- Val Loss: 2.3029561458398335 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 0.30436776322046954 --- Val Loss: 0.19172645314347306 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 20/1000 --- Train Loss: 0.33101695349376475 --- Val Loss: 0.15011366485159772 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 30/1000 --- Train Loss: 0.7430273490702042 --- Val Loss: 0.20955039729203426 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 40/1000 --- Train Loss: 0.6767508957526989 --- Val Loss: 0.040516291789213475 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 50/1000 --- Train Loss: 0.495825349915891 --- Val Loss: 0.056337503463369895 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 0.5603626834150801 --- Val Loss: 0.05616071167581789 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 70/1000 --- Train Loss: 0.848985428396551 --- Val Loss: 0.13259123764548578 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 80/1000 --- Train Loss: 0.4074961165040257 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/1000 --- Train Loss: 0.309657111583275 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/1000 --- Train Loss: 0.37904509327308056 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/1000 --- Train Loss: 0.31466863514516885 --- Val Loss: 0.11043039226557996 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 120/1000 --- Train Loss: 0.562592499693612 --- Val Loss: 0.23432431363811645 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 130/1000 --- Train Loss: 0.2225130762501344 --- Val Loss: 0.025859579057018835 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 140/1000 --- Train Loss: 0.3601274030964389 --- Val Loss: 0.07338242366958851 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 150/1000 --- Train Loss: 0.287913722318711 --- Val Loss: 0.04168753482396726 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 160/1000 --- Train Loss: 0.3026924875633065 --- Val Loss: 0.10546687872928642 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 170/1000 --- Train Loss: 0.13588145394750487 --- Val Loss: 0.06700040961612504 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 180/1000 --- Train Loss: 0.3392351098010594 --- Val Loss: 0.08894327933894022 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 190/1000 --- Train Loss: 0.20837572054572315 --- Val Loss: 0.10365131648508794 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 200/1000 --- Train Loss: 0.33948635274786776 --- Val Loss: 0.20691561500603295 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 210/1000 --- Train Loss: 0.26756085300102006 --- Val Loss: 0.143289400908066 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 220/1000 --- Train Loss: 0.38770230820786095 --- Val Loss: 0.27644123292536166 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 230/1000 --- Train Loss: 0.36451201502290775 --- Val Loss: 0.24743641757461451 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 240/1000 --- Train Loss: 0.3089599923298987 --- Val Loss: 0.3557913635833306 --- Train Acc: 0.91 --- Val Acc: 0.88\n",
      "Epoch 250/1000 --- Train Loss: 0.4353168108563262 --- Val Loss: 0.4182392023864011 --- Train Acc: 0.89 --- Val Acc: 0.85\n",
      "Epoch 260/1000 --- Train Loss: 0.5907660327154877 --- Val Loss: 0.45834642321520846 --- Train Acc: 0.82 --- Val Acc: 0.81\n",
      "Epoch 270/1000 --- Train Loss: 0.5794206339394216 --- Val Loss: 0.5650288472378671 --- Train Acc: 0.79 --- Val Acc: 0.79\n",
      "Epoch 280/1000 --- Train Loss: 0.5938379638775079 --- Val Loss: 0.5595478696266948 --- Train Acc: 0.78 --- Val Acc: 0.79\n",
      "Epoch 290/1000 --- Train Loss: 0.5582361431014715 --- Val Loss: 0.5387017383081311 --- Train Acc: 0.79 --- Val Acc: 0.79\n",
      "Epoch 300/1000 --- Train Loss: 0.5992538607161364 --- Val Loss: 0.5691424181128278 --- Train Acc: 0.78 --- Val Acc: 0.78\n",
      "Epoch 310/1000 --- Train Loss: 0.6259567182898012 --- Val Loss: 0.5760456696320606 --- Train Acc: 0.79 --- Val Acc: 0.78\n",
      "Epoch 320/1000 --- Train Loss: 0.742836144974359 --- Val Loss: 0.6693547714508943 --- Train Acc: 0.74 --- Val Acc: 0.72\n",
      "Epoch 330/1000 --- Train Loss: 0.8119501824789254 --- Val Loss: 0.7288605255839183 --- Train Acc: 0.73 --- Val Acc: 0.73\n",
      "Epoch 340/1000 --- Train Loss: 0.912589112414557 --- Val Loss: 0.8972509520526423 --- Train Acc: 0.70 --- Val Acc: 0.67\n",
      "Epoch 350/1000 --- Train Loss: 0.9353322518378607 --- Val Loss: 0.9055071348120749 --- Train Acc: 0.67 --- Val Acc: 0.67\n",
      "Epoch 360/1000 --- Train Loss: 0.8554273288543937 --- Val Loss: 0.8618539850573822 --- Train Acc: 0.66 --- Val Acc: 0.67\n",
      "Epoch 370/1000 --- Train Loss: 0.8572620960439388 --- Val Loss: 0.79357485950581 --- Train Acc: 0.68 --- Val Acc: 0.71\n",
      "Epoch 380/1000 --- Train Loss: 0.9237557049511531 --- Val Loss: 0.9551055232963552 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 390/1000 --- Train Loss: 0.955834163886957 --- Val Loss: 0.9774617143676045 --- Train Acc: 0.63 --- Val Acc: 0.62\n",
      "Epoch 400/1000 --- Train Loss: 0.9006638409397708 --- Val Loss: 0.9310295629874259 --- Train Acc: 0.65 --- Val Acc: 0.64\n",
      "Epoch 410/1000 --- Train Loss: 0.928229801572548 --- Val Loss: 0.9007218038912043 --- Train Acc: 0.65 --- Val Acc: 0.65\n",
      "Epoch 420/1000 --- Train Loss: 1.0377239757092989 --- Val Loss: 0.9636223491199298 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 430/1000 --- Train Loss: 1.043271155789786 --- Val Loss: 0.9689171866415768 --- Train Acc: 0.60 --- Val Acc: 0.63\n",
      "Epoch 440/1000 --- Train Loss: 1.0658827222623666 --- Val Loss: 1.0677743331579719 --- Train Acc: 0.56 --- Val Acc: 0.58\n",
      "Epoch 450/1000 --- Train Loss: 1.1704065141979692 --- Val Loss: 1.1172947657775318 --- Train Acc: 0.54 --- Val Acc: 0.57\n",
      "Epoch 460/1000 --- Train Loss: 1.3160722306136785 --- Val Loss: 1.3161373519281536 --- Train Acc: 0.52 --- Val Acc: 0.52\n",
      "Epoch 470/1000 --- Train Loss: 1.5017964680781037 --- Val Loss: 1.51423097729703 --- Train Acc: 0.46 --- Val Acc: 0.46\n",
      "Epoch 480/1000 --- Train Loss: 1.3886154873685286 --- Val Loss: 1.3830353416332046 --- Train Acc: 0.47 --- Val Acc: 0.46\n",
      "Epoch 490/1000 --- Train Loss: 1.4455740721365817 --- Val Loss: 1.4147631120415474 --- Train Acc: 0.45 --- Val Acc: 0.45\n",
      "Epoch 500/1000 --- Train Loss: 1.5768163014658942 --- Val Loss: 1.5192238148330675 --- Train Acc: 0.40 --- Val Acc: 0.42\n",
      "Epoch 510/1000 --- Train Loss: 1.751370981798445 --- Val Loss: 1.7185911849396716 --- Train Acc: 0.33 --- Val Acc: 0.36\n",
      "Epoch 520/1000 --- Train Loss: 1.8091304346696926 --- Val Loss: 1.779912922626712 --- Train Acc: 0.31 --- Val Acc: 0.33\n",
      "Epoch 530/1000 --- Train Loss: 1.8262225848477496 --- Val Loss: 1.8121018173129546 --- Train Acc: 0.29 --- Val Acc: 0.31\n",
      "Epoch 540/1000 --- Train Loss: 1.8353031020359738 --- Val Loss: 1.8001836334697923 --- Train Acc: 0.29 --- Val Acc: 0.32\n",
      "Epoch 550/1000 --- Train Loss: 1.7944192964529493 --- Val Loss: 1.8339149279212061 --- Train Acc: 0.29 --- Val Acc: 0.31\n",
      "Epoch 560/1000 --- Train Loss: 1.8418310143860115 --- Val Loss: 1.8346692879387632 --- Train Acc: 0.28 --- Val Acc: 0.31\n",
      "Epoch 570/1000 --- Train Loss: 1.9082999185441918 --- Val Loss: 1.8866130163546357 --- Train Acc: 0.26 --- Val Acc: 0.29\n",
      "Epoch 580/1000 --- Train Loss: 1.85429047504585 --- Val Loss: 1.8768334274136336 --- Train Acc: 0.25 --- Val Acc: 0.29\n",
      "Epoch 590/1000 --- Train Loss: 1.7890961503008092 --- Val Loss: 1.735885344316149 --- Train Acc: 0.30 --- Val Acc: 0.34\n",
      "Epoch 600/1000 --- Train Loss: 1.761251199297289 --- Val Loss: 1.755338571761708 --- Train Acc: 0.32 --- Val Acc: 0.33\n",
      "Epoch 610/1000 --- Train Loss: 1.7438542103470192 --- Val Loss: 1.7170845444406824 --- Train Acc: 0.31 --- Val Acc: 0.34\n",
      "Epoch 620/1000 --- Train Loss: 1.9185971502113408 --- Val Loss: 1.923750826093255 --- Train Acc: 0.24 --- Val Acc: 0.26\n",
      "Epoch 630/1000 --- Train Loss: 1.9116433656828407 --- Val Loss: 1.924150390123892 --- Train Acc: 0.24 --- Val Acc: 0.26\n",
      "Epoch 640/1000 --- Train Loss: 2.0469715587583646 --- Val Loss: 2.0994995636867793 --- Train Acc: 0.18 --- Val Acc: 0.20\n",
      "Epoch 650/1000 --- Train Loss: 2.0465544942389733 --- Val Loss: 2.090733331174455 --- Train Acc: 0.18 --- Val Acc: 0.20\n",
      "Epoch 660/1000 --- Train Loss: 2.0946021353969373 --- Val Loss: 2.0988393918250248 --- Train Acc: 0.18 --- Val Acc: 0.20\n",
      "Epoch 670/1000 --- Train Loss: 2.020968304736838 --- Val Loss: 2.0987583893337662 --- Train Acc: 0.18 --- Val Acc: 0.20\n",
      "Epoch 680/1000 --- Train Loss: 2.0563593428001976 --- Val Loss: 2.073616167811421 --- Train Acc: 0.19 --- Val Acc: 0.21\n",
      "Epoch 690/1000 --- Train Loss: 2.1390305803545835 --- Val Loss: 2.2003039040295276 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 700/1000 --- Train Loss: 2.15223910245436 --- Val Loss: 2.203605232309324 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 710/1000 --- Train Loss: 2.1346114155576736 --- Val Loss: 2.1838938633057072 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 720/1000 --- Train Loss: 2.122640150940587 --- Val Loss: 2.1915519146324836 --- Train Acc: 0.14 --- Val Acc: 0.16\n",
      "Epoch 730/1000 --- Train Loss: 2.140566107913951 --- Val Loss: 2.199903560574723 --- Train Acc: 0.14 --- Val Acc: 0.16\n",
      "Epoch 740/1000 --- Train Loss: 2.1208608909364517 --- Val Loss: 2.1825900185899667 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 750/1000 --- Train Loss: 2.157390606641399 --- Val Loss: 2.1749696910498395 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 760/1000 --- Train Loss: 2.1170370407483547 --- Val Loss: 2.17583285046768 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 770/1000 --- Train Loss: 2.1569422227969755 --- Val Loss: 2.1839095407059532 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 780/1000 --- Train Loss: 2.1483727505024413 --- Val Loss: 2.1756875620959106 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 790/1000 --- Train Loss: 2.1743492689542565 --- Val Loss: 2.175798857025579 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 800/1000 --- Train Loss: 2.176483968081283 --- Val Loss: 2.176280491703881 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 810/1000 --- Train Loss: 2.1493075256129464 --- Val Loss: 2.1667439242056226 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 820/1000 --- Train Loss: 2.1228764334704797 --- Val Loss: 2.1588264603561793 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 830/1000 --- Train Loss: 2.170557585400212 --- Val Loss: 2.1596976197745517 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 840/1000 --- Train Loss: 2.1251843176746323 --- Val Loss: 2.159758170317009 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 850/1000 --- Train Loss: 2.131794501402835 --- Val Loss: 2.16736823849989 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 860/1000 --- Train Loss: 2.1521966256064182 --- Val Loss: 2.1676868679969763 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 870/1000 --- Train Loss: 2.1489161541244215 --- Val Loss: 2.159270352903287 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 880/1000 --- Train Loss: 2.142734495847117 --- Val Loss: 2.2095854112557287 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 890/1000 --- Train Loss: 2.2452804579155075 --- Val Loss: 2.201938856305106 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 900/1000 --- Train Loss: 2.1618367461289782 --- Val Loss: 2.202243160215913 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 910/1000 --- Train Loss: 2.2474800207499217 --- Val Loss: 2.2727604354668274 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.216474151845145 --- Val Loss: 2.269664100606292 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.209597962152578 --- Val Loss: 2.2697300958607003 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: 2.2447509478506276 --- Val Loss: 2.259206681138776 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: 2.229169345181274 --- Val Loss: 2.2597569751289432 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.2308972847877238 --- Val Loss: 2.251120930153167 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 970/1000 --- Train Loss: 2.2430786299627723 --- Val Loss: 2.2514501108555924 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 980/1000 --- Train Loss: 2.210164096093106 --- Val Loss: 2.2516619385791 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 990/1000 --- Train Loss: 2.2161360614089 --- Val Loss: 2.242816060710623 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.1990737929691697 --- Val Loss: 2.1974597449740605 --- Train Acc: 0.47 --- Val Acc: 0.47\n",
      "Epoch 10/500 --- Train Loss: 1.6787276037278334 --- Val Loss: 1.5126059126602953 --- Train Acc: 0.40 --- Val Acc: 0.44\n",
      "Epoch 20/500 --- Train Loss: 2.211250949829452 --- Val Loss: 2.14099199615819 --- Train Acc: 0.17 --- Val Acc: 0.20\n",
      "Epoch 30/500 --- Train Loss: 2.1387410184757267 --- Val Loss: 2.114827462402829 --- Train Acc: 0.18 --- Val Acc: 0.20\n",
      "Epoch 40/500 --- Train Loss: 2.304455232448013 --- Val Loss: 2.309668287202024 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 50/500 --- Train Loss: 2.3015700330202224 --- Val Loss: 2.304441645896997 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/500 --- Train Loss: 2.301494326122921 --- Val Loss: 2.302913848212344 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 70/500 --- Train Loss: 2.3015244052787716 --- Val Loss: 2.3050336000665483 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/500 --- Train Loss: 2.3015048468607477 --- Val Loss: 2.302929094340982 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/500 --- Train Loss: 2.301542125656307 --- Val Loss: 2.3050450029644383 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 100/500 --- Train Loss: 2.3015708416972354 --- Val Loss: 2.305030593928917 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 110/500 --- Train Loss: 2.3015453739168628 --- Val Loss: 2.3039916887093863 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/500 --- Train Loss: 2.3014614320621316 --- Val Loss: 2.303422090108526 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 130/500 --- Train Loss: 2.301738945521716 --- Val Loss: 2.3042268067846585 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/500 --- Train Loss: 2.301617269649336 --- Val Loss: 2.3021179509883067 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/500 --- Train Loss: 2.3014775664459313 --- Val Loss: 2.3033896704526704 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 160/500 --- Train Loss: 2.3015098380627297 --- Val Loss: 2.3036713608662933 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 170/500 --- Train Loss: 2.3014925157742243 --- Val Loss: 2.3043445757810987 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/500 --- Train Loss: 2.3015322717376407 --- Val Loss: 2.304736144795686 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 190/500 --- Train Loss: 2.301509912231308 --- Val Loss: 2.3032519998984915 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 200/500 --- Train Loss: 2.3015581858146876 --- Val Loss: 2.3040076202132833 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 210/500 --- Train Loss: 2.3015506236891343 --- Val Loss: 2.3040919529990975 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/500 --- Train Loss: 2.3016088809395945 --- Val Loss: 2.30398488976672 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 230/500 --- Train Loss: 2.3015873335882233 --- Val Loss: 2.3051716077161593 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/500 --- Train Loss: 2.3015491654298312 --- Val Loss: 2.3071426669098436 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 250/500 --- Train Loss: 2.3015481528001795 --- Val Loss: 2.304546665834977 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 260/500 --- Train Loss: 2.301552936448181 --- Val Loss: 2.302921046534861 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/500 --- Train Loss: 2.301569255547258 --- Val Loss: 2.3033701664505957 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 280/500 --- Train Loss: 2.3015810833134496 --- Val Loss: 2.3037854818217025 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/500 --- Train Loss: 2.301520664450178 --- Val Loss: 2.303552542352753 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/500 --- Train Loss: 2.301596799059539 --- Val Loss: 2.3023840836905514 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/500 --- Train Loss: 2.3015206198668015 --- Val Loss: 2.304765301572712 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 320/500 --- Train Loss: 2.3015277475654803 --- Val Loss: 2.303276770189132 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 330/500 --- Train Loss: 2.301609305539991 --- Val Loss: 2.3039200347815703 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 340/500 --- Train Loss: 2.3015378412677414 --- Val Loss: 2.3055584199774626 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 350/500 --- Train Loss: 2.301494916542649 --- Val Loss: 2.3050669537056057 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 360/500 --- Train Loss: 2.3015045525151585 --- Val Loss: 2.304475163164772 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 370/500 --- Train Loss: 2.3014838413387984 --- Val Loss: 2.3042833412638513 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/500 --- Train Loss: 2.3015571027307837 --- Val Loss: 2.3046184505747775 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 390/500 --- Train Loss: 2.3015641282581303 --- Val Loss: 2.302695053892778 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/500 --- Train Loss: 2.3014506267605896 --- Val Loss: 2.3038479711934836 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/500 --- Train Loss: 2.3016285233047538 --- Val Loss: 2.3036471161200205 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/500 --- Train Loss: 2.3015769539396285 --- Val Loss: 2.304773873649135 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 430/500 --- Train Loss: 2.3014527746071316 --- Val Loss: 2.305260236181373 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/500 --- Train Loss: 2.3015869886379305 --- Val Loss: 2.3059094309078603 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 450/500 --- Train Loss: 2.3014776517947397 --- Val Loss: 2.304421867377126 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/500 --- Train Loss: 2.3014826082887443 --- Val Loss: 2.3048032579895303 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 470/500 --- Train Loss: 2.3015821424009686 --- Val Loss: 2.304692644757706 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 480/500 --- Train Loss: 2.3015031130756176 --- Val Loss: 2.305405412329695 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 490/500 --- Train Loss: 2.3015961851634867 --- Val Loss: 2.3031074465751336 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.301367585391657 --- Val Loss: 2.3033219635833575 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/1000 --- Train Loss: 0.9373321194729932 --- Val Loss: 0.8229533993474836 --- Train Acc: 0.68 --- Val Acc: 0.68\n",
      "Epoch 20/1000 --- Train Loss: 1.9871594548000027 --- Val Loss: 2.0088452323142802 --- Train Acc: 0.22 --- Val Acc: 0.20\n",
      "Epoch 30/1000 --- Train Loss: 2.285069672436959 --- Val Loss: 2.291356893210258 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.168118994452227 --- Val Loss: 2.19609590352616 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 50/1000 --- Train Loss: 2.2994074586787017 --- Val Loss: 2.30423720509465 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.3014251200838243 --- Val Loss: 2.3044127944883788 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: 2.3015934280225334 --- Val Loss: 2.306578946996245 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: 2.3015055308343544 --- Val Loss: 2.3045702409253224 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: 2.3014727300004774 --- Val Loss: 2.3054863768842657 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.3014590241366784 --- Val Loss: 2.3044720078436267 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 110/1000 --- Train Loss: 2.3015107947311053 --- Val Loss: 2.303226217096465 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 120/1000 --- Train Loss: 2.301463203762501 --- Val Loss: 2.3046745408809914 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 130/1000 --- Train Loss: 2.301446570576134 --- Val Loss: 2.3049147275964152 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: 2.3015025892311893 --- Val Loss: 2.3048527539205983 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: 2.262107668546533 --- Val Loss: 2.2671265717887312 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3014749329875914 --- Val Loss: 2.305308844027556 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: 2.3014469894584435 --- Val Loss: 2.304386583279398 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: 2.30147836157024 --- Val Loss: 2.3043762088299324 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: 2.301541358903489 --- Val Loss: 2.3056981621739943 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: 2.3014193012006907 --- Val Loss: 2.3045955806824643 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 210/1000 --- Train Loss: 2.3015016495412968 --- Val Loss: 2.3031390093673445 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: 2.3015641433278544 --- Val Loss: 2.304378680890201 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: 2.3014264276632135 --- Val Loss: 2.3043811952570246 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: 2.301504168367117 --- Val Loss: 2.3059046007371875 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: 2.301490826655958 --- Val Loss: 2.305108629809651 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: 2.301427849866261 --- Val Loss: 2.3047065143244443 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: 2.3015109125503894 --- Val Loss: 2.3040594821680234 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: 2.3015126026939883 --- Val Loss: 2.304430539404339 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: 2.3015424574836856 --- Val Loss: 2.3045855224053393 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: 2.3014691083623715 --- Val Loss: 2.3043142584395313 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.301577883233797 --- Val Loss: 2.304283480722045 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: 2.301636291271079 --- Val Loss: 2.3052558755370525 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: 2.3015331027452217 --- Val Loss: 2.305218038781493 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: 2.3015033997248917 --- Val Loss: 2.3047648495200437 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: 2.3014927406516255 --- Val Loss: 2.306176529246963 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: 2.3014343238822565 --- Val Loss: 2.305311133476517 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: 2.301471315787751 --- Val Loss: 2.3051010556672016 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3015021726858644 --- Val Loss: 2.3050751215868353 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: 2.3014302657408163 --- Val Loss: 2.3054119561672035 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.30151200707972 --- Val Loss: 2.3048872345359706 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: 2.3015475544983897 --- Val Loss: 2.305353683959585 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: 2.301459424214316 --- Val Loss: 2.3043108558507384 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: 2.301500043663065 --- Val Loss: 2.3058935568210743 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: 2.3015256979548906 --- Val Loss: 2.3053560009613916 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: 2.301475979346527 --- Val Loss: 2.304309071659787 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: 2.301581497070552 --- Val Loss: 2.305757868650277 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.3015277854334792 --- Val Loss: 2.3045354898542065 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: 2.3016015592699754 --- Val Loss: 2.3060309063487923 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: 2.3014697799099824 --- Val Loss: 2.303803808197295 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: 2.3015069985361754 --- Val Loss: 2.3042163016669552 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: 2.3014473704760863 --- Val Loss: 2.3049033181538237 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: 2.301517920856186 --- Val Loss: 2.3054073934460937 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: 2.3014469685246204 --- Val Loss: 2.304835936654165 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: 2.3014494756622583 --- Val Loss: 2.305261424491184 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: 2.3014429789107886 --- Val Loss: 2.3047485567161696 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: 2.3014770208469177 --- Val Loss: 2.3049109667296177 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: 2.30154633222931 --- Val Loss: 2.304603175634746 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: 2.301512192154342 --- Val Loss: 2.3044415477425484 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: 2.3014459736157304 --- Val Loss: 2.3047069884969282 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: 2.301465531812307 --- Val Loss: 2.3048435530313736 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: 2.3014489341619404 --- Val Loss: 2.3045435565071513 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: 2.301554458147233 --- Val Loss: 2.3057252610010033 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.301516967809385 --- Val Loss: 2.305023260908956 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: 2.301477648841765 --- Val Loss: 2.30437018239867 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: 2.3014861908098267 --- Val Loss: 2.303550219827329 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: 2.3015092047394807 --- Val Loss: 2.3052620254540126 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: 2.3014302204364694 --- Val Loss: 2.30535208387161 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: 2.3016431123744683 --- Val Loss: 2.3051320266811244 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: 2.301469962118915 --- Val Loss: 2.3053618020951845 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: 2.301444023368153 --- Val Loss: 2.305578939976523 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: 2.301418944866097 --- Val Loss: 2.305650123024624 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: 2.301573006036308 --- Val Loss: 2.3051909427699555 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: 2.3014599800849482 --- Val Loss: 2.3059838862839706 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: 2.3015000276071835 --- Val Loss: 2.306551140254038 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: 2.301684167040871 --- Val Loss: 2.3063046240114615 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: 2.3015297075787764 --- Val Loss: 2.304940748823976 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: 2.3015303670471274 --- Val Loss: 2.304172244839268 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: 2.3014970173176765 --- Val Loss: 2.3050594634173813 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: 2.3014652808069274 --- Val Loss: 2.304911212095855 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: 2.3014170755350296 --- Val Loss: 2.304865821053637 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: 2.3015471395066056 --- Val Loss: 2.304243875907214 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: 2.301454481182936 --- Val Loss: 2.3039708178333136 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: 2.3015321606646584 --- Val Loss: 2.3047552200970434 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: 2.3014295105101645 --- Val Loss: 2.3046563102569326 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: 2.3014540373999175 --- Val Loss: 2.305212104612594 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: 2.3014610222271688 --- Val Loss: 2.3039308279870196 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: 2.3014455273186996 --- Val Loss: 2.3047539575588853 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: 2.3016618843033636 --- Val Loss: 2.302581211321822 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: 2.301556441080538 --- Val Loss: 2.3057060865202477 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: 2.3014415980276426 --- Val Loss: 2.305242142981626 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: 2.3014769751249666 --- Val Loss: 2.3046348240332093 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: 2.3014632641082695 --- Val Loss: 2.3061476521028825 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: 2.3014756499291007 --- Val Loss: 2.304473791673965 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: 2.3015843744596096 --- Val Loss: 2.305029658950086 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: 2.301488800690443 --- Val Loss: 2.3047126824984145 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: 2.301618732121703 --- Val Loss: 2.3040760231530815 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: 2.301467029821653 --- Val Loss: 2.3045777133496497 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: 2.3014900125380224 --- Val Loss: 2.304446914778315 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: 2.301460630468132 --- Val Loss: 2.305759524523198 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.08333333333333333\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.301432640581465 --- Val Loss: 2.303391439895906 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/1000 --- Train Loss: 3.354766068942759 --- Val Loss: 1.530662103758889 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 20/1000 --- Train Loss: 2.151635477612533 --- Val Loss: 1.859567346822982 --- Train Acc: 0.25 --- Val Acc: 0.28\n",
      "Epoch 30/1000 --- Train Loss: 2.342209082654458 --- Val Loss: 2.2990844232942265 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/1000 --- Train Loss: 2.319257254683442 --- Val Loss: 2.3046369882507163 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: 2.3372922490645647 --- Val Loss: 2.304283629971471 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.304233626430523 --- Val Loss: 2.306866784717734 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3097517394760967 --- Val Loss: 2.3136184082764277 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 20/100 --- Train Loss: 2.3137358637441636 --- Val Loss: 2.3238555667435117 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 30/100 --- Train Loss: 2.3207148749512063 --- Val Loss: 2.3271400290691737 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3032804844467436 --- Val Loss: 2.3051868060479714 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.310316463619697 --- Val Loss: 2.3024562993255806 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.3129422857264785 --- Val Loss: 2.3135662714590945 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.306707128647035 --- Val Loss: 2.3125613648708185 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.3243743509543333 --- Val Loss: 2.3208028791153277 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/100 --- Train Loss: 2.3036930459167837 --- Val Loss: 2.3033843563862324 --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.1111111111111111\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3025013766994658 --- Val Loss: 2.302499339479898 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/100 --- Train Loss: 2.3018069157226213 --- Val Loss: 2.301787583547594 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 2.30131438351551 --- Val Loss: 2.3012809483761147 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: 2.300953385558222 --- Val Loss: 2.3009082166956603 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/100 --- Train Loss: 2.300666168808427 --- Val Loss: 2.3006108025801253 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/100 --- Train Loss: 2.3003942574853253 --- Val Loss: 2.300326151676016 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.3000178287318405 --- Val Loss: 2.299940347021037 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/100 --- Train Loss: 2.2992600283838023 --- Val Loss: 2.299176106154085 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/100 --- Train Loss: 2.297232617868364 --- Val Loss: 2.2971454588516975 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/100 --- Train Loss: 2.2909683445192313 --- Val Loss: 2.2907105486425428 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.12777777777777777\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 3.6288876222592394 --- Val Loss: 2.677049229971249 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 10/500 --- Train Loss: 2.315074740175399 --- Val Loss: 2.2981317574715954 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 20/500 --- Train Loss: 2.327970027101068 --- Val Loss: 2.3031914241738547 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.302661330607897 --- Val Loss: 2.2994142850783845 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.3049702207280984 --- Val Loss: 2.29599615722558 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 50/500 --- Train Loss: 2.304630418515554 --- Val Loss: 2.3081490955923853 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3018483117467454 --- Val Loss: 2.3002153575554485 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 0.7183290096738515 --- Val Loss: 0.543863331440291 --- Train Acc: 0.83 --- Val Acc: 0.77\n",
      "Epoch 20/100 --- Train Loss: 0.4418113011813485 --- Val Loss: 0.27592237489478305 --- Train Acc: 0.92 --- Val Acc: 0.92\n",
      "Epoch 30/100 --- Train Loss: 0.650380807349874 --- Val Loss: 0.3143398076050404 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 40/100 --- Train Loss: 0.8976950993277213 --- Val Loss: 0.6171294809959086 --- Train Acc: 0.85 --- Val Acc: 0.82\n",
      "Epoch 50/100 --- Train Loss: 1.1745798443053008 --- Val Loss: 0.7448072876040828 --- Train Acc: 0.75 --- Val Acc: 0.74\n",
      "Epoch 60/100 --- Train Loss: 1.7984725493985942 --- Val Loss: 1.6831666848793816 --- Train Acc: 0.39 --- Val Acc: 0.38\n",
      "Epoch 70/100 --- Train Loss: 2.197832255774242 --- Val Loss: 2.202975312182392 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 80/100 --- Train Loss: 1.9305385665519814 --- Val Loss: 1.8865972725685702 --- Train Acc: 0.28 --- Val Acc: 0.29\n",
      "Epoch 90/100 --- Train Loss: 2.299325105024926 --- Val Loss: 2.296955213223017 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3021491851936915 --- Val Loss: 2.3024338300861733 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.2928538123149638 --- Val Loss: 2.2949170006512865 --- Train Acc: 0.12 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 0.9343678832786267 --- Val Loss: 0.7915599876569138 --- Train Acc: 0.77 --- Val Acc: 0.79\n",
      "Epoch 30/100 --- Train Loss: 0.514089175047661 --- Val Loss: 0.22713932104041754 --- Train Acc: 0.92 --- Val Acc: 0.92\n",
      "Epoch 40/100 --- Train Loss: 0.43501335811971703 --- Val Loss: 0.15274661468303596 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 50/100 --- Train Loss: 0.359651459209619 --- Val Loss: 0.12394177155022874 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 60/100 --- Train Loss: 0.3413469568993816 --- Val Loss: 0.09668710978454738 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 70/100 --- Train Loss: 0.314954676097815 --- Val Loss: 0.09636664918075913 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 80/100 --- Train Loss: 0.3380865283814502 --- Val Loss: 0.08150441441945017 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 90/100 --- Train Loss: 0.35034979160946084 --- Val Loss: 0.09263017387119718 --- Train Acc: 0.96 --- Val Acc: 0.98\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.8944444444444445\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 3.9961237929162583 --- Val Loss: 3.545489090640954 --- Train Acc: 0.62 --- Val Acc: 0.61\n",
      "Epoch 10/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 20/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 30/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 40/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 60/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 80/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 100/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3006797497795093 --- Val Loss: 2.3036877599602477 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.3529433919100677 --- Val Loss: 2.3016005649219005 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/1000 --- Train Loss: 2.335469265336861 --- Val Loss: 2.301220202141389 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.325078524886351 --- Val Loss: 2.301971507269942 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.3010870247390662 --- Val Loss: 2.3035211912465776 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.301288827701039 --- Val Loss: 2.303875104023762 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: 2.3010498776684702 --- Val Loss: 2.303292928099072 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.3010185287268508 --- Val Loss: 2.3037120748435496 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.301060123005509 --- Val Loss: 2.3018595636403054 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: 2.30070821011291 --- Val Loss: 2.3025031782867478 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: 2.300885234226236 --- Val Loss: 2.302915873687623 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.301251403174658 --- Val Loss: 2.30127567073399 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.300862840077345 --- Val Loss: 2.3021185617677524 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.300933210649912 --- Val Loss: 2.3021922484314956 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.301072936103133 --- Val Loss: 2.3036298502005854 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: 2.3016524349717744 --- Val Loss: 2.303482801616976 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.301407540012506 --- Val Loss: 2.3030649536409125 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.301155507771707 --- Val Loss: 2.304344826852637 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: 2.300947955945166 --- Val Loss: 2.302260357556741 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.300882996089666 --- Val Loss: 2.302237170119588 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.301189400047207 --- Val Loss: 2.3045224664965356 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.300865739831873 --- Val Loss: 2.302369472105598 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.3010387140692563 --- Val Loss: 2.303699329852461 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.3010217748145694 --- Val Loss: 2.3030762874521464 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.3010647788296854 --- Val Loss: 2.304181156437268 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: 2.3011484896282037 --- Val Loss: 2.303779938070175 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.3010556771438893 --- Val Loss: 2.3021320835682717 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: 2.301114095699765 --- Val Loss: 2.303860603778469 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.3010289314033825 --- Val Loss: 2.302475223303239 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.3010944966082705 --- Val Loss: 2.3032326795755336 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.3015268964331685 --- Val Loss: 2.3044907358002598 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.301691463736458 --- Val Loss: 2.302911168552079 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.301009547611203 --- Val Loss: 2.304159550585531 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.3010539241667236 --- Val Loss: 2.3029473293116003 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3010453552425534 --- Val Loss: 2.3037627096281352 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.301167851802335 --- Val Loss: 2.3033762678841403 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.3015311288238256 --- Val Loss: 2.304151048445641 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3009710124149936 --- Val Loss: 2.301426921284293 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.301108410571386 --- Val Loss: 2.302059273862277 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.30100475571558 --- Val Loss: 2.3031010060829935 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.301299180530038 --- Val Loss: 2.3037360111093386 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: 2.30106737752473 --- Val Loss: 2.3037149989986374 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.300801026620164 --- Val Loss: 2.3026506035554664 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.300767499690564 --- Val Loss: 2.301655752488185 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3011097603376105 --- Val Loss: 2.303796214498554 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.301159354478669 --- Val Loss: 2.301767133147122 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.301082984792157 --- Val Loss: 2.302085938063833 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.300942778088838 --- Val Loss: 2.3033758756647353 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.3014597183183807 --- Val Loss: 2.3029560497443127 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: 2.3007841446024795 --- Val Loss: 2.3015883776135477 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.30100164982663 --- Val Loss: 2.3036228823106626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.3012561504672417 --- Val Loss: 2.302465318074438 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.3011024856297406 --- Val Loss: 2.3016814487087194 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3010237009772676 --- Val Loss: 2.3022710083447704 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.3009200994759853 --- Val Loss: 2.303250677021382 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.300818311429359 --- Val Loss: 2.302889025409147 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3016182465988173 --- Val Loss: 2.3042497183984008 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.300878174440406 --- Val Loss: 2.3030475149064795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.3009708182744886 --- Val Loss: 2.303981240738863 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3013991230500035 --- Val Loss: 2.3031965487694905 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.301052042373507 --- Val Loss: 2.3031858201184314 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.3008274783066778 --- Val Loss: 2.3024360764989478 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.300998243553733 --- Val Loss: 2.303839822802912 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.301173090002154 --- Val Loss: 2.30210674684766 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.300978177340301 --- Val Loss: 2.3038104439111144 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.3011981686720557 --- Val Loss: 2.3008910904581517 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.3011422034787667 --- Val Loss: 2.30369937442178 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3011261993853616 --- Val Loss: 2.3033750896655274 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.3009091476640275 --- Val Loss: 2.3023455172983986 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.300899507945008 --- Val Loss: 2.302737164574835 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.300874841082602 --- Val Loss: 2.3016098193388643 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.3008088495458834 --- Val Loss: 2.3024303966700566 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.30107880660699 --- Val Loss: 2.3011603573847816 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.3009298425334106 --- Val Loss: 2.3022016514986077 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.3015642482380683 --- Val Loss: 2.302738129250448 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: 2.3015531321317217 --- Val Loss: 2.302686148650124 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.301014565462149 --- Val Loss: 2.302924713265595 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.3010398760617607 --- Val Loss: 2.3028294496907846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.3010545385267847 --- Val Loss: 2.3039831872673737 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.3010593138122375 --- Val Loss: 2.3012065069519436 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: 2.301277126666513 --- Val Loss: 2.3044948271297603 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3010059216639327 --- Val Loss: 2.303767404910689 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: 2.3010084406448943 --- Val Loss: 2.302389978926562 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.301167898405433 --- Val Loss: 2.302656157196644 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.3010820742572595 --- Val Loss: 2.3030058264170408 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.3009771603221063 --- Val Loss: 2.3022945179232748 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: 2.301014973915281 --- Val Loss: 2.3035001337306586 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.3009252704728316 --- Val Loss: 2.301267116387601 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3012656036700836 --- Val Loss: 2.301490859851196 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.300975957028552 --- Val Loss: 2.302805632080418 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.301266764015945 --- Val Loss: 2.3040712966168457 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.3009311163446724 --- Val Loss: 2.30359259325775 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: 2.3008498412659697 --- Val Loss: 2.3033325932943653 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.3008296779191166 --- Val Loss: 2.303322059670834 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.300965165304553 --- Val Loss: 2.302627613993128 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.3009073161781166 --- Val Loss: 2.301646750771061 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.3010991321073955 --- Val Loss: 2.304717702303745 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.300948918432253 --- Val Loss: 2.302767024031607 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.301141981876307 --- Val Loss: 2.299958547334605 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3012338311493794 --- Val Loss: 2.3025828310934933 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302486065197881 --- Val Loss: 2.302466613150183 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.301687578607337 --- Val Loss: 2.3014694180650683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.3013221383424045 --- Val Loss: 2.3009036142254717 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.301133837664241 --- Val Loss: 2.300634402085802 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3010392503668586 --- Val Loss: 2.3004391631581953 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.3009973476268297 --- Val Loss: 2.3003234388944143 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.300976605295258 --- Val Loss: 2.3002506202633004 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3009660919854062 --- Val Loss: 2.3002474621932594 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.30096034169183 --- Val Loss: 2.3001874519309626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.300957620520355 --- Val Loss: 2.300170944151405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3017704052662467 --- Val Loss: 2.3022983909585673 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 1.2460755384754492 --- Val Loss: 0.9051533099130019 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 20/500 --- Train Loss: 1.7781039536039038 --- Val Loss: 1.6102554115241667 --- Train Acc: 0.41 --- Val Acc: 0.41\n",
      "Epoch 30/500 --- Train Loss: 2.606077385198295 --- Val Loss: 2.308444100309798 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/500 --- Train Loss: 2.31366239951464 --- Val Loss: 2.3011890954423992 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/500 --- Train Loss: 2.3017640609647296 --- Val Loss: 2.301763854451262 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/500 --- Train Loss: 2.3016575775338635 --- Val Loss: 2.301476097923737 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3377590181737347 --- Val Loss: 2.3011442865812826 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.3016378527263464 --- Val Loss: 2.301121404629325 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/500 --- Train Loss: 2.301644547739133 --- Val Loss: 2.301286609854031 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 100/500 --- Train Loss: 2.3016670436450997 --- Val Loss: 2.3012505941096326 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: 2.3136506945826842 --- Val Loss: 2.301110299331945 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: 2.3016915243944998 --- Val Loss: 2.3013494782762236 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.301661464094573 --- Val Loss: 2.300612642500114 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/500 --- Train Loss: 2.301648173114585 --- Val Loss: 2.301123418583964 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/500 --- Train Loss: 2.3016414640080662 --- Val Loss: 2.30107952757926 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/500 --- Train Loss: 2.3017385632695913 --- Val Loss: 2.3006850651715105 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/500 --- Train Loss: 2.3016863586391905 --- Val Loss: 2.301584238649505 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.301638966276042 --- Val Loss: 2.3008567936901527 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.3017006322574516 --- Val Loss: 2.300929886146019 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/500 --- Train Loss: 2.3019842727938737 --- Val Loss: 2.3004376692994324 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/500 --- Train Loss: 2.3016623950921837 --- Val Loss: 2.3004232271899996 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/500 --- Train Loss: 2.3016523687712405 --- Val Loss: 2.3015398018133397 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.301633738390959 --- Val Loss: 2.3009219828654106 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/500 --- Train Loss: 2.3016571808083843 --- Val Loss: 2.3013283904241475 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.3016694238581255 --- Val Loss: 2.3011533071236467 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.3016740564294333 --- Val Loss: 2.3011468127860955 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/500 --- Train Loss: 2.3016902910190344 --- Val Loss: 2.301807093186159 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.3016211193966503 --- Val Loss: 2.301015502689574 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.299636027510955 --- Val Loss: 2.3017283101891954 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/500 --- Train Loss: 2.30167555369314 --- Val Loss: 2.3009494361264617 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/500 --- Train Loss: 2.3016851014491695 --- Val Loss: 2.300777157003596 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/500 --- Train Loss: 2.3016821396056137 --- Val Loss: 2.3014633789671657 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/500 --- Train Loss: 2.3016934627166092 --- Val Loss: 2.300756507705046 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/500 --- Train Loss: 2.3016906692803927 --- Val Loss: 2.300824324530139 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.301663443401084 --- Val Loss: 2.3008046076846114 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/500 --- Train Loss: 2.301668216892037 --- Val Loss: 2.3011291141485013 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/500 --- Train Loss: 2.3016632815991946 --- Val Loss: 2.300697318698274 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/500 --- Train Loss: 2.3017146764611063 --- Val Loss: 2.301149831438606 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/500 --- Train Loss: 2.3016725871780137 --- Val Loss: 2.3004580464175755 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: 2.301705326573312 --- Val Loss: 2.301922587735969 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.3016532709246342 --- Val Loss: 2.300908202985876 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.3016527872856143 --- Val Loss: 2.300723906290269 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/500 --- Train Loss: 2.3016623921548 --- Val Loss: 2.3011372270783985 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.3016742686966762 --- Val Loss: 2.301469377134757 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/500 --- Train Loss: 2.3016658606789955 --- Val Loss: 2.30063442360748 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/500 --- Train Loss: 2.3016631895610518 --- Val Loss: 2.3008623262647943 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/500 --- Train Loss: 2.301662256677491 --- Val Loss: 2.3007689446029267 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/500 --- Train Loss: 2.301651169003458 --- Val Loss: 2.3013691255267763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.3016432939209683 --- Val Loss: 2.301020436209676 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3025033958956715 --- Val Loss: 2.30246061569254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.301830097917374 --- Val Loss: 2.3014081956662267 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.301361829888968 --- Val Loss: 2.3006189791572993 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.3010351323020446 --- Val Loss: 2.3000183104279563 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.3008066243085863 --- Val Loss: 2.2995599112586813 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.3006464811785996 --- Val Loss: 2.2992028979541255 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: 2.300535490737451 --- Val Loss: 2.2989297846611887 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.300457427154035 --- Val Loss: 2.29871336375044 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.300403814364228 --- Val Loss: 2.2985391242043445 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.3003659236684553 --- Val Loss: 2.298404936685853 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.300339266984353 --- Val Loss: 2.298293143786629 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.3003206273533086 --- Val Loss: 2.298196313116686 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.300307416996023 --- Val Loss: 2.2981160411620434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.3002982009253845 --- Val Loss: 2.298053906979871 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.3002917131568665 --- Val Loss: 2.297997825579088 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3002871054775547 --- Val Loss: 2.2979497485856437 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3002838906848417 --- Val Loss: 2.2979111197710322 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.3002815874608245 --- Val Loss: 2.297878764683427 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.300279952815566 --- Val Loss: 2.2978457041820555 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.300278802110238 --- Val Loss: 2.29782451389626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.300277941052459 --- Val Loss: 2.2978080078719745 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.300277393614182 --- Val Loss: 2.2977979778214856 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.3002769225181874 --- Val Loss: 2.2977824247737044 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.3002765914391197 --- Val Loss: 2.2977673915202343 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.3002764216737415 --- Val Loss: 2.297757383158094 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.300276239378981 --- Val Loss: 2.2977502639692275 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.300276071476942 --- Val Loss: 2.2977415647337502 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.300275947779508 --- Val Loss: 2.2977296907443945 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.300275902178251 --- Val Loss: 2.2977256713675556 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.300275831767346 --- Val Loss: 2.297723503416922 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3002757578373547 --- Val Loss: 2.29772084267907 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3002757290686144 --- Val Loss: 2.2977199604899017 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.3002756526451096 --- Val Loss: 2.2977152543483204 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.300275619241156 --- Val Loss: 2.2977133560234617 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.3002755654949167 --- Val Loss: 2.297711449910607 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.300275537357528 --- Val Loss: 2.297708145168304 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.3002755069689518 --- Val Loss: 2.2977032312992303 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3002754636599607 --- Val Loss: 2.2977029617942955 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3002754307122886 --- Val Loss: 2.2977004894780295 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3002754000405297 --- Val Loss: 2.2977011787937585 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3002753736332227 --- Val Loss: 2.29770434557689 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.300275334687655 --- Val Loss: 2.2977028714624175 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3002753205471826 --- Val Loss: 2.297696880129842 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.300275258536257 --- Val Loss: 2.2976989074223315 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3002751967378288 --- Val Loss: 2.297699369784641 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3002751413352924 --- Val Loss: 2.297698463576536 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.300275127518404 --- Val Loss: 2.2977000332425703 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3002750934320573 --- Val Loss: 2.297695593400157 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.3002750513242973 --- Val Loss: 2.2976934428510596 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.3002749846211232 --- Val Loss: 2.29769392630775 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.300274945869581 --- Val Loss: 2.297695089826138 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.300274881367951 --- Val Loss: 2.2976932790878384 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.3002748179413284 --- Val Loss: 2.297691900737294 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.3002747707086444 --- Val Loss: 2.2976928799712755 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3002747589946773 --- Val Loss: 2.2976901683546145 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.3002746622209136 --- Val Loss: 2.2976944541479236 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.30027462264641 --- Val Loss: 2.2976898129739816 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3002745709999886 --- Val Loss: 2.297692836128002 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.3002745223765677 --- Val Loss: 2.2976967527638132 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.300274440113963 --- Val Loss: 2.2977019375542165 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.3002743281344746 --- Val Loss: 2.2977017389543715 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.3002743168495963 --- Val Loss: 2.297700496115983 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3002742241991982 --- Val Loss: 2.2976984704663073 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.300274125236198 --- Val Loss: 2.2977005325292534 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.300274138429447 --- Val Loss: 2.2976975729121647 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.300274006709638 --- Val Loss: 2.297694343067008 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.300273961227466 --- Val Loss: 2.2976929106796273 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.300273835775966 --- Val Loss: 2.2976919728366605 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.300273721248027 --- Val Loss: 2.2976895637795893 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.300273584318522 --- Val Loss: 2.2976921225510885 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.3002736001243136 --- Val Loss: 2.2976943651922688 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.3002734985658453 --- Val Loss: 2.297692213795979 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.30027339489607 --- Val Loss: 2.2976903169588265 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3002733368417156 --- Val Loss: 2.297695777423538 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.3002731057300148 --- Val Loss: 2.297698203459248 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.3002730664788853 --- Val Loss: 2.297701553079477 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3002728482503745 --- Val Loss: 2.2976919651759764 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.300272912514689 --- Val Loss: 2.2976949188515023 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.300272645095394 --- Val Loss: 2.2976939362608975 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.3002725383747085 --- Val Loss: 2.2976886909979566 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.30027244038698 --- Val Loss: 2.297691046547035 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.300272267499009 --- Val Loss: 2.297688531916412 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.300272114902555 --- Val Loss: 2.2976935765203566 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.3002720181290863 --- Val Loss: 2.2976897528885196 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3002718411352268 --- Val Loss: 2.297692905046768 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.3002716702743036 --- Val Loss: 2.297695393868558 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.30027141451847 --- Val Loss: 2.297696904713327 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.3002712511372616 --- Val Loss: 2.297695890463793 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.3002711027700515 --- Val Loss: 2.2976980169072676 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3002709149524883 --- Val Loss: 2.297697941412495 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3002706942828577 --- Val Loss: 2.297697698685191 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.300270369042498 --- Val Loss: 2.2976954439998334 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.300270280558069 --- Val Loss: 2.2976926373175166 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.300269870277303 --- Val Loss: 2.2976904659335027 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.300269685459426 --- Val Loss: 2.297689348674288 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3002695823119916 --- Val Loss: 2.2976902665196666 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.300269284900971 --- Val Loss: 2.2976879054680275 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: 2.3002687937204027 --- Val Loss: 2.2976864750245167 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.300268564334073 --- Val Loss: 2.2976870715581885 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.300268056319981 --- Val Loss: 2.2976876993059303 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302054845087167 --- Val Loss: 2.3018616209071876 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/500 --- Train Loss: 2.3014130777925645 --- Val Loss: 2.2994232343709133 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/500 --- Train Loss: 2.3014196318689053 --- Val Loss: 2.2988651515690925 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/500 --- Train Loss: 2.3014132837979004 --- Val Loss: 2.299547813662332 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/500 --- Train Loss: 2.301413283140925 --- Val Loss: 2.299364601598388 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/500 --- Train Loss: 2.301411168945047 --- Val Loss: 2.298619650554926 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/500 --- Train Loss: 2.301418941326622 --- Val Loss: 2.2992195943785645 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/500 --- Train Loss: 2.30140540506114 --- Val Loss: 2.299556536411923 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 80/500 --- Train Loss: 2.301141869848053 --- Val Loss: 2.298500495133368 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/500 --- Train Loss: 2.170280445783214 --- Val Loss: 2.162452299945071 --- Train Acc: 0.30 --- Val Acc: 0.29\n",
      "Epoch 100/500 --- Train Loss: 0.9817253965219943 --- Val Loss: 0.8990150632383559 --- Train Acc: 0.57 --- Val Acc: 0.59\n",
      "Epoch 110/500 --- Train Loss: 0.3121371243716217 --- Val Loss: 0.17461515484721243 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 120/500 --- Train Loss: 0.15931210305079607 --- Val Loss: 0.051891370210037806 --- Train Acc: 0.98 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.10631307745306151 --- Val Loss: 0.019629696581084575 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.07190038120469659 --- Val Loss: 0.016130914329571384 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.062400612769552555 --- Val Loss: 0.008670041808927316 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.06061260666861808 --- Val Loss: 0.004383874908878791 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.0384000213823373 --- Val Loss: 0.0036071911265362533 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/500 --- Train Loss: 0.026293464995295585 --- Val Loss: 0.002996446988369789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.03158265224929314 --- Val Loss: 0.0027486965317138757 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/500 --- Train Loss: 0.025683555981760735 --- Val Loss: 0.0027376082433849826 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.021237895994319127 --- Val Loss: 0.002318237690695529 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.02876940064458767 --- Val Loss: 0.00200843113331297 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.02877943191950681 --- Val Loss: 0.00219059149804004 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.03186120823869051 --- Val Loss: 0.001213940394439521 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.027669518877720192 --- Val Loss: 0.0011824222510736122 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.017269100330017127 --- Val Loss: 0.0015168668735560853 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.014775069286799414 --- Val Loss: 0.0011189118770525107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/500 --- Train Loss: 0.02252225661338391 --- Val Loss: 0.0010862587557314482 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.015726679879074425 --- Val Loss: 0.0009781101654725507 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.01687817734541235 --- Val Loss: 0.0007226124203346604 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.008214853659790904 --- Val Loss: 0.0006834603188083628 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.01853253488196245 --- Val Loss: 0.0017275863307055857 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/500 --- Train Loss: 0.013002848537636105 --- Val Loss: 0.0007473761563156593 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/500 --- Train Loss: 0.011078001608024317 --- Val Loss: 0.0006113257918886003 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/500 --- Train Loss: 0.013853248915365107 --- Val Loss: 0.0006521250745860525 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.012211022323891912 --- Val Loss: 0.00048171470908344604 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.016472052847269048 --- Val Loss: 0.00037440768950029274 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/500 --- Train Loss: 0.02130754264350078 --- Val Loss: 0.0004211361389122462 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/500 --- Train Loss: 0.022250032189971475 --- Val Loss: 0.0003634495667788197 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.014955317596227804 --- Val Loss: 0.00042014356665643856 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/500 --- Train Loss: 0.010610080817444618 --- Val Loss: 0.0004292270215728459 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.007872362817633517 --- Val Loss: 0.0004865978651766281 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/500 --- Train Loss: 0.016512268115420826 --- Val Loss: 0.0004167953286852466 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/500 --- Train Loss: 0.015198577125190349 --- Val Loss: 0.0006595937903679835 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/500 --- Train Loss: 0.014293303985841114 --- Val Loss: 0.0005885970490258562 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/500 --- Train Loss: 0.011980000924527565 --- Val Loss: 0.00031937352272837786 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/500 --- Train Loss: 0.015341507301387601 --- Val Loss: 0.00030998302207554257 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/500 --- Train Loss: 0.010096790895168125 --- Val Loss: 0.0002475255203593591 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/500 --- Train Loss: 0.010653888573272147 --- Val Loss: 0.0002175389044195657 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.9416666666666667\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302536682911595 --- Val Loss: 2.3025872303939185 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/100 --- Train Loss: 2.30213837108445 --- Val Loss: 2.3026475150891623 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 20/100 --- Train Loss: 2.3018589934545286 --- Val Loss: 2.3027520373333927 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 30/100 --- Train Loss: 2.301664419602402 --- Val Loss: 2.3028764269151267 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/100 --- Train Loss: 2.301530017648658 --- Val Loss: 2.3030077109030596 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/100 --- Train Loss: 2.301436453314211 --- Val Loss: 2.303135369293013 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/100 --- Train Loss: 2.30137112866563 --- Val Loss: 2.3032573091889104 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.3013262444339047 --- Val Loss: 2.303364815410605 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.3012951044867984 --- Val Loss: 2.303458040305521 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 90/100 --- Train Loss: 2.301272899906186 --- Val Loss: 2.303542285171533 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 1.288305391410199 --- Val Loss: 1.1968699148809345 --- Train Acc: 0.60 --- Val Acc: 0.59\n",
      "Epoch 10/1000 --- Train Loss: 2.326735989156906 --- Val Loss: 2.301319527650603 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 0.5684049905359011 --- Val Loss: 0.4117258647685856 --- Train Acc: 0.86 --- Val Acc: 0.89\n",
      "Epoch 10/100 --- Train Loss: 2.1344745137315493 --- Val Loss: 2.1495276014037934 --- Train Acc: 0.18 --- Val Acc: 0.17\n",
      "Epoch 20/100 --- Train Loss: 2.6621971242595177 --- Val Loss: 2.3484255085111325 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3023846117089697 --- Val Loss: 2.3027834621505674 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 10/100 --- Train Loss: 2.3011695837736843 --- Val Loss: 2.3040029424300976 --- Train Acc: 0.11 --- Val Acc: 0.07\n",
      "Epoch 20/100 --- Train Loss: 2.1769161820360163 --- Val Loss: 2.1668526798219254 --- Train Acc: 0.39 --- Val Acc: 0.48\n",
      "Epoch 30/100 --- Train Loss: 0.5774523907616677 --- Val Loss: 0.4564982498019645 --- Train Acc: 0.86 --- Val Acc: 0.90\n",
      "Epoch 40/100 --- Train Loss: 0.22993517089907567 --- Val Loss: 0.13677944052073382 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 50/100 --- Train Loss: 0.1535093142689304 --- Val Loss: 0.049258808612769774 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 60/100 --- Train Loss: 0.12658129523832437 --- Val Loss: 0.027360591968416163 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 70/100 --- Train Loss: 0.08823974496558086 --- Val Loss: 0.01300646269861677 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.09876004166788514 --- Val Loss: 0.008302416683389423 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.07324302330104901 --- Val Loss: 0.006358958338702896 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.9638888888888889\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.2998129573007295 --- Val Loss: 2.2977672176962463 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3100108655219382 --- Val Loss: 2.2983168084869448 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/100 --- Train Loss: 2.2964813436202216 --- Val Loss: 2.2979714122224557 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/100 --- Train Loss: 2.3000966970890784 --- Val Loss: 2.2966929842649835 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/100 --- Train Loss: 2.3003539320642465 --- Val Loss: 2.2999464558239815 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/100 --- Train Loss: 2.300293296822259 --- Val Loss: 2.300515168580864 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 60/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 70/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 80/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 90/100 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.3022273244998264 --- Val Loss: 2.302418004552241 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.300687509158368 --- Val Loss: 2.3016125113688894 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.300661394507002 --- Val Loss: 2.3015861097012857 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3006599713165445 --- Val Loss: 2.301908724004579 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.300653949642435 --- Val Loss: 2.301666770458076 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.300651102824369 --- Val Loss: 2.3018388803817347 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.300648969753629 --- Val Loss: 2.3019970905836535 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3006416811399695 --- Val Loss: 2.3019506248263477 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.300617085143434 --- Val Loss: 2.3015779794250313 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.3005516303000144 --- Val Loss: 2.301740387777628 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09722222222222222\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3011719388490106 --- Val Loss: 2.3004686713426086 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 0.6416657941643134 --- Val Loss: 0.2018379182207396 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Epoch 20/1000 --- Train Loss: 1.6632121568968512 --- Val Loss: 0.7961151168117921 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 30/1000 --- Train Loss: 1.5903827392794645 --- Val Loss: 0.49920958485955064 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 40/1000 --- Train Loss: 1.7194307658038677 --- Val Loss: 0.5714461503185485 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Epoch 50/1000 --- Train Loss: 1.2805746221047491 --- Val Loss: 0.37296591230506326 --- Train Acc: 0.90 --- Val Acc: 0.90\n",
      "Epoch 60/1000 --- Train Loss: 1.7351261823289306 --- Val Loss: 1.2349290502458021 --- Train Acc: 0.54 --- Val Acc: 0.51\n",
      "Epoch 70/1000 --- Train Loss: 2.9916566497418837 --- Val Loss: 2.457043528924342 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.301170884781 --- Val Loss: 2.298220335263612 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: 2.3009018509470636 --- Val Loss: 2.2985185476304597 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: 2.3008905503226504 --- Val Loss: 2.298344002000234 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3008970291002795 --- Val Loss: 2.2980211989508854 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.3129142756218846 --- Val Loss: 2.297936737814601 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.3008926865304904 --- Val Loss: 2.298407243714644 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.300890681514562 --- Val Loss: 2.2980303496586587 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.3008892065340247 --- Val Loss: 2.298212589129514 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.3008912142545 --- Val Loss: 2.2986553207854445 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.300895771209353 --- Val Loss: 2.2978067756900193 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: 2.3008996498438945 --- Val Loss: 2.297919184052588 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.300893613986388 --- Val Loss: 2.298073066670148 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.30089393745935 --- Val Loss: 2.297844125446074 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.300893062128484 --- Val Loss: 2.2984197889368927 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.300891365513873 --- Val Loss: 2.298533941443375 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.300889192796556 --- Val Loss: 2.2982771719041093 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.3008931928901326 --- Val Loss: 2.2978564422407577 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: 2.3008974211957804 --- Val Loss: 2.2979980921305065 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.3008934587677867 --- Val Loss: 2.298164440376683 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: 2.3008957563890466 --- Val Loss: 2.2982461193807833 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.3008900627340076 --- Val Loss: 2.29826565224379 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.3008928081216538 --- Val Loss: 2.2983551274347964 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.3008951738254146 --- Val Loss: 2.2975965676034145 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: 2.300891501191742 --- Val Loss: 2.2985296755236706 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.3009035255800585 --- Val Loss: 2.2980504297269073 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.3008884972443737 --- Val Loss: 2.298274781336143 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3008953369929652 --- Val Loss: 2.2982161534781085 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.30089553923283 --- Val Loss: 2.298123058033435 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.30089438319145 --- Val Loss: 2.2980551923433152 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3008915804710264 --- Val Loss: 2.298219224328337 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.3008941513667676 --- Val Loss: 2.2979425741976782 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.300892522584058 --- Val Loss: 2.297992358205971 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: 2.3008897060660853 --- Val Loss: 2.298223616790011 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3008940014087558 --- Val Loss: 2.298107336663865 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.3008919940170496 --- Val Loss: 2.2980814365700257 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3008926132746645 --- Val Loss: 2.2985176364307334 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3008926801388565 --- Val Loss: 2.298035896434612 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.3009032672856886 --- Val Loss: 2.2977906080950126 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3008954124442127 --- Val Loss: 2.297542985708328 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.300909540185089 --- Val Loss: 2.298937154262607 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.3008961644381087 --- Val Loss: 2.29808462884405 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.30089310037143 --- Val Loss: 2.298173127800087 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3008904145297904 --- Val Loss: 2.2979989840747868 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.300898014299629 --- Val Loss: 2.298238848530106 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.30089348582612 --- Val Loss: 2.297936280971727 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3008921521909795 --- Val Loss: 2.2981781540959245 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.300892602488218 --- Val Loss: 2.298558688738383 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.300899885206549 --- Val Loss: 2.2981318478748007 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3008862178040372 --- Val Loss: 2.298067239185644 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.300893227290683 --- Val Loss: 2.2983275067558036 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.3008968244711525 --- Val Loss: 2.297974599241587 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.30089584984322 --- Val Loss: 2.298024165084467 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.3008972539797976 --- Val Loss: 2.297472676832028 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.3008947557081423 --- Val Loss: 2.2981461084561228 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.300897706311017 --- Val Loss: 2.2982710681734786 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.30089325319212 --- Val Loss: 2.298094538970248 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.3008876017127147 --- Val Loss: 2.29797250313552 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.300893636042769 --- Val Loss: 2.298134508065215 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.3008950070618233 --- Val Loss: 2.2980659169529885 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3008964348390135 --- Val Loss: 2.298270724930218 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.3008904473782223 --- Val Loss: 2.2977389570486015 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.3008951321583733 --- Val Loss: 2.2981523284131242 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3008894510023032 --- Val Loss: 2.298128670147477 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.3008893747052515 --- Val Loss: 2.2983122543626076 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.300892261145913 --- Val Loss: 2.29829331186939 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.300894357457184 --- Val Loss: 2.298235503692618 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.3008884150285627 --- Val Loss: 2.2979576242559636 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.3008892484741232 --- Val Loss: 2.2980772392598503 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.300894835497673 --- Val Loss: 2.2984174477927 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.300891833176325 --- Val Loss: 2.2982719786487715 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.300896435442948 --- Val Loss: 2.298034955062678 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.300897584998152 --- Val Loss: 2.2977707630328497 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: 2.300896943086726 --- Val Loss: 2.2980290519538507 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.300899364708611 --- Val Loss: 2.298487070978997 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.300891780046517 --- Val Loss: 2.29784183000999 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.3008948741128874 --- Val Loss: 2.2980067512560365 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.3008944298345533 --- Val Loss: 2.297947997757049 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.3008930323235615 --- Val Loss: 2.29839609389938 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3008921628558556 --- Val Loss: 2.298077196645374 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.3008926108775096 --- Val Loss: 2.2979854218503126 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3008958437128912 --- Val Loss: 2.2978621740136016 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.300894822310064 --- Val Loss: 2.298209641722063 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.3008932220176646 --- Val Loss: 2.2979864743904015 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.300895280098648 --- Val Loss: 2.2981870208442383 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: 2.300908846342092 --- Val Loss: 2.2987638664613406 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: 2.300890830231768 --- Val Loss: 2.2979781394093117 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.3008943701748628 --- Val Loss: 2.297612836082994 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.300892047485255 --- Val Loss: 2.2980207137484157 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.3008920656698786 --- Val Loss: 2.2985164199316013 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.3008933894637646 --- Val Loss: 2.2980620561738907 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.3008956005871695 --- Val Loss: 2.298178879058895 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: 2.3008889034050886 --- Val Loss: 2.298284124339395 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09722222222222222\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3020412553523126 --- Val Loss: 2.3029276105589993 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: 0.4955080698728554 --- Val Loss: 0.4217581324108577 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 20/500 --- Train Loss: 0.2555486980792372 --- Val Loss: 0.08218423118449113 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 30/500 --- Train Loss: 0.4516757658311339 --- Val Loss: 0.06737424069123563 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 40/500 --- Train Loss: 0.6208735470560378 --- Val Loss: 0.1891169599933682 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 50/500 --- Train Loss: 0.7978842350449247 --- Val Loss: 0.12820934640194667 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 60/500 --- Train Loss: 0.6845087655568031 --- Val Loss: 0.10139708434687744 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 70/500 --- Train Loss: 0.860717015506433 --- Val Loss: 0.17442834623991504 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 80/500 --- Train Loss: 0.3363039537469163 --- Val Loss: 0.10137854758992415 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 90/500 --- Train Loss: 0.5474372091767493 --- Val Loss: 0.11232132335163082 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 100/500 --- Train Loss: 0.35039348154257705 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/500 --- Train Loss: 0.5185823046830117 --- Val Loss: 0.22464254670325678 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 120/500 --- Train Loss: 0.5450356922797007 --- Val Loss: 0.22464254670325676 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 130/500 --- Train Loss: 0.29273249418605846 --- Val Loss: 0.0624337720355027 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.17582496863044747 --- Val Loss: 0.09610265449728655 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 150/500 --- Train Loss: 1.1624888272307257 --- Val Loss: 1.0331576024761504 --- Train Acc: 0.62 --- Val Acc: 0.60\n",
      "Epoch 160/500 --- Train Loss: 1.048158767372848 --- Val Loss: 1.043278536344693 --- Train Acc: 0.63 --- Val Acc: 0.61\n",
      "Epoch 170/500 --- Train Loss: 1.1934664980502108 --- Val Loss: 1.2357098426998443 --- Train Acc: 0.56 --- Val Acc: 0.53\n",
      "Epoch 180/500 --- Train Loss: 1.3326838263169651 --- Val Loss: 1.3831673268801479 --- Train Acc: 0.48 --- Val Acc: 0.46\n",
      "Epoch 190/500 --- Train Loss: 2.315678494436564 --- Val Loss: 2.3176191489712203 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 200/500 --- Train Loss: 2.28936892341257 --- Val Loss: 2.2886054161411313 --- Train Acc: 0.12 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.2735682225245624 --- Val Loss: 2.2893580061518306 --- Train Acc: 0.12 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.2917827194340323 --- Val Loss: 2.2902888254175178 --- Train Acc: 0.12 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.2817865742040575 --- Val Loss: 2.2903799544750534 --- Train Acc: 0.12 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.3224640759134525 --- Val Loss: 2.3069322295648163 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: 2.3004617760073085 --- Val Loss: 2.306760905245427 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: 2.298392495515229 --- Val Loss: 2.306910488219301 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: 2.300451713815004 --- Val Loss: 2.306669845287592 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: 2.30045213787066 --- Val Loss: 2.306764450853829 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: 2.300451376933221 --- Val Loss: 2.306795566930445 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: 2.300451199979311 --- Val Loss: 2.306808870662273 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: 2.3004510303688988 --- Val Loss: 2.3067947170216 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: 2.2984081296850696 --- Val Loss: 2.3066714753935242 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: 2.3223537964711154 --- Val Loss: 2.306836756684038 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: 2.296330911944074 --- Val Loss: 2.3068645777590255 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: 2.2983916999541227 --- Val Loss: 2.3069503707051777 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: 2.2983907906669194 --- Val Loss: 2.3069038669795465 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: 2.2963275678955566 --- Val Loss: 2.306791333513731 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: 2.3004543948193703 --- Val Loss: 2.3067153694682716 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: 2.3004543930322185 --- Val Loss: 2.306743137183376 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: 2.300452211651849 --- Val Loss: 2.306825201916126 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: 2.300451517352649 --- Val Loss: 2.3067901951946688 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: 2.3004517704367355 --- Val Loss: 2.30678722514046 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: 2.3004519903022875 --- Val Loss: 2.306838516324551 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: 2.3004516312570717 --- Val Loss: 2.306758296610963 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: 2.3004516132489576 --- Val Loss: 2.3068202192935994 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: 2.300452251119452 --- Val Loss: 2.3067637096611633 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: 2.312412751250934 --- Val Loss: 2.3067968241931345 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: 2.3004517914022307 --- Val Loss: 2.3067341912332244 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: 2.300452697720671 --- Val Loss: 2.306769617882704 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3025488293323493 --- Val Loss: 2.3025707703810387 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.3022540839835774 --- Val Loss: 2.3024880607987677 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/1000 --- Train Loss: 2.3020483020324023 --- Val Loss: 2.3024537276522197 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.301905981198818 --- Val Loss: 2.302461235855336 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.3018078475991937 --- Val Loss: 2.302484606422539 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.301740156343881 --- Val Loss: 2.3025228897614336 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: 2.301693235421016 --- Val Loss: 2.302556307205033 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.3016611330632886 --- Val Loss: 2.3025886392972064 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.3016387765781374 --- Val Loss: 2.30262468926862 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: 2.3016236933319054 --- Val Loss: 2.3026568935375975 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: 2.3016133310650506 --- Val Loss: 2.3026809064775473 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.301605967236946 --- Val Loss: 2.302706836468549 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.301601074793869 --- Val Loss: 2.3027289021190316 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.301597592325452 --- Val Loss: 2.302744775178254 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.301595254259934 --- Val Loss: 2.302760138148654 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.301593572819944 --- Val Loss: 2.302773482544377 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.3015925083929365 --- Val Loss: 2.3027845279713515 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.3015917641562806 --- Val Loss: 2.3027970415994496 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: 2.3015911423592836 --- Val Loss: 2.3028096794138846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.3015907652810426 --- Val Loss: 2.30281627942021 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.3015905358918687 --- Val Loss: 2.302815269018942 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.3015902956726757 --- Val Loss: 2.30281948982196 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.3015901599404036 --- Val Loss: 2.302826538133113 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.3015900856224336 --- Val Loss: 2.302825542524094 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.3015899840509926 --- Val Loss: 2.3028315175208567 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: 2.3015899126928185 --- Val Loss: 2.3028359297209806 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.3015898400832717 --- Val Loss: 2.3028386972395394 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: 2.301589766334083 --- Val Loss: 2.302838470553461 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.3015897316591025 --- Val Loss: 2.3028351346007505 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.3015896789452928 --- Val Loss: 2.302840719442955 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.301589611044057 --- Val Loss: 2.3028411044948984 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: 2.301589558062079 --- Val Loss: 2.3028449513575437 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.301589506305331 --- Val Loss: 2.302844510776906 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.3015894486593442 --- Val Loss: 2.3028442385486003 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.3015894211789507 --- Val Loss: 2.3028459998868764 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.301589350124468 --- Val Loss: 2.3028451042386195 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.301589318990239 --- Val Loss: 2.302845158983092 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3015892845425534 --- Val Loss: 2.3028420356921115 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.3015892075006996 --- Val Loss: 2.302840820177328 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.301589171754005 --- Val Loss: 2.3028439590435283 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: 2.301589123242382 --- Val Loss: 2.302844290426391 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3015890377727635 --- Val Loss: 2.3028454324290983 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.301588984486152 --- Val Loss: 2.3028444095000524 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3015889330652155 --- Val Loss: 2.302841519115885 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.301588884327005 --- Val Loss: 2.3028415253652 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.301588804875867 --- Val Loss: 2.3028374763457484 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3015887782793465 --- Val Loss: 2.3028337351941466 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.3015886496129094 --- Val Loss: 2.302841113492821 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.301588562535961 --- Val Loss: 2.302840473365164 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.3015885149344117 --- Val Loss: 2.302845292451846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3015884912076543 --- Val Loss: 2.3028445972174088 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.3015883897372844 --- Val Loss: 2.30284326098563 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.301588300994784 --- Val Loss: 2.302844853985793 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3015882022696337 --- Val Loss: 2.302843083356155 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.3015881180290076 --- Val Loss: 2.302841470969291 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.3015880111421834 --- Val Loss: 2.302838858266223 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3015879830563915 --- Val Loss: 2.3028365507908832 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.301587856534032 --- Val Loss: 2.302838335876618 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.3015877103028033 --- Val Loss: 2.302837518987049 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.301587688897452 --- Val Loss: 2.3028385112606014 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.3015875203402403 --- Val Loss: 2.3028394642349657 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.301587469688207 --- Val Loss: 2.302842621019116 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.3015872984909684 --- Val Loss: 2.302844887414907 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.3015870900090705 --- Val Loss: 2.3028418647182027 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.301587148687949 --- Val Loss: 2.3028455957747562 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.3015869351406915 --- Val Loss: 2.302846071357659 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.3015868059514863 --- Val Loss: 2.302847175433376 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.301586668080323 --- Val Loss: 2.302844995158902 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.301586446194508 --- Val Loss: 2.3028468228488896 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.3015863519659887 --- Val Loss: 2.3028463788152123 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.301586280108146 --- Val Loss: 2.3028455195092663 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.3015860436504525 --- Val Loss: 2.3028398552142693 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.301585866438846 --- Val Loss: 2.302836406706058 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.301585607053221 --- Val Loss: 2.3028387361994795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.301585526614051 --- Val Loss: 2.302841419153637 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.301585190588846 --- Val Loss: 2.302837974832124 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.3015849893276785 --- Val Loss: 2.302841482662567 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.3015848885349866 --- Val Loss: 2.302838943576327 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.30158455845305 --- Val Loss: 2.3028403139827796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.301584407776769 --- Val Loss: 2.302839138692786 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: 2.301583942789216 --- Val Loss: 2.3028399100423154 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3015838504122463 --- Val Loss: 2.302840408391626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.3015835366726622 --- Val Loss: 2.302838009498425 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.3015832738825246 --- Val Loss: 2.302838326859572 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.3015829080195274 --- Val Loss: 2.3028390734011395 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.3015825132536607 --- Val Loss: 2.302837179891636 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3015821505275427 --- Val Loss: 2.302834035780151 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.301581907568547 --- Val Loss: 2.302833942016798 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3015811895717952 --- Val Loss: 2.3028385285963795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.301580903100974 --- Val Loss: 2.3028400526290755 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.301580519834062 --- Val Loss: 2.302839427022551 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.3015799480356263 --- Val Loss: 2.3028426886038567 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: 2.3015791870056947 --- Val Loss: 2.302839607335622 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.3015787286518927 --- Val Loss: 2.3028378407546546 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.3015783653662365 --- Val Loss: 2.302836092500606 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.301577645889938 --- Val Loss: 2.3028302431465795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.3015773739689114 --- Val Loss: 2.3028296477782124 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.3015763801492404 --- Val Loss: 2.302831860418695 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.301575338046864 --- Val Loss: 2.3028307448826997 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: 2.3015747529158217 --- Val Loss: 2.3028321872733484 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "\n",
      "Best Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Best Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomSearch(NeuralNetwork, param_grid, n_iter=100)\n",
    "best_params, best_accuracy = random_search.search(X, y)\n",
    "print(f\"\\nBest Params: {best_params}, Best Accuracy: {best_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
