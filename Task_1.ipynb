{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380f9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, l1=0.0, l2=0.0):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # Initialise (weights) velocity terms for momentum optimization\n",
    "        self.velocity_biases = np.zeros_like(self.biases) # Initialise (biases) velocity terms for momentum optimization\n",
    "        self.l1 = l1 # L1 regularization coefficient (default 0.0).\n",
    "        self.l2 = l2 # L2 regularization coefficient (default 0.0).\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        '''\n",
    "        # Regularization terms\n",
    "        l1_reg = self.l1 * np.sign(self.weights)\n",
    "        l2_reg = self.l2 * self.weights\n",
    "\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient) + l1_reg + l2_reg\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if optimizer == 'GD':\n",
    "            # Update weights and biases\n",
    "            self.weights += learning_rate * weights_gradient\n",
    "            self.biases += learning_rate * biases_gradient\n",
    "        elif optimizer == 'Momentum':\n",
    "            # Momentum update for weights and biases\n",
    "            self.velocity_weights = momentum * self.velocity_weights + learning_rate * weights_gradient\n",
    "            self.velocity_biases = momentum * self.velocity_biases + learning_rate * biases_gradient\n",
    "\n",
    "            # Update weights and biases using velocity\n",
    "            self.weights += self.velocity_weights\n",
    "            self.biases += self.velocity_biases\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for printing/plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for printing/plotting\n",
    "        self.accuracy_history = [] #placeholder to store the (train) accuracy for printing/plotting\n",
    "        self.val_accuracy_history = [] #placeholder to store the accuracy calculated on the validation set for printing/plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Computes the accuracy of predictions by comparing them with the true labels. \n",
    "        Accuracy is computed as the proportion of correct predictions to the total number of predictions.\n",
    "        '''\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate, optimizer, momentum)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, optimizer='GD', momentum=0.9, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate, optimizer, momentum) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_predictions = self.predict(X_train)\n",
    "            train_accuracy = self.compute_accuracy(train_predictions, np.argmax(y_train, axis=1))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_predictions = self.predict(X_val)\n",
    "            val_accuracy = self.compute_accuracy(val_predictions, np.argmax(y_val, axis=1))\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.plot(self.accuracy_history, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracy_history, label='Val Accuracy')\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 2.302356789917726 --- Val Loss: 2.3030626335665714 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/1000 --- Train Loss: 2.3015546697331475 --- Val Loss: 2.30481692923134 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: 2.301546929308975 --- Val Loss: 2.305424323508293 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 30/1000 --- Train Loss: 2.301547701748579 --- Val Loss: 2.305904673126399 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/1000 --- Train Loss: 2.3015448387861417 --- Val Loss: 2.3056093337588055 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.301543182229616 --- Val Loss: 2.305386636037665 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: 2.3015400500602885 --- Val Loss: 2.305506964970304 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.301538943521579 --- Val Loss: 2.305403722661314 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: 2.301537585140498 --- Val Loss: 2.3056023931103287 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: 2.301536702818009 --- Val Loss: 2.3054550596727497 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: 2.3015300454707375 --- Val Loss: 2.3049256533644016 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: 2.301500815838859 --- Val Loss: 2.305293016883942 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 120/1000 --- Train Loss: 2.301421575468958 --- Val Loss: 2.3054974024198724 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.300949523726798 --- Val Loss: 2.30534957909564 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.280722795618599 --- Val Loss: 2.2849282117767706 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: 1.7324776785074736 --- Val Loss: 1.722695006580204 --- Train Acc: 0.22 --- Val Acc: 0.17\n",
      "Epoch 160/1000 --- Train Loss: 1.4034969542349074 --- Val Loss: 1.3528349579525638 --- Train Acc: 0.39 --- Val Acc: 0.45\n",
      "Epoch 170/1000 --- Train Loss: 0.9899853801118286 --- Val Loss: 0.9272678464693561 --- Train Acc: 0.63 --- Val Acc: 0.66\n",
      "Epoch 180/1000 --- Train Loss: 0.553095782550537 --- Val Loss: 0.5042225102117264 --- Train Acc: 0.84 --- Val Acc: 0.86\n",
      "Epoch 190/1000 --- Train Loss: 0.22172539375370087 --- Val Loss: 0.18661181479609912 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 200/1000 --- Train Loss: 0.09969235728766364 --- Val Loss: 0.08041118952129303 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 210/1000 --- Train Loss: 0.06783669950894512 --- Val Loss: 0.045012039149903364 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/1000 --- Train Loss: 0.03871483257211618 --- Val Loss: 0.02822491526566302 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 230/1000 --- Train Loss: 0.026162935593576145 --- Val Loss: 0.01792051409748029 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.0242229818939071 --- Val Loss: 0.01251708405900228 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.019439589168431477 --- Val Loss: 0.01117575869537181 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.01219480616676006 --- Val Loss: 0.006884608368049146 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.015328853038733428 --- Val Loss: 0.0054689126116652 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.014122756433136147 --- Val Loss: 0.004574034532210348 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.008425846235401544 --- Val Loss: 0.0037746497542826734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.0073959206236514545 --- Val Loss: 0.0030728385877774596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.007148098688519604 --- Val Loss: 0.0027991784951494074 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.005041958753974565 --- Val Loss: 0.002223121051152566 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.00427818696316838 --- Val Loss: 0.002362246874743891 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.005700404077166819 --- Val Loss: 0.0019793487116419733 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.004117177801307024 --- Val Loss: 0.001875360851975013 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.003404584846800598 --- Val Loss: 0.0015054740243684794 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.00750153412091846 --- Val Loss: 0.0013681528863190265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.004900327180073538 --- Val Loss: 0.0011217439874369693 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.002478677574025675 --- Val Loss: 0.0011604415584896607 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.005181552439081183 --- Val Loss: 0.001112844646963107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.0030060344231055067 --- Val Loss: 0.0009976813638938025 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.003257692725594857 --- Val Loss: 0.0009015471290758989 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.002989100781547583 --- Val Loss: 0.0008064031307479407 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.004705266474253968 --- Val Loss: 0.0007353573855329655 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.0037917525446124903 --- Val Loss: 0.0008154231319784656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.0018817097202069246 --- Val Loss: 0.0006896236236265254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.003273891234296671 --- Val Loss: 0.0006231060851387359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.002841324631376345 --- Val Loss: 0.0005826234265036463 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.005529645241276387 --- Val Loss: 0.0006025376748158585 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.004834511829244008 --- Val Loss: 0.0005841302590456079 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.0022551512676501147 --- Val Loss: 0.0005929074988992282 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.0014258305202364972 --- Val Loss: 0.0005568942918209986 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.0033415880469512297 --- Val Loss: 0.0005428013721999217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.0016716506092732447 --- Val Loss: 0.0005140428000073823 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.0022087375274295155 --- Val Loss: 0.0005806397741862596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.0023205312700000213 --- Val Loss: 0.00044902562972978663 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.0013265668251402868 --- Val Loss: 0.0004182700302532922 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.0029719674853805884 --- Val Loss: 0.00043381965071879265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.0018615445910901175 --- Val Loss: 0.00038591409212441476 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.002673591095321845 --- Val Loss: 0.00038047702107641795 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.0015927166734335384 --- Val Loss: 0.0003390874075411512 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.001403086630539543 --- Val Loss: 0.0003395927081047969 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.0016298534285919969 --- Val Loss: 0.0003043140790131179 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.003188343838256794 --- Val Loss: 0.0003037961554159207 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.001178879428725212 --- Val Loss: 0.0002998157748673638 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.0013581980257668803 --- Val Loss: 0.0003163654897775714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.0032236103888950932 --- Val Loss: 0.0002784704854040845 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.0009284679583418629 --- Val Loss: 0.0003285367483168252 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.001204313131240686 --- Val Loss: 0.00028002351251090753 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.002579338606128365 --- Val Loss: 0.00026544339880157984 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.0011147655446382071 --- Val Loss: 0.00026953880353518803 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.0013907567637035344 --- Val Loss: 0.00025357650085694217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.0017643192046140138 --- Val Loss: 0.0002833933806050634 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.0024717046315138494 --- Val Loss: 0.00023481440871528158 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.0009597840813464966 --- Val Loss: 0.00023771964402890644 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.0012800187834710287 --- Val Loss: 0.00022990928746920869 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.0006377793507080582 --- Val Loss: 0.00023173974704575266 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.0006142630087686835 --- Val Loss: 0.00019827361253252528 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.006025131591577447 --- Val Loss: 0.00018225709909021223 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.0019101733602403523 --- Val Loss: 0.00017738431519360998 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.0005648173150589995 --- Val Loss: 0.0002014665609429143 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.00595106788479372 --- Val Loss: 0.014795724022187469 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.0008271741996441107 --- Val Loss: 0.0002189994529216031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.0027625114994478286 --- Val Loss: 0.00020335966110294188 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.002559200368868788 --- Val Loss: 0.00019554454017426436 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.0009409726239936969 --- Val Loss: 0.00017363034437281849 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.0010633287691086287 --- Val Loss: 0.000162436666650023 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.000742021167402009 --- Val Loss: 0.00016039636398033078 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.0009433387329315694 --- Val Loss: 0.00014595128575259982 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.0005527412346128654 --- Val Loss: 0.00017904441613359568 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.0006894446656948526 --- Val Loss: 0.00016361388608799403 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.0006268102119439098 --- Val Loss: 0.00013366206697259205 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.0009014159312917142 --- Val Loss: 0.00012425043769076892 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.000971177890405463 --- Val Loss: 0.00015154412213958562 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.0008286063959864463 --- Val Loss: 0.00012741082209208763 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.0007453002922626374 --- Val Loss: 0.00011777075926920235 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.0006321569835548771 --- Val Loss: 0.00011387439406768384 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.0006641458102533699 --- Val Loss: 0.00010477575096066011 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.0004588805706631513 --- Val Loss: 0.00011414369091215687 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32)) \n",
    "network.add_layer(ReLU())\n",
    "#network.add_layer(Layer(64, 32, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=1000, learning_rate=0.01, optimizer='Momentum', momentum=0.9, batch_size=64)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2500 --- Train Loss: 0.6924793029070984 --- Val Loss: 0.6924295946989082 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 10/2500 --- Train Loss: 0.6860562181310774 --- Val Loss: 0.6838158223168646 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 20/2500 --- Train Loss: 0.6807321392337471 --- Val Loss: 0.6755345001695018 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 30/2500 --- Train Loss: 0.6792673407244719 --- Val Loss: 0.6719831010574683 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 40/2500 --- Train Loss: 0.6792277596356675 --- Val Loss: 0.6709468648120274 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 50/2500 --- Train Loss: 0.6792827724641768 --- Val Loss: 0.6707524645234354 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 60/2500 --- Train Loss: 0.6792549184692865 --- Val Loss: 0.670826509705079 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 70/2500 --- Train Loss: 0.6792170181769884 --- Val Loss: 0.6709760350710035 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 80/2500 --- Train Loss: 0.6791978086903345 --- Val Loss: 0.6711059172251427 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 90/2500 --- Train Loss: 0.6791913812710705 --- Val Loss: 0.6711808300397636 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 100/2500 --- Train Loss: 0.6791897564759164 --- Val Loss: 0.6712071749319828 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 110/2500 --- Train Loss: 0.6791895861558322 --- Val Loss: 0.6712061084106702 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 120/2500 --- Train Loss: 0.6791898329353667 --- Val Loss: 0.6711956977897502 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 130/2500 --- Train Loss: 0.6791900036541096 --- Val Loss: 0.6711856785307206 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 140/2500 --- Train Loss: 0.6791898834208515 --- Val Loss: 0.6711792251438415 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 150/2500 --- Train Loss: 0.6791894473251981 --- Val Loss: 0.6711760756591999 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 160/2500 --- Train Loss: 0.6791887510839182 --- Val Loss: 0.6711748693187548 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 170/2500 --- Train Loss: 0.6791878452023807 --- Val Loss: 0.6711743445944282 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 180/2500 --- Train Loss: 0.6791867269456177 --- Val Loss: 0.6711736996736403 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 190/2500 --- Train Loss: 0.6791853867285834 --- Val Loss: 0.6711725773877371 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 200/2500 --- Train Loss: 0.6791837198083952 --- Val Loss: 0.6711708192702587 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 210/2500 --- Train Loss: 0.679181619056553 --- Val Loss: 0.6711683704103107 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 220/2500 --- Train Loss: 0.67917892631927 --- Val Loss: 0.671165155545312 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 230/2500 --- Train Loss: 0.6791754272684392 --- Val Loss: 0.6711609409661933 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 240/2500 --- Train Loss: 0.6791707254657421 --- Val Loss: 0.6711553209802977 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 250/2500 --- Train Loss: 0.6791641112203937 --- Val Loss: 0.671147536572631 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 260/2500 --- Train Loss: 0.6791547071418835 --- Val Loss: 0.6711365845909485 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 270/2500 --- Train Loss: 0.6791413565507924 --- Val Loss: 0.6711210962409224 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 280/2500 --- Train Loss: 0.6791217718501182 --- Val Loss: 0.6710980359709219 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 290/2500 --- Train Loss: 0.6790915030976775 --- Val Loss: 0.6710625540901692 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 300/2500 --- Train Loss: 0.6790427400704303 --- Val Loss: 0.6710064262537954 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 310/2500 --- Train Loss: 0.6789608217683604 --- Val Loss: 0.6709131129539829 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 320/2500 --- Train Loss: 0.678817518191593 --- Val Loss: 0.6707498210155484 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 330/2500 --- Train Loss: 0.678549121141344 --- Val Loss: 0.6704428789633944 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 340/2500 --- Train Loss: 0.6779927656447874 --- Val Loss: 0.6698137888595389 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 350/2500 --- Train Loss: 0.6766980222463601 --- Val Loss: 0.6683568238287784 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 360/2500 --- Train Loss: 0.6731887626373375 --- Val Loss: 0.6644025917812443 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 370/2500 --- Train Loss: 0.6609499835619199 --- Val Loss: 0.65068326307468 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 380/2500 --- Train Loss: 0.5992455426256829 --- Val Loss: 0.582028134211053 --- Train Acc: 0.72 --- Val Acc: 0.73\n",
      "Epoch 390/2500 --- Train Loss: 0.37993856571672474 --- Val Loss: 0.35818268003054626 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 400/2500 --- Train Loss: 0.22254852011060572 --- Val Loss: 0.2128340465591823 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 410/2500 --- Train Loss: 0.12744264206722683 --- Val Loss: 0.13387353661273116 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 420/2500 --- Train Loss: 0.09447319035213395 --- Val Loss: 0.11454442449465337 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 430/2500 --- Train Loss: 0.08363752962637633 --- Val Loss: 0.11110790766944066 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 440/2500 --- Train Loss: 0.07901379452965722 --- Val Loss: 0.10924836032035001 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 450/2500 --- Train Loss: 0.07608250440784366 --- Val Loss: 0.10779691276694568 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 460/2500 --- Train Loss: 0.07404836586167247 --- Val Loss: 0.10680749730569794 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 470/2500 --- Train Loss: 0.07246714726638491 --- Val Loss: 0.10612364066991824 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 480/2500 --- Train Loss: 0.07108810845218151 --- Val Loss: 0.10535903356924761 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 490/2500 --- Train Loss: 0.0698792096310493 --- Val Loss: 0.10459723401456175 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 500/2500 --- Train Loss: 0.06881806164863198 --- Val Loss: 0.10391561068529682 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 510/2500 --- Train Loss: 0.06783782240966467 --- Val Loss: 0.10330837498642412 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 520/2500 --- Train Loss: 0.06691131397943524 --- Val Loss: 0.10278021847922965 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 530/2500 --- Train Loss: 0.06604459946338737 --- Val Loss: 0.10231534205751167 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 540/2500 --- Train Loss: 0.06523660385365551 --- Val Loss: 0.10191745495785963 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 550/2500 --- Train Loss: 0.06449086639273147 --- Val Loss: 0.101579736603808 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 560/2500 --- Train Loss: 0.06377828690334612 --- Val Loss: 0.10129488008264963 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 570/2500 --- Train Loss: 0.06297834698832944 --- Val Loss: 0.10107092881120817 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 580/2500 --- Train Loss: 0.062292676411011765 --- Val Loss: 0.10086074587181026 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 590/2500 --- Train Loss: 0.061706952852767526 --- Val Loss: 0.1006067772842592 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 600/2500 --- Train Loss: 0.06112092247454727 --- Val Loss: 0.10027933931608934 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 610/2500 --- Train Loss: 0.06055678100047988 --- Val Loss: 0.09990770722319633 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 620/2500 --- Train Loss: 0.06001720302237866 --- Val Loss: 0.09952082138725325 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 630/2500 --- Train Loss: 0.0594990845594849 --- Val Loss: 0.09914075238158872 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 640/2500 --- Train Loss: 0.05899539438864389 --- Val Loss: 0.09875213924296052 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 650/2500 --- Train Loss: 0.0585079904369871 --- Val Loss: 0.09836030033837861 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 660/2500 --- Train Loss: 0.058036342702891826 --- Val Loss: 0.09797139854756123 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 670/2500 --- Train Loss: 0.05757944900957001 --- Val Loss: 0.09757351459105652 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 680/2500 --- Train Loss: 0.05713116896228223 --- Val Loss: 0.09717658799200832 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 690/2500 --- Train Loss: 0.05668957492746009 --- Val Loss: 0.09677702509769134 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 700/2500 --- Train Loss: 0.05625477984205946 --- Val Loss: 0.09637200330966196 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 710/2500 --- Train Loss: 0.05582908084800359 --- Val Loss: 0.09597451022554582 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 720/2500 --- Train Loss: 0.055416723322170625 --- Val Loss: 0.09558981828234324 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 730/2500 --- Train Loss: 0.055012440927152795 --- Val Loss: 0.09521584598961007 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 740/2500 --- Train Loss: 0.05460547180207558 --- Val Loss: 0.0948443754103224 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 750/2500 --- Train Loss: 0.054193473876844284 --- Val Loss: 0.09446510039083846 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 760/2500 --- Train Loss: 0.05363955805864673 --- Val Loss: 0.09403782524149075 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 770/2500 --- Train Loss: 0.052997363960495286 --- Val Loss: 0.09356999855946736 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 780/2500 --- Train Loss: 0.05232883283492529 --- Val Loss: 0.09309308936595552 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 790/2500 --- Train Loss: 0.05164021446409263 --- Val Loss: 0.09261177607988688 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 800/2500 --- Train Loss: 0.050959410229279696 --- Val Loss: 0.09214696471027466 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 810/2500 --- Train Loss: 0.05030838756950194 --- Val Loss: 0.0916992140950258 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 820/2500 --- Train Loss: 0.049715058300084464 --- Val Loss: 0.09133684320460975 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 830/2500 --- Train Loss: 0.04915303727846 --- Val Loss: 0.091061914475941 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 840/2500 --- Train Loss: 0.048607296976333605 --- Val Loss: 0.09086887708441993 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 850/2500 --- Train Loss: 0.04805524389465677 --- Val Loss: 0.09077735332695966 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 860/2500 --- Train Loss: 0.04748943916300954 --- Val Loss: 0.09079139477375668 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 870/2500 --- Train Loss: 0.04700633783647362 --- Val Loss: 0.09094274170901857 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 880/2500 --- Train Loss: 0.0466026080461611 --- Val Loss: 0.09135698348497844 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 890/2500 --- Train Loss: 0.046157966956987075 --- Val Loss: 0.09183184501432731 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 900/2500 --- Train Loss: 0.04569850548652556 --- Val Loss: 0.09240226372859546 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 910/2500 --- Train Loss: 0.045215292437860134 --- Val Loss: 0.09305760947763839 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 920/2500 --- Train Loss: 0.04471110784558375 --- Val Loss: 0.0937797160058221 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 930/2500 --- Train Loss: 0.0441934923082055 --- Val Loss: 0.09457824494978805 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 940/2500 --- Train Loss: 0.04369658608598823 --- Val Loss: 0.09543784233026605 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 950/2500 --- Train Loss: 0.043211015794558806 --- Val Loss: 0.09640004224937167 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 960/2500 --- Train Loss: 0.04275706864769297 --- Val Loss: 0.09744705163920282 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 970/2500 --- Train Loss: 0.04232766734780746 --- Val Loss: 0.0986130940205192 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 980/2500 --- Train Loss: 0.04193916994623412 --- Val Loss: 0.09988822037839543 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 990/2500 --- Train Loss: 0.04159524178353691 --- Val Loss: 0.10127828114425717 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 1000/2500 --- Train Loss: 0.04127871526865035 --- Val Loss: 0.10275796141852425 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 1010/2500 --- Train Loss: 0.040994689769965564 --- Val Loss: 0.1043233205382862 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1020/2500 --- Train Loss: 0.04075617836901247 --- Val Loss: 0.10595124334009738 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1030/2500 --- Train Loss: 0.040556573987425903 --- Val Loss: 0.1076373232656664 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1040/2500 --- Train Loss: 0.04038247707376802 --- Val Loss: 0.10940128961069776 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1050/2500 --- Train Loss: 0.04025887513235468 --- Val Loss: 0.11118587961753766 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1060/2500 --- Train Loss: 0.040162475105081724 --- Val Loss: 0.11298954074851487 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1070/2500 --- Train Loss: 0.04009723177775737 --- Val Loss: 0.11480773769343759 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1080/2500 --- Train Loss: 0.040057777022080104 --- Val Loss: 0.11660596113625739 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1090/2500 --- Train Loss: 0.040039077111845074 --- Val Loss: 0.11838245922991307 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1100/2500 --- Train Loss: 0.04004635167547525 --- Val Loss: 0.1201332409686117 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1110/2500 --- Train Loss: 0.040075381542121744 --- Val Loss: 0.12184378608097322 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1120/2500 --- Train Loss: 0.040118024989477544 --- Val Loss: 0.12354725580436163 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1130/2500 --- Train Loss: 0.04017979395768296 --- Val Loss: 0.12521848833476726 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1140/2500 --- Train Loss: 0.04025977619401267 --- Val Loss: 0.12686278770009743 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1150/2500 --- Train Loss: 0.04035308856309104 --- Val Loss: 0.12847130493262468 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1160/2500 --- Train Loss: 0.040452985146916025 --- Val Loss: 0.13002577728005543 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1170/2500 --- Train Loss: 0.040560669900468596 --- Val Loss: 0.1315467249873203 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1180/2500 --- Train Loss: 0.04067503213345099 --- Val Loss: 0.1330242107760386 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1190/2500 --- Train Loss: 0.0407916992531997 --- Val Loss: 0.13445741119226312 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1200/2500 --- Train Loss: 0.0409209111479254 --- Val Loss: 0.13586473256763135 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1210/2500 --- Train Loss: 0.04105431471760021 --- Val Loss: 0.13723584184884322 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1220/2500 --- Train Loss: 0.041184438735090034 --- Val Loss: 0.1385777908080538 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1230/2500 --- Train Loss: 0.04132296595227166 --- Val Loss: 0.13988668218658656 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1240/2500 --- Train Loss: 0.04146377922438865 --- Val Loss: 0.14116832667694476 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1250/2500 --- Train Loss: 0.04160659290396998 --- Val Loss: 0.14241789262650237 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1260/2500 --- Train Loss: 0.041751316807609726 --- Val Loss: 0.1436437653542351 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1270/2500 --- Train Loss: 0.04189472655718256 --- Val Loss: 0.14484780458306443 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1280/2500 --- Train Loss: 0.042039430809805836 --- Val Loss: 0.14601153156187502 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1290/2500 --- Train Loss: 0.042182516725093835 --- Val Loss: 0.14714282552973143 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1300/2500 --- Train Loss: 0.04232556355494501 --- Val Loss: 0.14825298329066577 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1310/2500 --- Train Loss: 0.04246563599089903 --- Val Loss: 0.14934529285967948 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1320/2500 --- Train Loss: 0.042603578077480254 --- Val Loss: 0.15039894924591982 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1330/2500 --- Train Loss: 0.042740815632185986 --- Val Loss: 0.15142494813213844 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1340/2500 --- Train Loss: 0.0428757875462658 --- Val Loss: 0.15241306177937647 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1350/2500 --- Train Loss: 0.04300778300875094 --- Val Loss: 0.15337221997086314 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1360/2500 --- Train Loss: 0.04313928683772398 --- Val Loss: 0.15430711158369564 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1370/2500 --- Train Loss: 0.04326899586467752 --- Val Loss: 0.1552234381051787 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1380/2500 --- Train Loss: 0.04339908142306805 --- Val Loss: 0.15611204617711266 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1390/2500 --- Train Loss: 0.04352997559785013 --- Val Loss: 0.1569951206163043 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1400/2500 --- Train Loss: 0.04365979803990749 --- Val Loss: 0.15785305938680763 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1410/2500 --- Train Loss: 0.04378818282212572 --- Val Loss: 0.15870043885464763 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1420/2500 --- Train Loss: 0.04391500025974994 --- Val Loss: 0.1595232807494011 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1430/2500 --- Train Loss: 0.04404187786637682 --- Val Loss: 0.160339648573067 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1440/2500 --- Train Loss: 0.04416651521965812 --- Val Loss: 0.1611352602922109 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1450/2500 --- Train Loss: 0.0442905071059422 --- Val Loss: 0.16192193708384828 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1460/2500 --- Train Loss: 0.04441232860117571 --- Val Loss: 0.16268628585881922 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1470/2500 --- Train Loss: 0.044533300270253734 --- Val Loss: 0.16343791992957857 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1480/2500 --- Train Loss: 0.044652040659078296 --- Val Loss: 0.16416827784374477 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1490/2500 --- Train Loss: 0.04477021059054362 --- Val Loss: 0.16489704039149858 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1500/2500 --- Train Loss: 0.04488574045698095 --- Val Loss: 0.1656008493267652 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1510/2500 --- Train Loss: 0.04500063038121416 --- Val Loss: 0.16629885782517964 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1520/2500 --- Train Loss: 0.04511479206095981 --- Val Loss: 0.1669838403833437 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1530/2500 --- Train Loss: 0.045226926245534226 --- Val Loss: 0.16765388846145698 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1540/2500 --- Train Loss: 0.045338656812074676 --- Val Loss: 0.168317871978973 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1550/2500 --- Train Loss: 0.045448284931229654 --- Val Loss: 0.16896496232855943 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1560/2500 --- Train Loss: 0.045557097930974934 --- Val Loss: 0.16960544280480847 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1570/2500 --- Train Loss: 0.04566421522310771 --- Val Loss: 0.17023058974146066 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1580/2500 --- Train Loss: 0.04577131840450817 --- Val Loss: 0.17085034369511157 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1590/2500 --- Train Loss: 0.04587607535441428 --- Val Loss: 0.1714585189031559 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1600/2500 --- Train Loss: 0.0459803737901826 --- Val Loss: 0.17205826505718447 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1610/2500 --- Train Loss: 0.046083050102844865 --- Val Loss: 0.17264768769516078 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1620/2500 --- Train Loss: 0.04618447161754105 --- Val Loss: 0.17322368538492033 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1630/2500 --- Train Loss: 0.04628431624563278 --- Val Loss: 0.17379014574391122 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1640/2500 --- Train Loss: 0.04638376358634472 --- Val Loss: 0.174351776436855 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1650/2500 --- Train Loss: 0.04648176638078301 --- Val Loss: 0.17490296632385527 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1660/2500 --- Train Loss: 0.04657744486687328 --- Val Loss: 0.1754369942111482 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1670/2500 --- Train Loss: 0.04667214717480953 --- Val Loss: 0.17596761071499517 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1680/2500 --- Train Loss: 0.046766068858790276 --- Val Loss: 0.17648802327359472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1690/2500 --- Train Loss: 0.04685955996857369 --- Val Loss: 0.17700552572454217 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1700/2500 --- Train Loss: 0.046951941679563756 --- Val Loss: 0.17751533361830352 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1710/2500 --- Train Loss: 0.04704278455458912 --- Val Loss: 0.17801403524001422 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1720/2500 --- Train Loss: 0.04713395737842025 --- Val Loss: 0.17851375787194843 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1730/2500 --- Train Loss: 0.04722385108934176 --- Val Loss: 0.17900492283250394 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1740/2500 --- Train Loss: 0.04731305669181227 --- Val Loss: 0.17949017302506864 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1750/2500 --- Train Loss: 0.047400881561361866 --- Val Loss: 0.17996742449151432 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1760/2500 --- Train Loss: 0.04748825455731468 --- Val Loss: 0.1804395660786955 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1770/2500 --- Train Loss: 0.04757503117285454 --- Val Loss: 0.18090754950959712 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1780/2500 --- Train Loss: 0.047660388100469 --- Val Loss: 0.18136760377480293 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1790/2500 --- Train Loss: 0.04774551452338416 --- Val Loss: 0.18182460187713745 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1800/2500 --- Train Loss: 0.04782955300602349 --- Val Loss: 0.18227403329240494 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1810/2500 --- Train Loss: 0.04791267019447825 --- Val Loss: 0.18271771952926924 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1820/2500 --- Train Loss: 0.04799513196160239 --- Val Loss: 0.18315602805003972 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1830/2500 --- Train Loss: 0.04807651626670899 --- Val Loss: 0.18358832649981502 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1840/2500 --- Train Loss: 0.04815725104429281 --- Val Loss: 0.1840153043385026 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1850/2500 --- Train Loss: 0.04823772691422009 --- Val Loss: 0.18444095514811285 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1860/2500 --- Train Loss: 0.048317130323348764 --- Val Loss: 0.18485921383195544 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1870/2500 --- Train Loss: 0.04839585449572762 --- Val Loss: 0.18527397486435507 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1880/2500 --- Train Loss: 0.04847297115246804 --- Val Loss: 0.18567852901885964 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1890/2500 --- Train Loss: 0.04855012752469913 --- Val Loss: 0.18608314162064601 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1900/2500 --- Train Loss: 0.04862646760027315 --- Val Loss: 0.18648172765214258 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1910/2500 --- Train Loss: 0.048701808605584776 --- Val Loss: 0.186874285072957 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1920/2500 --- Train Loss: 0.048776795259115005 --- Val Loss: 0.18726490836774082 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1930/2500 --- Train Loss: 0.04885069505346972 --- Val Loss: 0.1876490555742903 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1940/2500 --- Train Loss: 0.04892445528414401 --- Val Loss: 0.18803113650914588 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1950/2500 --- Train Loss: 0.04899739629872687 --- Val Loss: 0.18840863576850145 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1960/2500 --- Train Loss: 0.04906919130604742 --- Val Loss: 0.18877956870318283 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1970/2500 --- Train Loss: 0.04914107915767654 --- Val Loss: 0.18915031744197547 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1980/2500 --- Train Loss: 0.04921189735438775 --- Val Loss: 0.18951504785611414 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 1990/2500 --- Train Loss: 0.04928229362910934 --- Val Loss: 0.18987589852317877 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2000/2500 --- Train Loss: 0.049352114024629655 --- Val Loss: 0.19023427357717876 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2010/2500 --- Train Loss: 0.0494206085196289 --- Val Loss: 0.19058462973123427 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2020/2500 --- Train Loss: 0.0494897905024447 --- Val Loss: 0.19093885867437307 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2030/2500 --- Train Loss: 0.049557617410629654 --- Val Loss: 0.19128413454346546 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2040/2500 --- Train Loss: 0.04962471022945205 --- Val Loss: 0.19162592531311212 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2050/2500 --- Train Loss: 0.049691713160171916 --- Val Loss: 0.1919672036827845 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2060/2500 --- Train Loss: 0.04975764094190231 --- Val Loss: 0.19230131202215564 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2070/2500 --- Train Loss: 0.049823731456189034 --- Val Loss: 0.19263696222094323 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2080/2500 --- Train Loss: 0.04988871566363439 --- Val Loss: 0.19296542394323132 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2090/2500 --- Train Loss: 0.04995332720520785 --- Val Loss: 0.193292157354709 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2100/2500 --- Train Loss: 0.0500171655312416 --- Val Loss: 0.1936137740330032 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2110/2500 --- Train Loss: 0.05008107396458284 --- Val Loss: 0.19393614659222672 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2120/2500 --- Train Loss: 0.05014415192907212 --- Val Loss: 0.19425361431171048 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2130/2500 --- Train Loss: 0.05020660540571682 --- Val Loss: 0.19456771899988554 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2140/2500 --- Train Loss: 0.050269327628101705 --- Val Loss: 0.19488267189894642 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2150/2500 --- Train Loss: 0.050330552292639455 --- Val Loss: 0.19518914189516073 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2160/2500 --- Train Loss: 0.05039174994186828 --- Val Loss: 0.19549555217273198 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2170/2500 --- Train Loss: 0.05045204931581054 --- Val Loss: 0.19579653549820833 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2180/2500 --- Train Loss: 0.05051253233489119 --- Val Loss: 0.1960985047471052 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2190/2500 --- Train Loss: 0.05057235777345013 --- Val Loss: 0.19639674873423227 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2200/2500 --- Train Loss: 0.050631576420880585 --- Val Loss: 0.19669142607407913 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2210/2500 --- Train Loss: 0.05069058456827316 --- Val Loss: 0.19698492100810724 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2220/2500 --- Train Loss: 0.05074889711092526 --- Val Loss: 0.19727426581549867 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2230/2500 --- Train Loss: 0.05080715365129209 --- Val Loss: 0.19756372176537457 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2240/2500 --- Train Loss: 0.05086495105183297 --- Val Loss: 0.19784953436915279 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2250/2500 --- Train Loss: 0.05092170095454628 --- Val Loss: 0.1981306687334096 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2260/2500 --- Train Loss: 0.05097895610268643 --- Val Loss: 0.19841401397815117 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2270/2500 --- Train Loss: 0.05103491065624275 --- Val Loss: 0.1986901061709317 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2280/2500 --- Train Loss: 0.05109124504544744 --- Val Loss: 0.1989679321488131 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2290/2500 --- Train Loss: 0.051146840332877315 --- Val Loss: 0.19924214025124512 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2300/2500 --- Train Loss: 0.051201949895040906 --- Val Loss: 0.19951317965786153 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2310/2500 --- Train Loss: 0.051256946100548643 --- Val Loss: 0.1997833902439144 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2320/2500 --- Train Loss: 0.05131126524957898 --- Val Loss: 0.20004997221322887 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2330/2500 --- Train Loss: 0.05136539770637834 --- Val Loss: 0.20031443165876092 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2340/2500 --- Train Loss: 0.051419899401554135 --- Val Loss: 0.20058052115378525 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2350/2500 --- Train Loss: 0.05147393445104807 --- Val Loss: 0.2008435085692472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2360/2500 --- Train Loss: 0.05152825810664451 --- Val Loss: 0.20110712750438295 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2370/2500 --- Train Loss: 0.051582033852184336 --- Val Loss: 0.20136799630922445 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2380/2500 --- Train Loss: 0.05163564050763575 --- Val Loss: 0.2016276880898069 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2390/2500 --- Train Loss: 0.05168858267309819 --- Val Loss: 0.20188348295936917 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2400/2500 --- Train Loss: 0.051741773807213634 --- Val Loss: 0.20214115419109002 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2410/2500 --- Train Loss: 0.051794367455122156 --- Val Loss: 0.20239499603761324 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2420/2500 --- Train Loss: 0.051846199530267256 --- Val Loss: 0.20264513413559923 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2430/2500 --- Train Loss: 0.05189777547616921 --- Val Loss: 0.2028935751141166 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2440/2500 --- Train Loss: 0.05194888845070302 --- Val Loss: 0.20313989490601472 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2450/2500 --- Train Loss: 0.05199946804449014 --- Val Loss: 0.20338300302256326 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2460/2500 --- Train Loss: 0.05205018412564615 --- Val Loss: 0.2036270957279281 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2470/2500 --- Train Loss: 0.05210034164073493 --- Val Loss: 0.2038677660822 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2480/2500 --- Train Loss: 0.05215058632592951 --- Val Loss: 0.20410926356051587 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 2490/2500 --- Train Loss: 0.05220006857998703 --- Val Loss: 0.20434664904373093 --- Train Acc: 1.00 --- Val Acc: 0.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model without regularisers\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 32))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=2500, batch_size=X_train.shape[0], optimizer='Momentum', learning_rate=0.1)\n",
    "\n",
    "network.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2500 --- Train Loss: 0.6925577172075759 --- Val Loss: 0.692679502173549 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 10/2500 --- Train Loss: 0.6891334241475388 --- Val Loss: 0.6946289433637719 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 20/2500 --- Train Loss: 0.6863294521077419 --- Val Loss: 0.6990837092195274 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 30/2500 --- Train Loss: 0.6856036852060223 --- Val Loss: 0.7034773120442475 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 40/2500 --- Train Loss: 0.6856172468746095 --- Val Loss: 0.7059152715441537 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 50/2500 --- Train Loss: 0.6856529682498226 --- Val Loss: 0.7065228622532403 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 60/2500 --- Train Loss: 0.6856324264097986 --- Val Loss: 0.7062139568033573 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 70/2500 --- Train Loss: 0.68560518830509 --- Val Loss: 0.7057067760824347 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 80/2500 --- Train Loss: 0.6855908078061775 --- Val Loss: 0.7053246177698285 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 90/2500 --- Train Loss: 0.6855855743945573 --- Val Loss: 0.7051283638041929 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 100/2500 --- Train Loss: 0.6855839204477991 --- Val Loss: 0.7050693168667289 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 110/2500 --- Train Loss: 0.6855842329266697 --- Val Loss: 0.7050796495415962 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 120/2500 --- Train Loss: 0.6855845267262338 --- Val Loss: 0.7051103212896239 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 130/2500 --- Train Loss: 0.6855852390496269 --- Val Loss: 0.7051366985077558 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 140/2500 --- Train Loss: 0.6855855505506484 --- Val Loss: 0.7051514425075344 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 150/2500 --- Train Loss: 0.6855843435339626 --- Val Loss: 0.7051562486905388 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 160/2500 --- Train Loss: 0.6855848412462738 --- Val Loss: 0.7051557790548005 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 170/2500 --- Train Loss: 0.6855838214335623 --- Val Loss: 0.7051536341155806 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 180/2500 --- Train Loss: 0.6855823789133936 --- Val Loss: 0.7051510933749644 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 190/2500 --- Train Loss: 0.6855816545284371 --- Val Loss: 0.705148818590035 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 200/2500 --- Train Loss: 0.685579831309347 --- Val Loss: 0.7051471584390944 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 210/2500 --- Train Loss: 0.6855777986084128 --- Val Loss: 0.7051460909691984 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 220/2500 --- Train Loss: 0.6855774548970884 --- Val Loss: 0.7051446713227468 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 230/2500 --- Train Loss: 0.6855752682488166 --- Val Loss: 0.7051428013610013 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 240/2500 --- Train Loss: 0.6855707032731877 --- Val Loss: 0.7051405838735231 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 250/2500 --- Train Loss: 0.6855670863503612 --- Val Loss: 0.7051372442845228 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 260/2500 --- Train Loss: 0.6855612272684652 --- Val Loss: 0.7051319810024845 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 270/2500 --- Train Loss: 0.6855535478953652 --- Val Loss: 0.7051244411315584 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 280/2500 --- Train Loss: 0.6855389979419774 --- Val Loss: 0.70511554274202 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 290/2500 --- Train Loss: 0.6855249522563782 --- Val Loss: 0.7051022119361665 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 300/2500 --- Train Loss: 0.6855038304502528 --- Val Loss: 0.705081414187011 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 310/2500 --- Train Loss: 0.6854562796510769 --- Val Loss: 0.7050476259440549 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 320/2500 --- Train Loss: 0.6853852250660776 --- Val Loss: 0.7049917040271106 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 330/2500 --- Train Loss: 0.6852551195740761 --- Val Loss: 0.7048941020804997 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 340/2500 --- Train Loss: 0.6850451674878628 --- Val Loss: 0.7047077758778048 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 350/2500 --- Train Loss: 0.6845702209777501 --- Val Loss: 0.7043225030895485 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 360/2500 --- Train Loss: 0.6835936964990997 --- Val Loss: 0.7034289381471354 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 370/2500 --- Train Loss: 0.6810596951258576 --- Val Loss: 0.7009968681710667 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 380/2500 --- Train Loss: 0.671807269842611 --- Val Loss: 0.6927584090826215 --- Train Acc: 0.56 --- Val Acc: 0.48\n",
      "Epoch 390/2500 --- Train Loss: 0.630693337317326 --- Val Loss: 0.653096756660973 --- Train Acc: 0.68 --- Val Acc: 0.55\n",
      "Epoch 400/2500 --- Train Loss: 0.4374902337301318 --- Val Loss: 0.4653857554187719 --- Train Acc: 0.94 --- Val Acc: 0.91\n",
      "Epoch 410/2500 --- Train Loss: 0.2551528248438314 --- Val Loss: 0.2796903174158699 --- Train Acc: 0.95 --- Val Acc: 0.89\n",
      "Epoch 420/2500 --- Train Loss: 0.1421373269610035 --- Val Loss: 0.18871529181401397 --- Train Acc: 0.96 --- Val Acc: 0.91\n",
      "Epoch 430/2500 --- Train Loss: 0.10360815526473403 --- Val Loss: 0.14595872194265988 --- Train Acc: 0.98 --- Val Acc: 0.95\n",
      "Epoch 440/2500 --- Train Loss: 0.09604531833765739 --- Val Loss: 0.12372674897673286 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 450/2500 --- Train Loss: 0.0779966813562053 --- Val Loss: 0.11156419772326119 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 460/2500 --- Train Loss: 0.08118003179478507 --- Val Loss: 0.10443427074589813 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 470/2500 --- Train Loss: 0.07265917067691638 --- Val Loss: 0.09940628257140806 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 480/2500 --- Train Loss: 0.05937766295675424 --- Val Loss: 0.09402064133646319 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 490/2500 --- Train Loss: 0.07587951908351372 --- Val Loss: 0.09020110641172192 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 500/2500 --- Train Loss: 0.0675947525272003 --- Val Loss: 0.08730582857671766 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 510/2500 --- Train Loss: 0.05736416633656382 --- Val Loss: 0.08366843771464715 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 520/2500 --- Train Loss: 0.06189375534887527 --- Val Loss: 0.08049217974790424 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 530/2500 --- Train Loss: 0.0657601592226128 --- Val Loss: 0.07839059963538017 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 540/2500 --- Train Loss: 0.053482606722494205 --- Val Loss: 0.07632893858512184 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 550/2500 --- Train Loss: 0.05691139069257995 --- Val Loss: 0.07404646009517234 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 560/2500 --- Train Loss: 0.0548932552638231 --- Val Loss: 0.0708717980771834 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 570/2500 --- Train Loss: 0.055757804333436474 --- Val Loss: 0.0684604995470899 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 580/2500 --- Train Loss: 0.051178977291736226 --- Val Loss: 0.06592258629508824 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 590/2500 --- Train Loss: 0.04895154156874915 --- Val Loss: 0.06383471716048918 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 600/2500 --- Train Loss: 0.052179797311194806 --- Val Loss: 0.06306790643277418 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 610/2500 --- Train Loss: 0.05475541341863779 --- Val Loss: 0.06178927589096079 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 620/2500 --- Train Loss: 0.051157639098418965 --- Val Loss: 0.06065563100603594 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 630/2500 --- Train Loss: 0.04886240588828738 --- Val Loss: 0.05837248092345561 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 640/2500 --- Train Loss: 0.042670462423276885 --- Val Loss: 0.05695016801207144 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 650/2500 --- Train Loss: 0.05051777315154005 --- Val Loss: 0.05607991875476498 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 660/2500 --- Train Loss: 0.04654215973640705 --- Val Loss: 0.05401971418738074 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 670/2500 --- Train Loss: 0.05193024613445353 --- Val Loss: 0.05256364399373361 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 680/2500 --- Train Loss: 0.03882844337771752 --- Val Loss: 0.0511690382672796 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 690/2500 --- Train Loss: 0.04281422643025754 --- Val Loss: 0.04946291947830186 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 700/2500 --- Train Loss: 0.044285021283403135 --- Val Loss: 0.04652102351066648 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 710/2500 --- Train Loss: 0.03371315936236395 --- Val Loss: 0.043104000612120406 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 720/2500 --- Train Loss: 0.03242816581113936 --- Val Loss: 0.03991860440391339 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 730/2500 --- Train Loss: 0.03594941626271351 --- Val Loss: 0.037280022592183444 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 740/2500 --- Train Loss: 0.03721866843423039 --- Val Loss: 0.03650381077613564 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 750/2500 --- Train Loss: 0.035076548041801846 --- Val Loss: 0.03493521782205544 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 760/2500 --- Train Loss: 0.027143207124121223 --- Val Loss: 0.03312502525428773 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 770/2500 --- Train Loss: 0.031835551457307466 --- Val Loss: 0.032753393831213805 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 780/2500 --- Train Loss: 0.02822166191060708 --- Val Loss: 0.03250124336217378 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 790/2500 --- Train Loss: 0.027594398625817394 --- Val Loss: 0.033076198719669235 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 800/2500 --- Train Loss: 0.025919989061970312 --- Val Loss: 0.030308377861508338 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 810/2500 --- Train Loss: 0.022491953890893212 --- Val Loss: 0.028402894015821526 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 820/2500 --- Train Loss: 0.03184995736788751 --- Val Loss: 0.027165104569708894 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 830/2500 --- Train Loss: 0.020554860029349635 --- Val Loss: 0.025805801699749685 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 840/2500 --- Train Loss: 0.02274855899152136 --- Val Loss: 0.024264682611628945 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 850/2500 --- Train Loss: 0.020541458488808072 --- Val Loss: 0.022928929717579077 --- Train Acc: 1.00 --- Val Acc: 0.98\n",
      "Epoch 860/2500 --- Train Loss: 0.022598643804083468 --- Val Loss: 0.021984460059653568 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/2500 --- Train Loss: 0.017800144998972676 --- Val Loss: 0.02131863626147353 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/2500 --- Train Loss: 0.016530972146817466 --- Val Loss: 0.019990763325520012 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/2500 --- Train Loss: 0.016379196047576774 --- Val Loss: 0.019858533776825362 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/2500 --- Train Loss: 0.018396432752437943 --- Val Loss: 0.018234041461422528 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/2500 --- Train Loss: 0.017443741268115234 --- Val Loss: 0.018027131583693918 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/2500 --- Train Loss: 0.016672981377504455 --- Val Loss: 0.016487010853609256 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/2500 --- Train Loss: 0.013315420416259962 --- Val Loss: 0.01552317573126966 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/2500 --- Train Loss: 0.01340080738110059 --- Val Loss: 0.014836588152534703 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/2500 --- Train Loss: 0.014956460215127212 --- Val Loss: 0.014227214209085402 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/2500 --- Train Loss: 0.01540441043074769 --- Val Loss: 0.013786793014935886 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/2500 --- Train Loss: 0.017212557723502372 --- Val Loss: 0.0135043095136448 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/2500 --- Train Loss: 0.016028904634731757 --- Val Loss: 0.013105353265873098 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/2500 --- Train Loss: 0.010889700787190645 --- Val Loss: 0.013238579730755878 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1000/2500 --- Train Loss: 0.011515051742132822 --- Val Loss: 0.012126116902025646 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1010/2500 --- Train Loss: 0.015434307715974128 --- Val Loss: 0.011883608864946852 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1020/2500 --- Train Loss: 0.010150371682015331 --- Val Loss: 0.01203416303432844 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1030/2500 --- Train Loss: 0.0107396908018682 --- Val Loss: 0.011453837338648243 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1040/2500 --- Train Loss: 0.016745086266827254 --- Val Loss: 0.01090458563948916 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1050/2500 --- Train Loss: 0.01236587402926537 --- Val Loss: 0.010345988029719524 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1060/2500 --- Train Loss: 0.009568205819248402 --- Val Loss: 0.010231263840948782 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1070/2500 --- Train Loss: 0.011176780313181077 --- Val Loss: 0.010251517125721556 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1080/2500 --- Train Loss: 0.012627837093695991 --- Val Loss: 0.009684756109527296 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1090/2500 --- Train Loss: 0.010441080335174508 --- Val Loss: 0.009546498286292431 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1100/2500 --- Train Loss: 0.010855721872207132 --- Val Loss: 0.009687517499935022 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1110/2500 --- Train Loss: 0.01335694894486748 --- Val Loss: 0.009258334245946038 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1120/2500 --- Train Loss: 0.008766907933044904 --- Val Loss: 0.008963584908522714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1130/2500 --- Train Loss: 0.008799490903683104 --- Val Loss: 0.00868129925588892 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1140/2500 --- Train Loss: 0.007478159931783346 --- Val Loss: 0.00870886447049181 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1150/2500 --- Train Loss: 0.011807462890098045 --- Val Loss: 0.008423244731008207 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1160/2500 --- Train Loss: 0.007943315151074748 --- Val Loss: 0.00806116678388466 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1170/2500 --- Train Loss: 0.010635342408091705 --- Val Loss: 0.00784736151456827 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1180/2500 --- Train Loss: 0.008347175655010013 --- Val Loss: 0.007345949945100359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1190/2500 --- Train Loss: 0.006533802443466034 --- Val Loss: 0.0071928880610624924 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1200/2500 --- Train Loss: 0.00860859676969678 --- Val Loss: 0.007059382217916327 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1210/2500 --- Train Loss: 0.007494396558749582 --- Val Loss: 0.00685813039287664 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1220/2500 --- Train Loss: 0.008220685726598495 --- Val Loss: 0.006738904705080171 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1230/2500 --- Train Loss: 0.009085533756161532 --- Val Loss: 0.006604144835489658 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1240/2500 --- Train Loss: 0.006805342514544842 --- Val Loss: 0.006688601472990807 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1250/2500 --- Train Loss: 0.006829677505765332 --- Val Loss: 0.006529809575136468 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1260/2500 --- Train Loss: 0.005584510649765428 --- Val Loss: 0.0064930163492925 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1270/2500 --- Train Loss: 0.007058498754381977 --- Val Loss: 0.006334345658533159 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1280/2500 --- Train Loss: 0.006172114295411593 --- Val Loss: 0.006626488560288286 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1290/2500 --- Train Loss: 0.009799453410386799 --- Val Loss: 0.00611034512047772 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1300/2500 --- Train Loss: 0.006395324346742605 --- Val Loss: 0.005787519124466273 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1310/2500 --- Train Loss: 0.00909095705663671 --- Val Loss: 0.00574648340131496 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1320/2500 --- Train Loss: 0.007689761920721603 --- Val Loss: 0.005784290619909677 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1330/2500 --- Train Loss: 0.006764089524500433 --- Val Loss: 0.005738167566161426 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1340/2500 --- Train Loss: 0.007451444142958114 --- Val Loss: 0.005500014825728182 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1350/2500 --- Train Loss: 0.008863569035604205 --- Val Loss: 0.005310042635886073 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1360/2500 --- Train Loss: 0.006260278880429502 --- Val Loss: 0.005114391901152316 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1370/2500 --- Train Loss: 0.008973280698511431 --- Val Loss: 0.005167994909405628 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1380/2500 --- Train Loss: 0.006147520151490131 --- Val Loss: 0.005281805304577775 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1390/2500 --- Train Loss: 0.008827449319982772 --- Val Loss: 0.00504644513759304 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1400/2500 --- Train Loss: 0.00445116008014962 --- Val Loss: 0.004905254776606873 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1410/2500 --- Train Loss: 0.006520490115931331 --- Val Loss: 0.0048671799659000745 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1420/2500 --- Train Loss: 0.005760621962825843 --- Val Loss: 0.0048599988264965135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1430/2500 --- Train Loss: 0.006668707317267629 --- Val Loss: 0.0049088597875778265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1440/2500 --- Train Loss: 0.006188624205956204 --- Val Loss: 0.00498103941545784 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1450/2500 --- Train Loss: 0.005523871783905831 --- Val Loss: 0.004805846808733509 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1460/2500 --- Train Loss: 0.004590454749358945 --- Val Loss: 0.004589572968566204 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1470/2500 --- Train Loss: 0.0053914708927028125 --- Val Loss: 0.004482732936111933 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1480/2500 --- Train Loss: 0.00547584879858105 --- Val Loss: 0.004404062360690368 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1490/2500 --- Train Loss: 0.005649384294168263 --- Val Loss: 0.004291859893025001 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1500/2500 --- Train Loss: 0.004752838077348759 --- Val Loss: 0.00421930376595559 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1510/2500 --- Train Loss: 0.0038986791970038476 --- Val Loss: 0.004145667112535133 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1520/2500 --- Train Loss: 0.005423344856032354 --- Val Loss: 0.004008631591413477 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1530/2500 --- Train Loss: 0.00376831498749665 --- Val Loss: 0.003927255238827974 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1540/2500 --- Train Loss: 0.00605861037364188 --- Val Loss: 0.0038798503212766505 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1550/2500 --- Train Loss: 0.00585762791758987 --- Val Loss: 0.003954304433869479 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1560/2500 --- Train Loss: 0.005682993664306846 --- Val Loss: 0.003877049091919389 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1570/2500 --- Train Loss: 0.0026731056991223952 --- Val Loss: 0.003722826296815434 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1580/2500 --- Train Loss: 0.0046217424596165695 --- Val Loss: 0.003663620552429227 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1590/2500 --- Train Loss: 0.005677141562176914 --- Val Loss: 0.0035824881588614466 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1600/2500 --- Train Loss: 0.004258531851546634 --- Val Loss: 0.003530594218498366 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1610/2500 --- Train Loss: 0.00446686433454183 --- Val Loss: 0.003507197747594906 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1620/2500 --- Train Loss: 0.005625003744455006 --- Val Loss: 0.003449394223827215 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1630/2500 --- Train Loss: 0.004128891977993279 --- Val Loss: 0.0034062615619243897 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1640/2500 --- Train Loss: 0.006167761762024622 --- Val Loss: 0.0033437901415674432 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1650/2500 --- Train Loss: 0.004715602752193707 --- Val Loss: 0.0032841258153674157 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1660/2500 --- Train Loss: 0.003924785867561825 --- Val Loss: 0.0032847442421720294 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1670/2500 --- Train Loss: 0.005183046818384398 --- Val Loss: 0.0033666527561367543 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1680/2500 --- Train Loss: 0.0033922710834041804 --- Val Loss: 0.003394204461319913 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1690/2500 --- Train Loss: 0.00699111577888676 --- Val Loss: 0.003365981283333395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1700/2500 --- Train Loss: 0.004236136198295884 --- Val Loss: 0.0032925452196873343 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1710/2500 --- Train Loss: 0.004630508922737061 --- Val Loss: 0.003226832980994135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1720/2500 --- Train Loss: 0.002623361571752669 --- Val Loss: 0.003101710113523808 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1730/2500 --- Train Loss: 0.004917059409304458 --- Val Loss: 0.003058955732198172 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1740/2500 --- Train Loss: 0.0038187150292149294 --- Val Loss: 0.003197282427808356 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1750/2500 --- Train Loss: 0.005476821129083707 --- Val Loss: 0.002994189846483205 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1760/2500 --- Train Loss: 0.0044265226693081285 --- Val Loss: 0.002867404938223572 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1770/2500 --- Train Loss: 0.004598156125590851 --- Val Loss: 0.0028730604865842605 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1780/2500 --- Train Loss: 0.006006738462271136 --- Val Loss: 0.0030503235194653438 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1790/2500 --- Train Loss: 0.0032304930606482556 --- Val Loss: 0.0029250646551532145 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1800/2500 --- Train Loss: 0.004820026486423086 --- Val Loss: 0.0029395718919891386 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1810/2500 --- Train Loss: 0.005055405918283756 --- Val Loss: 0.002980220842265415 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1820/2500 --- Train Loss: 0.0030207758160488854 --- Val Loss: 0.002899418999436486 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1830/2500 --- Train Loss: 0.0037117524942759093 --- Val Loss: 0.002832946142561336 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1840/2500 --- Train Loss: 0.0037392789262088687 --- Val Loss: 0.0028218760629401254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1850/2500 --- Train Loss: 0.003794404164247366 --- Val Loss: 0.002778429008324975 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1860/2500 --- Train Loss: 0.0028232252718904606 --- Val Loss: 0.0027584577255549162 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1870/2500 --- Train Loss: 0.003969838581613724 --- Val Loss: 0.0027015428592911408 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1880/2500 --- Train Loss: 0.0034413845635821306 --- Val Loss: 0.002653706060483604 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1890/2500 --- Train Loss: 0.004200461856001796 --- Val Loss: 0.0026248550511507067 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1900/2500 --- Train Loss: 0.005120497134064653 --- Val Loss: 0.002589322365284933 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1910/2500 --- Train Loss: 0.004182331398882365 --- Val Loss: 0.0025311300059961883 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1920/2500 --- Train Loss: 0.0034156387592583107 --- Val Loss: 0.0025144140115719903 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1930/2500 --- Train Loss: 0.0024525119066902815 --- Val Loss: 0.0024662208274282946 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1940/2500 --- Train Loss: 0.004025217881669416 --- Val Loss: 0.0024668127421115056 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1950/2500 --- Train Loss: 0.0032875173089876174 --- Val Loss: 0.0025004421937992126 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1960/2500 --- Train Loss: 0.004612135712848872 --- Val Loss: 0.0025085563815625267 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1970/2500 --- Train Loss: 0.004249081904233921 --- Val Loss: 0.0025621148443868698 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1980/2500 --- Train Loss: 0.004028802071775898 --- Val Loss: 0.0025274821328804555 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 1990/2500 --- Train Loss: 0.005236907919154941 --- Val Loss: 0.0025376364624345666 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2000/2500 --- Train Loss: 0.007361168452557653 --- Val Loss: 0.00246117443075788 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2010/2500 --- Train Loss: 0.0023443833203713417 --- Val Loss: 0.002417988290531425 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2020/2500 --- Train Loss: 0.002447577874489802 --- Val Loss: 0.002408017923973172 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2030/2500 --- Train Loss: 0.007648311209486878 --- Val Loss: 0.002391301338372838 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2040/2500 --- Train Loss: 0.0036875646769111113 --- Val Loss: 0.0023646842185099147 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2050/2500 --- Train Loss: 0.0033021665658419133 --- Val Loss: 0.0023464109428179966 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2060/2500 --- Train Loss: 0.004121379558683919 --- Val Loss: 0.002307568313554346 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2070/2500 --- Train Loss: 0.0038907365217764386 --- Val Loss: 0.0022481488963558985 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2080/2500 --- Train Loss: 0.0032502547955569803 --- Val Loss: 0.0022367214777964115 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2090/2500 --- Train Loss: 0.0025843273563891543 --- Val Loss: 0.0022119852584569564 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2100/2500 --- Train Loss: 0.003938545988013592 --- Val Loss: 0.002212732389146638 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2110/2500 --- Train Loss: 0.004239767105579096 --- Val Loss: 0.002205671710865539 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2120/2500 --- Train Loss: 0.004241044441720289 --- Val Loss: 0.0021998857450752246 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2130/2500 --- Train Loss: 0.00966160895091894 --- Val Loss: 0.0021669723465468256 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2140/2500 --- Train Loss: 0.00322456172876762 --- Val Loss: 0.002153661975429827 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2150/2500 --- Train Loss: 0.0028126120340996627 --- Val Loss: 0.0021478427137531886 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2160/2500 --- Train Loss: 0.002676051608573846 --- Val Loss: 0.0022528758027098995 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2170/2500 --- Train Loss: 0.003529754026916811 --- Val Loss: 0.0022775356091579435 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2180/2500 --- Train Loss: 0.003191371203987122 --- Val Loss: 0.0021110392817325664 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2190/2500 --- Train Loss: 0.0024764944555032576 --- Val Loss: 0.002051156680609694 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2200/2500 --- Train Loss: 0.003934107552448639 --- Val Loss: 0.0020175811458594847 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2210/2500 --- Train Loss: 0.0023741056092781167 --- Val Loss: 0.001998069365639531 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2220/2500 --- Train Loss: 0.0035760892762159583 --- Val Loss: 0.001975363624713395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2230/2500 --- Train Loss: 0.002789422378656887 --- Val Loss: 0.0019978208519205215 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2240/2500 --- Train Loss: 0.004331634132703507 --- Val Loss: 0.0019832375434528037 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2250/2500 --- Train Loss: 0.002633284691498197 --- Val Loss: 0.001969756884107421 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2260/2500 --- Train Loss: 0.0031389997569016416 --- Val Loss: 0.0019672897920308104 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2270/2500 --- Train Loss: 0.002637054869151932 --- Val Loss: 0.001974339015603073 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2280/2500 --- Train Loss: 0.0031566176575855342 --- Val Loss: 0.0019795312647501177 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2290/2500 --- Train Loss: 0.003345533535930763 --- Val Loss: 0.001924161392397691 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2300/2500 --- Train Loss: 0.0029374646889828424 --- Val Loss: 0.0018727624595598483 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2310/2500 --- Train Loss: 0.003250169717249075 --- Val Loss: 0.001835570729435845 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2320/2500 --- Train Loss: 0.0021898457460344492 --- Val Loss: 0.0018111468596234627 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2330/2500 --- Train Loss: 0.0026234496353101885 --- Val Loss: 0.0017740748846582339 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2340/2500 --- Train Loss: 0.0019459813894549078 --- Val Loss: 0.0017474401424379573 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2350/2500 --- Train Loss: 0.0033187722741593565 --- Val Loss: 0.00175158186123569 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2360/2500 --- Train Loss: 0.0032893709906180613 --- Val Loss: 0.0017390357757778412 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2370/2500 --- Train Loss: 0.0023568888787830976 --- Val Loss: 0.0017213461456032219 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2380/2500 --- Train Loss: 0.0022216855763914794 --- Val Loss: 0.0017074384397092865 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2390/2500 --- Train Loss: 0.0040496297804489614 --- Val Loss: 0.001761271309063095 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2400/2500 --- Train Loss: 0.0032524851458473813 --- Val Loss: 0.0018033482022370274 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2410/2500 --- Train Loss: 0.0028229482282509373 --- Val Loss: 0.0017738302120820736 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2420/2500 --- Train Loss: 0.002161376201267752 --- Val Loss: 0.0017440434848295474 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2430/2500 --- Train Loss: 0.0018855834396790782 --- Val Loss: 0.0017146725871072211 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2440/2500 --- Train Loss: 0.002268527143541802 --- Val Loss: 0.0016873283837634816 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2450/2500 --- Train Loss: 0.0021518593712970065 --- Val Loss: 0.0016826504320027918 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2460/2500 --- Train Loss: 0.0025450582758539163 --- Val Loss: 0.001724849359586847 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2470/2500 --- Train Loss: 0.0022947422293078837 --- Val Loss: 0.0017302420498946439 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2480/2500 --- Train Loss: 0.00399434931625758 --- Val Loss: 0.0017098912855572153 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 2490/2500 --- Train Loss: 0.002237026806635627 --- Val Loss: 0.0016587038200702882 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the neural network model without regularisers\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(32, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=2500, batch_size=X_train.shape[0], optimizer='Momentum', learning_rate=0.1)\n",
    "\n",
    "network.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49815a38",
   "metadata": {},
   "source": [
    "# Next Step (Out of Scope)\n",
    "\n",
    "* optimization of hyperparameters (random search and grid search function?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702fa55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21dae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    def __init__(self, network, param_grid, n_iter=10):\n",
    "        self.network = NeuralNetwork\n",
    "        self.param_grid = param_grid\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def sample_params(self):\n",
    "        sampled_params = {}\n",
    "        for param, values in self.param_grid.items():\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        return sampled_params\n",
    "\n",
    "    def evaluate(self, X_train, y_train, X_val, y_val, params):\n",
    "        network = self.network()\n",
    "\n",
    "        config = params['layer_configs']\n",
    "        # Add the first Dense layer\n",
    "        network.add_layer(Layer(64, config['layer1_nodes'], l1=config['layer1_l1'], l2=config['layer1_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        network.add_layer(Dropout(0.25))\n",
    "\n",
    "        print(config['layer1_nodes'])\n",
    "        # Add the second Dense layer\n",
    "        network.add_layer(Layer(config['layer1_nodes'], config['layer2_nodes'], l1=config['layer2_l1'], l2=config['layer2_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        # Add the output Softmax layer\n",
    "        network.add_layer(Layer(config['layer2_nodes'], 10))\n",
    "        network.add_layer(Softmax())\n",
    "        \n",
    "        network.train(X_train, y_train, epochs=params['epochs'], learning_rate=params['learning_rate'],\n",
    "                      optimizer=params['optimizer'], momentum=params['momentum'], batch_size=params['batch_size'])\n",
    "        \n",
    "        y_pred = network.predict(X_val)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def search(self, X, y):\n",
    "        best_params = None\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            params = self.sample_params()\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            accuracy = self.evaluate(X_train, y_train, X_val, y_val, params)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "\n",
    "            print(f\"Params: {params}, Accuracy: {accuracy}\")\n",
    "\n",
    "        return best_params, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20145077",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'layer_configs': [\n",
    "        {\n",
    "            'layer1_nodes': layer1_nodes,\n",
    "            'layer1_l1': layer1_l1,\n",
    "            'layer1_l2': layer1_l2,\n",
    "            'layer2_nodes': layer2_nodes,\n",
    "            'layer2_l1': layer2_l1,\n",
    "            'layer2_l2': layer2_l2\n",
    "        }\n",
    "        for layer1_nodes in [32, 64, 128]  # Possible node counts for the first Dense layer\n",
    "        for layer1_l1 in [0.0, 0.01]       # L1 regularization for the first Dense layer\n",
    "        for layer1_l2 in [0.0, 0.01]       # L2 regularization for the first Dense layer\n",
    "        for layer2_nodes in [16, 32, 64]   # Possible node counts for the second Dense layer\n",
    "        for layer2_l1 in [0.0, 0.01]       # L1 regularization for the second Dense layer\n",
    "        for layer2_l2 in [0.0, 0.01]       # L2 regularization for the second Dense layer\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'epochs': [100, 500, 1000],\n",
    "    'optimizer': ['GD', 'Momentum'],\n",
    "    'momentum': [0.5, 0.9],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd34073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "Epoch 0/100 --- Train Loss: 0.5999855043392593 --- Val Loss: 0.5184779946092196 --- Train Acc: 0.86 --- Val Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 --- Train Loss: 1.9736622173200513 --- Val Loss: 2.012554687860109 --- Train Acc: 0.24 --- Val Acc: 0.22\n",
      "Epoch 20/100 --- Train Loss: 2.3141052452932476 --- Val Loss: 2.298881316877556 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3282237266348615 --- Val Loss: 2.2995982299973954 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.298121232053494 --- Val Loss: 2.2992258521342626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.292297161109781 --- Val Loss: 2.298238102510764 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3004727544799395 --- Val Loss: 2.299817073251843 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.3003352799871983 --- Val Loss: 2.2998766647194833 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.300441803492836 --- Val Loss: 2.3011424278194994 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3004233171236357 --- Val Loss: 2.299077081469976 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.302011439297 --- Val Loss: 2.301463499549136 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 0.5533321718646385 --- Val Loss: 0.12474962415653879 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 20/100 --- Train Loss: 1.2847330655422775 --- Val Loss: 0.3337924367947385 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 30/100 --- Train Loss: 1.104460411902192 --- Val Loss: 0.7148682143465206 --- Train Acc: 0.81 --- Val Acc: 0.81\n",
      "Epoch 40/100 --- Train Loss: 1.2544141055452498 --- Val Loss: 1.1906429752114784 --- Train Acc: 0.57 --- Val Acc: 0.53\n",
      "Epoch 50/100 --- Train Loss: 1.4329557856159247 --- Val Loss: 1.3555572379482936 --- Train Acc: 0.49 --- Val Acc: 0.48\n",
      "Epoch 60/100 --- Train Loss: 1.6705781734101837 --- Val Loss: 1.6787415160638648 --- Train Acc: 0.38 --- Val Acc: 0.37\n",
      "Epoch 70/100 --- Train Loss: 1.904660286675092 --- Val Loss: 1.9395355515722228 --- Train Acc: 0.25 --- Val Acc: 0.27\n",
      "Epoch 80/100 --- Train Loss: 2.042517618982838 --- Val Loss: 2.084516381192997 --- Train Acc: 0.20 --- Val Acc: 0.20\n",
      "Epoch 90/100 --- Train Loss: 2.0002692730447706 --- Val Loss: 2.0409641084053183 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.15555555555555556\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3023536416955546 --- Val Loss: 2.3021031078838643 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.3010480771203943 --- Val Loss: 2.2988832015841267 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.2962047993790025 --- Val Loss: 2.2926800135154677 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: 1.906779527904955 --- Val Loss: 1.8477939110032422 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 40/1000 --- Train Loss: 0.8690203796503909 --- Val Loss: 0.7640393863106343 --- Train Acc: 0.81 --- Val Acc: 0.78\n",
      "Epoch 50/1000 --- Train Loss: 0.5804793707466696 --- Val Loss: 0.404953989767038 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 60/1000 --- Train Loss: 0.4676990972591807 --- Val Loss: 0.27910152873454264 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 70/1000 --- Train Loss: 0.4047974339644763 --- Val Loss: 0.20938465203519094 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 80/1000 --- Train Loss: 0.43265457298336246 --- Val Loss: 0.18925654561706096 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 90/1000 --- Train Loss: 0.4112709103291219 --- Val Loss: 0.14644963284246734 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 100/1000 --- Train Loss: 0.3736640090911676 --- Val Loss: 0.12871976663552107 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 110/1000 --- Train Loss: 0.41731556316051893 --- Val Loss: 0.13335678653630445 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 120/1000 --- Train Loss: 0.3700259444295674 --- Val Loss: 0.10267966669729102 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 130/1000 --- Train Loss: 0.30402003789853577 --- Val Loss: 0.11411608108398984 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 140/1000 --- Train Loss: 0.3074919217964152 --- Val Loss: 0.07896390299625522 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 150/1000 --- Train Loss: 0.3062936171332492 --- Val Loss: 0.06122248847638451 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 160/1000 --- Train Loss: 0.28849106582764866 --- Val Loss: 0.0421664031235748 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 170/1000 --- Train Loss: 0.35130463757807806 --- Val Loss: 0.06483612222228208 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 180/1000 --- Train Loss: 0.24612433818288754 --- Val Loss: 0.04039685763134246 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 190/1000 --- Train Loss: 0.36573350512471325 --- Val Loss: 0.05679325565324692 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 200/1000 --- Train Loss: 0.39528474658775886 --- Val Loss: 0.09257984374566992 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 210/1000 --- Train Loss: 0.3817251066432929 --- Val Loss: 0.069144551572881 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 220/1000 --- Train Loss: 0.3657215390554863 --- Val Loss: 0.04049989657503687 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 230/1000 --- Train Loss: 0.3942020671226752 --- Val Loss: 0.0749687470388529 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 240/1000 --- Train Loss: 0.4955173879715996 --- Val Loss: 0.07628850421042482 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 250/1000 --- Train Loss: 0.42323900479936427 --- Val Loss: 0.09831136528728791 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 260/1000 --- Train Loss: 0.43750396845702455 --- Val Loss: 0.08749922006524734 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 270/1000 --- Train Loss: 0.43739503895455506 --- Val Loss: 0.11965295617503051 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 280/1000 --- Train Loss: 0.49594128995367237 --- Val Loss: 0.1493883003347014 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 290/1000 --- Train Loss: 0.5016012880798135 --- Val Loss: 0.1230567714475073 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 300/1000 --- Train Loss: 0.6512328946511692 --- Val Loss: 0.2329874140115493 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 310/1000 --- Train Loss: 0.5639004940166822 --- Val Loss: 0.21682628337663448 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 320/1000 --- Train Loss: 0.5061423808399905 --- Val Loss: 0.2170878860874055 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 330/1000 --- Train Loss: 0.5854231367331595 --- Val Loss: 0.21540238594991865 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 340/1000 --- Train Loss: 0.694266232715664 --- Val Loss: 0.3004553110234194 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 350/1000 --- Train Loss: 0.6741147293675224 --- Val Loss: 0.3798765291373849 --- Train Acc: 0.90 --- Val Acc: 0.88\n",
      "Epoch 360/1000 --- Train Loss: 0.7486388615209506 --- Val Loss: 0.3721572862420508 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 370/1000 --- Train Loss: 0.8700336648749093 --- Val Loss: 0.4862872224209113 --- Train Acc: 0.85 --- Val Acc: 0.84\n",
      "Epoch 380/1000 --- Train Loss: 0.7844041523234694 --- Val Loss: 0.44855290838146855 --- Train Acc: 0.85 --- Val Acc: 0.86\n",
      "Epoch 390/1000 --- Train Loss: 0.840254205042708 --- Val Loss: 0.48979599677549723 --- Train Acc: 0.86 --- Val Acc: 0.85\n",
      "Epoch 400/1000 --- Train Loss: 0.9717231761297369 --- Val Loss: 0.5447470688310894 --- Train Acc: 0.84 --- Val Acc: 0.84\n",
      "Epoch 410/1000 --- Train Loss: 1.1241927115637182 --- Val Loss: 0.7308472460517437 --- Train Acc: 0.78 --- Val Acc: 0.76\n",
      "Epoch 420/1000 --- Train Loss: 1.1109110837083507 --- Val Loss: 0.6573709391272455 --- Train Acc: 0.79 --- Val Acc: 0.79\n",
      "Epoch 430/1000 --- Train Loss: 1.1092782276437365 --- Val Loss: 0.8456695031041861 --- Train Acc: 0.70 --- Val Acc: 0.70\n",
      "Epoch 440/1000 --- Train Loss: 1.1056604920597561 --- Val Loss: 1.0107929917858105 --- Train Acc: 0.68 --- Val Acc: 0.63\n",
      "Epoch 450/1000 --- Train Loss: 1.3088755309579407 --- Val Loss: 1.1720137439663736 --- Train Acc: 0.60 --- Val Acc: 0.57\n",
      "Epoch 460/1000 --- Train Loss: 1.3630937529537166 --- Val Loss: 1.0551885053871655 --- Train Acc: 0.58 --- Val Acc: 0.59\n",
      "Epoch 470/1000 --- Train Loss: 1.493226265555102 --- Val Loss: 1.4037680781204804 --- Train Acc: 0.51 --- Val Acc: 0.48\n",
      "Epoch 480/1000 --- Train Loss: 1.6196060348324015 --- Val Loss: 1.2742844255957622 --- Train Acc: 0.52 --- Val Acc: 0.51\n",
      "Epoch 490/1000 --- Train Loss: 1.6753967634783449 --- Val Loss: 1.5987840592921796 --- Train Acc: 0.41 --- Val Acc: 0.39\n",
      "Epoch 500/1000 --- Train Loss: 1.8161143099728883 --- Val Loss: 1.8168899673659316 --- Train Acc: 0.30 --- Val Acc: 0.30\n",
      "Epoch 510/1000 --- Train Loss: 2.0024067549120517 --- Val Loss: 2.094660613463582 --- Train Acc: 0.20 --- Val Acc: 0.18\n",
      "Epoch 520/1000 --- Train Loss: 1.891485981174709 --- Val Loss: 1.959807966129462 --- Train Acc: 0.27 --- Val Acc: 0.24\n",
      "Epoch 530/1000 --- Train Loss: 1.854391358853092 --- Val Loss: 1.8697344929766795 --- Train Acc: 0.29 --- Val Acc: 0.27\n",
      "Epoch 540/1000 --- Train Loss: 2.1644445651580724 --- Val Loss: 2.16495329722956 --- Train Acc: 0.17 --- Val Acc: 0.16\n",
      "Epoch 550/1000 --- Train Loss: 2.1986014789582855 --- Val Loss: 2.2202033395273437 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: 2.2195838934975645 --- Val Loss: 2.240315243430494 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: 2.14171296131725 --- Val Loss: 2.2551909469175824 --- Train Acc: 0.14 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.1650520284079784 --- Val Loss: 2.1963389917096343 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 590/1000 --- Train Loss: 2.2012972607699672 --- Val Loss: 2.2352501618508094 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.2080880234018783 --- Val Loss: 2.270271910972144 --- Train Acc: 0.13 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.211144503314433 --- Val Loss: 2.2327242665953135 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.2845683109252453 --- Val Loss: 2.3045481577977616 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.2476129754668923 --- Val Loss: 2.292103643612468 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 640/1000 --- Train Loss: 2.2563995883577186 --- Val Loss: 2.2911755112735004 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 650/1000 --- Train Loss: 2.2957304949967337 --- Val Loss: 2.298260662929979 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: 2.2876082206255486 --- Val Loss: 2.29774389686969 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: 2.2827575892155836 --- Val Loss: 2.2974479512916512 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: 2.288531629670692 --- Val Loss: 2.297387638131499 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: 2.300654765713967 --- Val Loss: 2.297150121038427 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.294781860980208 --- Val Loss: 2.2970163523178835 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.2906591400200877 --- Val Loss: 2.2970039359061447 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.2968077569680423 --- Val Loss: 2.296956765883378 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: 2.2987714637936585 --- Val Loss: 2.2969239512574706 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 740/1000 --- Train Loss: 2.322852890106748 --- Val Loss: 2.2968327710839898 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: 2.294831654923094 --- Val Loss: 2.296942805174748 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: 2.294778292567759 --- Val Loss: 2.2968447921161506 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: 2.2965853429432337 --- Val Loss: 2.2968933224199453 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 780/1000 --- Train Loss: 2.2986046771328392 --- Val Loss: 2.2968712655730257 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 790/1000 --- Train Loss: 2.2947793989444447 --- Val Loss: 2.2968245428334817 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.2967972244055783 --- Val Loss: 2.2967994878439466 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.296797890300276 --- Val Loss: 2.2968609087023117 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: 2.3007373513491953 --- Val Loss: 2.2968364539001414 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: 2.2987667101817375 --- Val Loss: 2.296824489298127 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: 2.300736059338258 --- Val Loss: 2.2967979713433593 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: 2.2948793415556508 --- Val Loss: 2.296879593231434 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.294851917996861 --- Val Loss: 2.2969588921169835 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: 2.3007430107939775 --- Val Loss: 2.2969405576287363 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: 2.306849411019448 --- Val Loss: 2.2969761916184446 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: 2.298802233007638 --- Val Loss: 2.2969776843302507 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: 2.30074172173645 --- Val Loss: 2.296945761490134 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: 2.2987667467993127 --- Val Loss: 2.2968799292871895 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.300736720295268 --- Val Loss: 2.2968589901091754 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.2987676316427996 --- Val Loss: 2.296811170317952 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: 2.3007355156020304 --- Val Loss: 2.2968570491321123 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: 2.300735246247712 --- Val Loss: 2.2968375799696723 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.3007362935551083 --- Val Loss: 2.2968905448342545 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.296795692456576 --- Val Loss: 2.2968549942192253 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.300736067504765 --- Val Loss: 2.2968825477983543 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: 2.300736042840048 --- Val Loss: 2.296835811278673 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3079016500216394 --- Val Loss: 2.3179280098472628 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.314706901143255 --- Val Loss: 2.30971631985025 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.3190682773562368 --- Val Loss: 2.3270914563915204 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.306794816289979 --- Val Loss: 2.3217365690308673 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 40/1000 --- Train Loss: 2.3051949700519825 --- Val Loss: 2.3044400874058235 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.311083936943365 --- Val Loss: 2.320281143050674 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.310083000328314 --- Val Loss: 2.3281317116287887 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.311286900269716 --- Val Loss: 2.300009301491421 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.3119802678089454 --- Val Loss: 2.3315156818427063 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 90/1000 --- Train Loss: 2.3085519811798347 --- Val Loss: 2.3142906813440325 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 100/1000 --- Train Loss: 2.3077470832622957 --- Val Loss: 2.3105846943191337 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.314289228275405 --- Val Loss: 2.3010916679312525 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.307485037618217 --- Val Loss: 2.312057274360391 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.307816377670478 --- Val Loss: 2.301867928949315 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.3028812422783482 --- Val Loss: 2.3035557518047063 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.311806280752387 --- Val Loss: 2.3163197623731127 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3120115995199506 --- Val Loss: 2.3030083665727057 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 170/1000 --- Train Loss: 2.3066803909374434 --- Val Loss: 2.30698601380861 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: 2.307382487349041 --- Val Loss: 2.319135699928359 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.320248040068271 --- Val Loss: 2.3322740500468373 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: 2.3064106789500265 --- Val Loss: 2.311708915681398 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.321231541049538 --- Val Loss: 2.331280740513725 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: 2.3076362430324724 --- Val Loss: 2.316861780374755 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 230/1000 --- Train Loss: 2.3125172070520024 --- Val Loss: 2.3069413370924354 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.310137016224422 --- Val Loss: 2.3196620485431354 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 250/1000 --- Train Loss: 2.3146483037665244 --- Val Loss: 2.3218746003023587 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: 2.310402029515372 --- Val Loss: 2.2998827769616725 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.3170869327254175 --- Val Loss: 2.321161311980031 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 280/1000 --- Train Loss: 2.3150583372866405 --- Val Loss: 2.3206020838988795 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.3109760159412107 --- Val Loss: 2.3116105695503686 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.314861485009709 --- Val Loss: 2.314809833696721 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: 2.30514548378756 --- Val Loss: 2.3069596696953663 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: 2.3092546369745146 --- Val Loss: 2.322076875372099 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.309039407721538 --- Val Loss: 2.3228252250560124 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: 2.3163540245506633 --- Val Loss: 2.3292008540618934 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: 2.3274066089849104 --- Val Loss: 2.361710538557727 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: 2.3112942741821523 --- Val Loss: 2.315116293850105 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: 2.312801694251867 --- Val Loss: 2.3209951926514987 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3084528090756753 --- Val Loss: 2.300131791048612 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.309384206796736 --- Val Loss: 2.3167022954489016 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.3126248286672166 --- Val Loss: 2.3197655010770046 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 410/1000 --- Train Loss: 2.3147938913267323 --- Val Loss: 2.3159805785273564 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.3105418999736327 --- Val Loss: 2.306687834653238 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: 2.309892563952587 --- Val Loss: 2.29501636077944 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3060181856605944 --- Val Loss: 2.315894369426802 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.320633366273046 --- Val Loss: 2.3142189082467524 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: 2.3077003291761353 --- Val Loss: 2.3116506602947196 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.313201961594777 --- Val Loss: 2.311227768672081 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.308311713034914 --- Val Loss: 2.314464988950519 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.3113639379109974 --- Val Loss: 2.311405552232979 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 500/1000 --- Train Loss: 2.316853222805209 --- Val Loss: 2.315811676633905 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: 2.305704753983222 --- Val Loss: 2.3020612146751733 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.302884235549997 --- Val Loss: 2.305913199950074 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.323927989801645 --- Val Loss: 2.3035471031708954 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.3183905793632156 --- Val Loss: 2.3195693589379696 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: 2.3232541609776014 --- Val Loss: 2.319533322825822 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: 2.313475085658839 --- Val Loss: 2.3172437388457885 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.303926383880064 --- Val Loss: 2.307856055789743 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 580/1000 --- Train Loss: 2.3066471639782162 --- Val Loss: 2.3146454206932727 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 590/1000 --- Train Loss: 2.3095307496853783 --- Val Loss: 2.3034475467399873 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: 2.310024251939036 --- Val Loss: 2.305694014402898 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.309040178403386 --- Val Loss: 2.3106541259085853 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3131906103057003 --- Val Loss: 2.3040994272399358 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: 2.3058761965713828 --- Val Loss: 2.3055994179865267 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 640/1000 --- Train Loss: 2.309175249698966 --- Val Loss: 2.3046501165021698 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.3073945121382415 --- Val Loss: 2.318194578070759 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: 2.3134874262601994 --- Val Loss: 2.30460181472953 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: 2.317020275047714 --- Val Loss: 2.330591846310178 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: 2.3150715420408954 --- Val Loss: 2.301817614773947 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: 2.3077492251588017 --- Val Loss: 2.3009641214959315 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: 2.3109953914281576 --- Val Loss: 2.3132649557531684 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.3112434592456133 --- Val Loss: 2.3101359481906334 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.311157754721371 --- Val Loss: 2.3112589142985143 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.320564602563599 --- Val Loss: 2.3179536312211884 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: 2.3148509994547775 --- Val Loss: 2.3202447536767723 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: 2.313486267998186 --- Val Loss: 2.326804208994547 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.315343528872116 --- Val Loss: 2.3147724674086874 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: 2.307222596888231 --- Val Loss: 2.310154327440256 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.309902933868237 --- Val Loss: 2.3154747372041347 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: 2.317210430177777 --- Val Loss: 2.3155896396854807 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: 2.3084365133364817 --- Val Loss: 2.300257920206204 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.3101096306367306 --- Val Loss: 2.31224050176899 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3133962723545727 --- Val Loss: 2.3091048667478145 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.3161300504565507 --- Val Loss: 2.3307291395274943 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 840/1000 --- Train Loss: 2.3112598311358328 --- Val Loss: 2.307557979440662 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.3085038171502315 --- Val Loss: 2.300274025655105 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.315208880608467 --- Val Loss: 2.3168244436678993 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.315341954337449 --- Val Loss: 2.3181053683335664 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 880/1000 --- Train Loss: 2.3140043457701225 --- Val Loss: 2.3112647148487326 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.318113782554869 --- Val Loss: 2.321478999478225 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: 2.3086930286429146 --- Val Loss: 2.308835269561117 --- Train Acc: 0.10 --- Val Acc: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_420\\3559256763.py:50: RuntimeWarning: overflow encountered in add\n",
      "  self.weights += self.velocity_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3024344876416922 --- Val Loss: 2.3021912933696695 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/100 --- Train Loss: 2.301681666811276 --- Val Loss: 2.299747642532327 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.301503065841283 --- Val Loss: 2.298699752287707 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3014632419332925 --- Val Loss: 2.298285811514586 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/100 --- Train Loss: 2.3014529990065884 --- Val Loss: 2.2980495560357594 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.301450036768018 --- Val Loss: 2.297936859392635 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.301449478439546 --- Val Loss: 2.297891434175459 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3014492178851906 --- Val Loss: 2.297874548024523 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.301449111903596 --- Val Loss: 2.2978452897230675 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.301449060396747 --- Val Loss: 2.297852239994448 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.30252938798153 --- Val Loss: 2.302594856313988 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3021236681026425 --- Val Loss: 2.3027294257359445 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.3017445501357052 --- Val Loss: 2.3027276805770454 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.298657125898102 --- Val Loss: 2.299742400201479 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.1773059572918583 --- Val Loss: 2.173858331368432 --- Train Acc: 0.45 --- Val Acc: 0.46\n",
      "Epoch 50/500 --- Train Loss: 1.1544251555628493 --- Val Loss: 1.0756584713617825 --- Train Acc: 0.69 --- Val Acc: 0.72\n",
      "Epoch 60/500 --- Train Loss: 0.6066128745448404 --- Val Loss: 0.5058514659024163 --- Train Acc: 0.85 --- Val Acc: 0.88\n",
      "Epoch 70/500 --- Train Loss: 0.3473520627372894 --- Val Loss: 0.24220784715380186 --- Train Acc: 0.92 --- Val Acc: 0.94\n",
      "Epoch 80/500 --- Train Loss: 0.23273151749392873 --- Val Loss: 0.12917514589786305 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 90/500 --- Train Loss: 0.17055177503116337 --- Val Loss: 0.0728079571222616 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 100/500 --- Train Loss: 0.1439644583452351 --- Val Loss: 0.04580009069683281 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 110/500 --- Train Loss: 0.13234624833632522 --- Val Loss: 0.03431761908906942 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 120/500 --- Train Loss: 0.10162717610757434 --- Val Loss: 0.016506001707777163 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.1308142843830718 --- Val Loss: 0.014326456969895183 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.09149902100346127 --- Val Loss: 0.01114809135548031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.0928328852480408 --- Val Loss: 0.008465869413484244 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.07673135216525566 --- Val Loss: 0.004241925294150241 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.11488737921913673 --- Val Loss: 0.004339940411328046 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/500 --- Train Loss: 0.12572736401898604 --- Val Loss: 0.004395749172997894 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.08487609098392794 --- Val Loss: 0.0018669085473684295 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/500 --- Train Loss: 0.08550078578640498 --- Val Loss: 0.0011630467465396168 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.11214327886556467 --- Val Loss: 0.002441182502368521 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.11607601626467637 --- Val Loss: 0.001405985013071934 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.12678921112833444 --- Val Loss: 0.0014094536069758905 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.1176917713315986 --- Val Loss: 0.0005222075167877477 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.10632967942546218 --- Val Loss: 0.0017897031599918027 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.08825923059355603 --- Val Loss: 0.0008521165025935108 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.3031337677963663 --- Val Loss: 0.0964157160730606 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 280/500 --- Train Loss: 0.11895076861160724 --- Val Loss: 0.00013576059459881462 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.12787221309999244 --- Val Loss: 0.0019136231939271618 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.08573067099849244 --- Val Loss: 0.0004760352027019031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.11016445296790733 --- Val Loss: 0.002662311619808393 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.1044791921569125 --- Val Loss: 0.0007811995365826746 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/500 --- Train Loss: 0.2155786662377698 --- Val Loss: 0.0037252742673046036 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/500 --- Train Loss: 0.1401652108719765 --- Val Loss: 0.0059992527949094395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/500 --- Train Loss: 0.11468329203222434 --- Val Loss: 0.0003465379718781125 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.12842745380967283 --- Val Loss: 6.0477680204401365e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.06770394223701703 --- Val Loss: 1.0689218199930784e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/500 --- Train Loss: 0.1507566332045444 --- Val Loss: 3.100668394314715e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/500 --- Train Loss: 0.17047735383330428 --- Val Loss: 2.599463798879381e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.2299921863092159 --- Val Loss: 1.0208237070604912e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/500 --- Train Loss: 0.17981615816835161 --- Val Loss: 3.569483659336487e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.13163716447399784 --- Val Loss: 3.011401966519195e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/500 --- Train Loss: 0.0610077876328023 --- Val Loss: 1.5345635276368304e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/500 --- Train Loss: 0.05753368897355579 --- Val Loss: 7.504603310418118e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/500 --- Train Loss: 0.20955462980006956 --- Val Loss: 2.401593057904207e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/500 --- Train Loss: 0.2721928039482032 --- Val Loss: 0.00019973937228098243 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/500 --- Train Loss: 0.20712930935203297 --- Val Loss: 0.009191896858880931 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/500 --- Train Loss: 0.19675035194047383 --- Val Loss: 0.0008674641414779185 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/500 --- Train Loss: 0.28342253269460177 --- Val Loss: 1.245371941771986e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.95\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3018828725257885 --- Val Loss: 2.302453495189011 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 0.31543521299481325 --- Val Loss: 0.23000306820187355 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 20/500 --- Train Loss: 0.28162063805117815 --- Val Loss: 0.07903203244733437 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 30/500 --- Train Loss: 0.44933823889845054 --- Val Loss: 0.07707138241310396 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 40/500 --- Train Loss: 0.8177367465809571 --- Val Loss: 0.1428833314649073 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 50/500 --- Train Loss: 0.5885931320724652 --- Val Loss: 0.07104632097746696 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 60/500 --- Train Loss: 0.5966001695390973 --- Val Loss: 0.05616178994667823 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/500 --- Train Loss: 0.3684624237206901 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/500 --- Train Loss: 0.33311951479899554 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/500 --- Train Loss: 0.32778386054284797 --- Val Loss: 0.022559042687607615 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/500 --- Train Loss: 0.35637186590344594 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/500 --- Train Loss: 0.26482514756049974 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/500 --- Train Loss: 0.19175724108985084 --- Val Loss: 0.056160711675817904 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.275598290899644 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.13361550414937468 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.16818892314043954 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.25228333471065684 --- Val Loss: 0.056160711675817904 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.3784249520659828 --- Val Loss: 0.11232132335163082 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 180/500 --- Train Loss: 0.26629906997235975 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.714802598346852 --- Val Loss: 0.39312438173069564 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 200/500 --- Train Loss: 0.2720844829118859 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.43448789311279434 --- Val Loss: 0.16848193502744374 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/500 --- Train Loss: 0.3363777462808742 --- Val Loss: 0.05616071167581789 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.3223620110191712 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.2564643831944011 --- Val Loss: 0.00837676508067734 --- Train Acc: 0.99 --- Val Acc: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_420\\118437895.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.5167547562343593 --- Val Loss: 2.2408856609171965 --- Train Acc: 0.50 --- Val Acc: 0.47\n",
      "Epoch 10/1000 --- Train Loss: 2.3221897760613817 --- Val Loss: 2.3245191099430147 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.300557251880854 --- Val Loss: 2.2977351475338077 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.7691132313867866 --- Val Loss: 1.8083260961901564 --- Train Acc: 0.48 --- Val Acc: 0.46\n",
      "Epoch 20/1000 --- Train Loss: 14.492270360600786 --- Val Loss: 14.433277300683933 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302405248945103 --- Val Loss: 2.302358059663533 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3015468924304647 --- Val Loss: 2.301193756669537 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.3013487105860464 --- Val Loss: 2.3008368441740488 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.301303064176355 --- Val Loss: 2.300722489192698 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3012911560577556 --- Val Loss: 2.300663771351669 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.301288100515383 --- Val Loss: 2.300641122002933 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3012870281671534 --- Val Loss: 2.300615017168025 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.30128648042876 --- Val Loss: 2.3006084458477147 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.3012858299749577 --- Val Loss: 2.300598917551925 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.3012850540375682 --- Val Loss: 2.3006091391741443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 3.3724016434240367 --- Val Loss: 2.5750967610273827 --- Train Acc: 0.63 --- Val Acc: 0.60\n",
      "Epoch 10/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.301262078540388 --- Val Loss: 2.2986256661832574 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 10/1000 --- Train Loss: 0.35163245720691816 --- Val Loss: 0.19420962200064637 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 20/1000 --- Train Loss: 0.4325578724716512 --- Val Loss: 0.2169743361483166 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 30/1000 --- Train Loss: 0.5835001035710169 --- Val Loss: 0.46008017483499747 --- Train Acc: 0.88 --- Val Acc: 0.84\n",
      "Epoch 40/1000 --- Train Loss: 0.8809123291397629 --- Val Loss: 0.7896499885433507 --- Train Acc: 0.72 --- Val Acc: 0.72\n",
      "Epoch 50/1000 --- Train Loss: 1.6772141988718272 --- Val Loss: 1.2559712375145187 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 60/1000 --- Train Loss: 1.7508617305693113 --- Val Loss: 1.5362985731538765 --- Train Acc: 0.41 --- Val Acc: 0.46\n",
      "Epoch 70/1000 --- Train Loss: 1.8728721701633193 --- Val Loss: 1.8374234733434358 --- Train Acc: 0.30 --- Val Acc: 0.34\n",
      "Epoch 80/1000 --- Train Loss: 2.209296842718724 --- Val Loss: 2.275676327049414 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 90/1000 --- Train Loss: 2.1248908433257516 --- Val Loss: 2.1131756004258455 --- Train Acc: 0.17 --- Val Acc: 0.21\n",
      "Epoch 100/1000 --- Train Loss: 2.260954349389411 --- Val Loss: 2.2570404361663567 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.310953230228709 --- Val Loss: 2.2983826205095776 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3108898253311554 --- Val Loss: 2.2977196495854475 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.298863197055025 --- Val Loss: 2.298108660559207 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.30117346797277 --- Val Loss: 2.2973178362089897 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3009862581187317 --- Val Loss: 2.296916322339282 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3009661459763144 --- Val Loss: 2.296536064768543 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 170/1000 --- Train Loss: 2.3009313367109314 --- Val Loss: 2.297693465681572 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3009177839421167 --- Val Loss: 2.297075980153106 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.30271135900108 --- Val Loss: 2.2975002319304316 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.3009379422821454 --- Val Loss: 2.298195727531903 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.300953481574311 --- Val Loss: 2.296471828724529 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.300941379852613 --- Val Loss: 2.2982898843824024 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.313016692209793 --- Val Loss: 2.297530477171048 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.300913310038245 --- Val Loss: 2.297716594286915 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.300951895197969 --- Val Loss: 2.2979222711730722 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.300932899702762 --- Val Loss: 2.2974288389978583 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 270/1000 --- Train Loss: 2.3009472919870193 --- Val Loss: 2.2973065008732942 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.300934701424807 --- Val Loss: 2.297737647663254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.300921099699788 --- Val Loss: 2.297279472428036 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3009134739965016 --- Val Loss: 2.2970270827362076 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3009180183484466 --- Val Loss: 2.29793464549308 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.298900547890098 --- Val Loss: 2.297638299184465 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 330/1000 --- Train Loss: 2.300931485435679 --- Val Loss: 2.2985761414849737 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.301001276607487 --- Val Loss: 2.2966960623671144 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3009889399633474 --- Val Loss: 2.2973973918602297 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 360/1000 --- Train Loss: 2.3009638101052574 --- Val Loss: 2.2983962599425243 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3130384959726484 --- Val Loss: 2.2980052639549995 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.301033209329296 --- Val Loss: 2.2976581805170726 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3009653364201403 --- Val Loss: 2.2975629839724028 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3009625752547467 --- Val Loss: 2.2976487796290592 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.30091340708154 --- Val Loss: 2.2968861670761607 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3009955395073587 --- Val Loss: 2.297883383421162 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.3009582136561795 --- Val Loss: 2.2973442701289772 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3009105867090094 --- Val Loss: 2.2967475499307417 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 450/1000 --- Train Loss: 2.3009590693579995 --- Val Loss: 2.2976345422151274 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3009062315008526 --- Val Loss: 2.2979601995066243 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3009328832399527 --- Val Loss: 2.2974429183890677 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.300921683198684 --- Val Loss: 2.2978363039815353 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 490/1000 --- Train Loss: 2.3010117086573305 --- Val Loss: 2.2983590317818683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.3009550698248002 --- Val Loss: 2.297715134795542 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.3009396215723728 --- Val Loss: 2.2979803109992445 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 520/1000 --- Train Loss: 2.3009487215634703 --- Val Loss: 2.2981470429881647 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 530/1000 --- Train Loss: 2.300921443734486 --- Val Loss: 2.2981546400923567 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3009398747107803 --- Val Loss: 2.297511920777055 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.300956271188749 --- Val Loss: 2.296481596375842 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 560/1000 --- Train Loss: 2.3009513113891256 --- Val Loss: 2.298113029648751 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.300904177232254 --- Val Loss: 2.29781123400925 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.300902059874445 --- Val Loss: 2.298079800439229 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.300932868455381 --- Val Loss: 2.2967400705992422 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.300939789445803 --- Val Loss: 2.297337460572678 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.3009561225721282 --- Val Loss: 2.2973370695557587 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.300991081940134 --- Val Loss: 2.297490023147301 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.3009914938987523 --- Val Loss: 2.29867801092359 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.300949316492304 --- Val Loss: 2.29688509708272 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 650/1000 --- Train Loss: 2.30100399645512 --- Val Loss: 2.296943007789764 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 660/1000 --- Train Loss: 2.3009086853377574 --- Val Loss: 2.297839501320914 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3009100373886167 --- Val Loss: 2.2971382628982298 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.300907820892586 --- Val Loss: 2.297166462050309 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.300899810443929 --- Val Loss: 2.2972549526858437 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.300991386500565 --- Val Loss: 2.2963389209056473 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 710/1000 --- Train Loss: 2.3009047032757315 --- Val Loss: 2.297858662381733 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3009558057308874 --- Val Loss: 2.29751044697387 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3009346036136575 --- Val Loss: 2.2976438803205395 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.301070626327392 --- Val Loss: 2.2969228525688696 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.300974154914746 --- Val Loss: 2.2982892393898693 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.300937899093978 --- Val Loss: 2.297535511966879 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 770/1000 --- Train Loss: 2.3009208671162797 --- Val Loss: 2.2976209833831174 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.3008924588434456 --- Val Loss: 2.2974050065307434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.3009845047114137 --- Val Loss: 2.2976338274856527 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 800/1000 --- Train Loss: 2.300913109051609 --- Val Loss: 2.2967684289324923 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 810/1000 --- Train Loss: 2.3009413384718203 --- Val Loss: 2.297982807150421 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3009321714927586 --- Val Loss: 2.2969612028718207 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 830/1000 --- Train Loss: 2.3009599189392365 --- Val Loss: 2.2973090687863906 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3009221095814842 --- Val Loss: 2.2975232067555065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.300921912555487 --- Val Loss: 2.297548135797392 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.3010335000671134 --- Val Loss: 2.2979937151085563 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.300998152429528 --- Val Loss: 2.297327319144117 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 880/1000 --- Train Loss: 2.3009056801992616 --- Val Loss: 2.297376579380831 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 890/1000 --- Train Loss: 2.2989050737803343 --- Val Loss: 2.2974422921438107 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3009742537978566 --- Val Loss: 2.297191717415453 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.300970573351162 --- Val Loss: 2.2978484724150663 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.3009466161318777 --- Val Loss: 2.2971486728937784 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.3009233541843708 --- Val Loss: 2.298214125416691 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.300942632056455 --- Val Loss: 2.2979722249899086 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3009509788505387 --- Val Loss: 2.2973055298253744 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.298875915737164 --- Val Loss: 2.2975817139702808 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 970/1000 --- Train Loss: 2.300910328523414 --- Val Loss: 2.298097911418154 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3009906097617963 --- Val Loss: 2.2975532270388785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3009018260350036 --- Val Loss: 2.2976874774278104 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302526906686371 --- Val Loss: 2.3025518511520136 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.301978903883389 --- Val Loss: 2.3022282233728135 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.3005023693324116 --- Val Loss: 2.3008479333254743 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.292958570218565 --- Val Loss: 2.293076767326175 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.262117153819159 --- Val Loss: 2.260466617678956 --- Train Acc: 0.29 --- Val Acc: 0.31\n",
      "Epoch 50/500 --- Train Loss: 2.1502582621758592 --- Val Loss: 2.140462195338187 --- Train Acc: 0.35 --- Val Acc: 0.37\n",
      "Epoch 60/500 --- Train Loss: 1.8209358479213853 --- Val Loss: 1.7885714629071912 --- Train Acc: 0.51 --- Val Acc: 0.52\n",
      "Epoch 70/500 --- Train Loss: 1.2938711990349245 --- Val Loss: 1.2300380559675053 --- Train Acc: 0.73 --- Val Acc: 0.75\n",
      "Epoch 80/500 --- Train Loss: 0.9034576957134695 --- Val Loss: 0.8046382558148106 --- Train Acc: 0.84 --- Val Acc: 0.83\n",
      "Epoch 90/500 --- Train Loss: 0.7040731762239106 --- Val Loss: 0.5399072920363669 --- Train Acc: 0.89 --- Val Acc: 0.88\n",
      "Epoch 100/500 --- Train Loss: 0.5453943118642831 --- Val Loss: 0.3967108665383717 --- Train Acc: 0.91 --- Val Acc: 0.89\n",
      "Epoch 110/500 --- Train Loss: 0.47513153182279916 --- Val Loss: 0.3096307803958356 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 120/500 --- Train Loss: 0.411120348847773 --- Val Loss: 0.24898563333693793 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 130/500 --- Train Loss: 0.3968231236038502 --- Val Loss: 0.20909081668040358 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 140/500 --- Train Loss: 0.3701736846324492 --- Val Loss: 0.1795514831331852 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 150/500 --- Train Loss: 0.34573206664123635 --- Val Loss: 0.15603305173086546 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 160/500 --- Train Loss: 0.3270611667824682 --- Val Loss: 0.13453704242374692 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 170/500 --- Train Loss: 0.37479466879322304 --- Val Loss: 0.11552994302434554 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 180/500 --- Train Loss: 0.29482190018467264 --- Val Loss: 0.1109481978979421 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 190/500 --- Train Loss: 0.3411837428244217 --- Val Loss: 0.09073170937302594 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 200/500 --- Train Loss: 0.3059127890915407 --- Val Loss: 0.08259467227808961 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 210/500 --- Train Loss: 0.29745649144902203 --- Val Loss: 0.08257845559546735 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 220/500 --- Train Loss: 0.3086468419770901 --- Val Loss: 0.06799913298910015 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 230/500 --- Train Loss: 0.3638914363551733 --- Val Loss: 0.06491525059290187 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 240/500 --- Train Loss: 0.32247294966835643 --- Val Loss: 0.060641768395267207 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 250/500 --- Train Loss: 0.2931212448982924 --- Val Loss: 0.0622591507207879 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 260/500 --- Train Loss: 0.32861872218506416 --- Val Loss: 0.0466298764583375 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 270/500 --- Train Loss: 0.29133016809007173 --- Val Loss: 0.06557990165203816 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 280/500 --- Train Loss: 0.31592690490701897 --- Val Loss: 0.04601842174704226 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 290/500 --- Train Loss: 0.27059008943807017 --- Val Loss: 0.04472801654161403 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 300/500 --- Train Loss: 0.3789351517794367 --- Val Loss: 0.05084986977995003 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 310/500 --- Train Loss: 0.3067741677898673 --- Val Loss: 0.04596261993723374 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 320/500 --- Train Loss: 0.3535239132039332 --- Val Loss: 0.04907175081147382 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 330/500 --- Train Loss: 0.43183218220240266 --- Val Loss: 0.06538912033780865 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 340/500 --- Train Loss: 0.37400060027846416 --- Val Loss: 0.052933434049042584 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 350/500 --- Train Loss: 0.44063899618813135 --- Val Loss: 0.09542802368877923 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 360/500 --- Train Loss: 0.4693429460439701 --- Val Loss: 0.07880397967864303 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 370/500 --- Train Loss: 0.5984084401981835 --- Val Loss: 0.09322062143181147 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 380/500 --- Train Loss: 1.0023625071638684 --- Val Loss: 0.45244174189504 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 390/500 --- Train Loss: 0.5947106336452417 --- Val Loss: 0.12944401784556794 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 400/500 --- Train Loss: 0.6084098670863674 --- Val Loss: 0.13208686027151406 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 410/500 --- Train Loss: 0.971088184392748 --- Val Loss: 0.28961087031289434 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 420/500 --- Train Loss: 1.0832871693697743 --- Val Loss: 0.2926877673094404 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 430/500 --- Train Loss: 1.0954031232706316 --- Val Loss: 0.3214142497594526 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 440/500 --- Train Loss: 2.0459604402174616 --- Val Loss: 0.9764198135126778 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 450/500 --- Train Loss: 0.5929830319873766 --- Val Loss: 0.06414099707384904 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 460/500 --- Train Loss: 1.4016152301638394 --- Val Loss: 0.6877549287236069 --- Train Acc: 0.89 --- Val Acc: 0.91\n",
      "Epoch 470/500 --- Train Loss: 0.8270613769349037 --- Val Loss: 0.049663098301868186 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 480/500 --- Train Loss: 1.0581077542671689 --- Val Loss: 0.13908235922702636 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 490/500 --- Train Loss: 0.9529473721256403 --- Val Loss: 0.1906742550040357 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.9277777777777778\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.558653084698243 --- Val Loss: 2.310765893605998 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302485594177745 --- Val Loss: 2.3023787487389127 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.301772013537767 --- Val Loss: 2.301134406827238 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.301428079321185 --- Val Loss: 2.300384420843406 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.301253367091234 --- Val Loss: 2.299925931122994 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.3011708637914707 --- Val Loss: 2.299667926051119 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/500 --- Train Loss: 2.301128388128477 --- Val Loss: 2.29946834995315 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3011079636871203 --- Val Loss: 2.2993621657584047 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3010959216514872 --- Val Loss: 2.2992328055026796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.301090931125964 --- Val Loss: 2.299173872514016 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3010884773653753 --- Val Loss: 2.299166622353942 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3010867810750106 --- Val Loss: 2.2991147871341475 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.301085572977698 --- Val Loss: 2.2990588727692716 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.3010854658653837 --- Val Loss: 2.2990516848129965 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.301085340439316 --- Val Loss: 2.299105461244191 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3010852579487446 --- Val Loss: 2.299031677753801 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.301085017751158 --- Val Loss: 2.2990267996683653 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/500 --- Train Loss: 2.3010848260748555 --- Val Loss: 2.299030696860871 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/500 --- Train Loss: 2.3010847736344835 --- Val Loss: 2.2989977921514773 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.30108464310949 --- Val Loss: 2.299015256097193 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/500 --- Train Loss: 2.3010846625903563 --- Val Loss: 2.299017492699495 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.3010845278967853 --- Val Loss: 2.2990007558295265 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3010844293013766 --- Val Loss: 2.298979437470668 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.3010843654431357 --- Val Loss: 2.299039800297368 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.3010841809746845 --- Val Loss: 2.299030125787642 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.3010841937544475 --- Val Loss: 2.299045627711285 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.3010839980656015 --- Val Loss: 2.2990117703889386 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.3010838743823743 --- Val Loss: 2.2990250065760174 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.3010837604395538 --- Val Loss: 2.2990183775234083 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.3010835460383627 --- Val Loss: 2.2990436942185064 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.301083410812158 --- Val Loss: 2.2990208652217796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.3010832303514994 --- Val Loss: 2.299017557507811 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.301083211912685 --- Val Loss: 2.299044441969539 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: 2.3010828738257625 --- Val Loss: 2.2990132578493445 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3010827007086077 --- Val Loss: 2.2990415524964383 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.3010824388204214 --- Val Loss: 2.2989995439600763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: 2.3010821525603267 --- Val Loss: 2.299025951145306 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.3010819986972932 --- Val Loss: 2.299059772047861 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.3010816916279193 --- Val Loss: 2.2990110093788783 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.3010813462926007 --- Val Loss: 2.299027548756217 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.301080943848594 --- Val Loss: 2.2989728643336416 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: 2.3010805421358183 --- Val Loss: 2.299018586630666 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.3010800120443817 --- Val Loss: 2.2990760701958988 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.30107955774741 --- Val Loss: 2.29898209999726 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.3010791667730004 --- Val Loss: 2.2989811417218986 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.3010782147323514 --- Val Loss: 2.29903600567404 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.301077707768909 --- Val Loss: 2.299003950560114 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: 2.3010767704450132 --- Val Loss: 2.2990178868941973 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: 2.301075951851791 --- Val Loss: 2.299016640974325 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.3010750315806567 --- Val Loss: 2.298985802655977 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.301073547885436 --- Val Loss: 2.2990151926426257 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 1.3535577567515735 --- Val Loss: 1.301917617356367 --- Train Acc: 0.60 --- Val Acc: 0.58\n",
      "Epoch 10/100 --- Train Loss: 2.304318855403745 --- Val Loss: 2.309308228675639 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.305433313540744 --- Val Loss: 2.299091188351344 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3093627402212973 --- Val Loss: 2.302842031527007 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3042988327190095 --- Val Loss: 2.2991436204943523 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.3144866592005964 --- Val Loss: 2.2989312526400902 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3063074054852897 --- Val Loss: 2.30097981940824 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3064758150757356 --- Val Loss: 2.307785161355592 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.311387590287514 --- Val Loss: 2.307940319724163 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.304562824473739 --- Val Loss: 2.2997408060765467 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.307846925449628 --- Val Loss: 2.308599831928564 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: 2.3126656761415 --- Val Loss: 2.3221430644717165 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/500 --- Train Loss: 2.313151005301703 --- Val Loss: 2.3083619049467043 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.308254763057268 --- Val Loss: 2.3174080561801564 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.31241211426111 --- Val Loss: 2.316958916796222 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/500 --- Train Loss: 2.3289104056364276 --- Val Loss: 2.3203620406231775 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.317430211006766 --- Val Loss: 2.3127005434905334 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: 2.310040242335666 --- Val Loss: 2.3123906049066396 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.3073099760690416 --- Val Loss: 2.3134867511664794 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: 2.3131793309944904 --- Val Loss: 2.306522414016731 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3166206389829758 --- Val Loss: 2.3155955746235186 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: 2.30908152517267 --- Val Loss: 2.303608928644491 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: 2.3158504345944952 --- Val Loss: 2.3294986956692187 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.315925036328178 --- Val Loss: 2.316524274938793 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3198320730633504 --- Val Loss: 2.3162359718648866 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.306204051923323 --- Val Loss: 2.3111831690503366 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: 2.3089353181102674 --- Val Loss: 2.3095253636822237 --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 170/500 --- Train Loss: 2.3133940161340623 --- Val Loss: 2.3180640363507536 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 180/500 --- Train Loss: 2.3074924735818074 --- Val Loss: 2.3017032781902373 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.3216588805208507 --- Val Loss: 2.3377635213766483 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: 2.310666774644831 --- Val Loss: 2.3169954301290403 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3121604030878493 --- Val Loss: 2.3220435190051933 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 220/500 --- Train Loss: 2.3109703871766656 --- Val Loss: 2.3055948947968847 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.3122190849023454 --- Val Loss: 2.322608243416174 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 240/500 --- Train Loss: 2.306777385780402 --- Val Loss: 2.3100430734736053 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 250/500 --- Train Loss: 2.3089377879193838 --- Val Loss: 2.3138847948697934 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 260/500 --- Train Loss: 2.314154533785585 --- Val Loss: 2.330154332010174 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 270/500 --- Train Loss: 2.3153279352306755 --- Val Loss: 2.323882600820275 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: 2.3231942045154 --- Val Loss: 2.3149734775503745 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.317712042648707 --- Val Loss: 2.3138630413 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: 2.3190966666890342 --- Val Loss: 2.326373023299379 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: 2.324451507961057 --- Val Loss: 2.3241770144604925 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: 2.3068739402153198 --- Val Loss: 2.301670725300154 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3221470151552976 --- Val Loss: 2.3379262932158915 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.311428732682658 --- Val Loss: 2.3110818144210152 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.312714770458515 --- Val Loss: 2.3089133430245448 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: 2.325192210430295 --- Val Loss: 2.330976223087449 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.310439723048147 --- Val Loss: 2.314804148693406 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: 2.311893322771993 --- Val Loss: 2.320214017273359 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: 2.314306632576996 --- Val Loss: 2.3087207644434735 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: 2.3143769749880456 --- Val Loss: 2.3197393447174335 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 410/500 --- Train Loss: 2.30374434483464 --- Val Loss: 2.304710625208447 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 420/500 --- Train Loss: 2.311467042129436 --- Val Loss: 2.3078209160608485 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.3121142750555252 --- Val Loss: 2.311287170783942 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.3188195765545068 --- Val Loss: 2.314553311258272 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.31085636231234 --- Val Loss: 2.3150528514298236 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: 2.3085192509054906 --- Val Loss: 2.3198467586255327 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 470/500 --- Train Loss: 2.3104168139858596 --- Val Loss: 2.311662758203702 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.3145970143427963 --- Val Loss: 2.3127963100597624 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.3124267208447242 --- Val Loss: 2.310022714887891 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.12777777777777777\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 3.0140924006157856 --- Val Loss: 2.8394976249533816 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3567093077452452 --- Val Loss: 2.3341307212884357 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.322757668223267 --- Val Loss: 2.30767427984912 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: 2.3252859730108404 --- Val Loss: 2.3216491271011472 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3388693701635694 --- Val Loss: 2.3626355610444216 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/100 --- Train Loss: 2.3296595679078056 --- Val Loss: 2.316369633683837 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.3173218062452023 --- Val Loss: 2.316543617701126 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.312288684398315 --- Val Loss: 2.320670486429323 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.331641428519821 --- Val Loss: 2.3030858295841914 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.3259300623553694 --- Val Loss: 2.3174896177097524 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 1.923990047286862 --- Val Loss: 1.9438492289925833 --- Train Acc: 0.31 --- Val Acc: 0.29\n",
      "Epoch 10/1000 --- Train Loss: 2.2830156788163727 --- Val Loss: 2.2666531064955393 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 20/1000 --- Train Loss: 2.311157989489715 --- Val Loss: 2.3023651918265156 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.316287278312946 --- Val Loss: 2.3035885300389083 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/1000 --- Train Loss: 2.2975301067188023 --- Val Loss: 2.302056166335434 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.2981298814926596 --- Val Loss: 2.3000405852384223 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.3014473173987358 --- Val Loss: 2.3021916389133206 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: 2.302790717063921 --- Val Loss: 2.306063378661122 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: 2.299360035566055 --- Val Loss: 2.303031186872245 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.302006745330923 --- Val Loss: 2.310629732482613 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.2965801841799247 --- Val Loss: 2.3097662939340378 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3026338152338885 --- Val Loss: 2.3028404698493103 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3024128957758005 --- Val Loss: 2.301068260573312 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.3021400052056062 --- Val Loss: 2.3048152440395793 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.3020844241141547 --- Val Loss: 2.3027661509715998 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3032470157471607 --- Val Loss: 2.3020925810907347 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.3028620067931844 --- Val Loss: 2.3069330235063212 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: 2.3025571849790962 --- Val Loss: 2.3019602985235856 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: 2.3021693976798603 --- Val Loss: 2.300369727609632 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.304041157170586 --- Val Loss: 2.29822485050719 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.3025235390635275 --- Val Loss: 2.303085429136741 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.3022902345206284 --- Val Loss: 2.3051939763815565 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: 2.3031029314157787 --- Val Loss: 2.305087271061883 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.3034093968648883 --- Val Loss: 2.3104164656785344 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: 2.3035514027225807 --- Val Loss: 2.304099413856719 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: 2.3023420842444278 --- Val Loss: 2.30543243482179 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.3016780562843424 --- Val Loss: 2.302080030313019 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.30329411577976 --- Val Loss: 2.301914284494217 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.3032812253717676 --- Val Loss: 2.3082390052510044 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.302035765643277 --- Val Loss: 2.305392282867552 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: 2.302565563866403 --- Val Loss: 2.3052953244147325 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.3033776845499307 --- Val Loss: 2.3078510116193978 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: 2.301840112535388 --- Val Loss: 2.3078054075471757 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: 2.3034830104067807 --- Val Loss: 2.3049093530468223 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: 2.3017996351434853 --- Val Loss: 2.3013530976222443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3015545084986773 --- Val Loss: 2.302491571290176 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.302200534862563 --- Val Loss: 2.3059577647513785 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.302873315008996 --- Val Loss: 2.305941608536129 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3021748641616204 --- Val Loss: 2.3032447231729236 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.3037532588574337 --- Val Loss: 2.301509395963957 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.3028134616378657 --- Val Loss: 2.3059125274521923 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3016091961978242 --- Val Loss: 2.305823387542258 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: 2.303863118244021 --- Val Loss: 2.3016469217363245 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/1000 --- Train Loss: 2.30343022728772 --- Val Loss: 2.3089271635678825 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3029181622284898 --- Val Loss: 2.3021979496215246 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.3024332294343934 --- Val Loss: 2.308438266273924 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: 2.301582961670353 --- Val Loss: 2.3048734829279955 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.3026062533029528 --- Val Loss: 2.302130065263441 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.3042288812865674 --- Val Loss: 2.301478895602671 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.302719969138863 --- Val Loss: 2.3061703536000038 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: 2.3023230675537936 --- Val Loss: 2.3026210380650403 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.302894854905443 --- Val Loss: 2.302537792522683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.304004859884797 --- Val Loss: 2.2978743612374526 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.302908549576294 --- Val Loss: 2.3009298491711405 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.302572939100537 --- Val Loss: 2.305036632593471 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.303301109231989 --- Val Loss: 2.310211515507988 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: 2.3016514108653103 --- Val Loss: 2.3058611073315443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3022021733472284 --- Val Loss: 2.3059106484733527 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.302244716249101 --- Val Loss: 2.3045705278960185 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3016842386235505 --- Val Loss: 2.298906905098527 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.3027676099519794 --- Val Loss: 2.3052608730092987 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: 2.3033384609348593 --- Val Loss: 2.304245565418042 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: 2.3019131618377755 --- Val Loss: 2.304130340849055 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.302002943373641 --- Val Loss: 2.3004146974685016 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3017479358064743 --- Val Loss: 2.3040827506287243 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: 2.301920827313197 --- Val Loss: 2.3044412544389394 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3022994316924663 --- Val Loss: 2.304267594327226 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3025097756823496 --- Val Loss: 2.302508427637741 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.3029399026351998 --- Val Loss: 2.3054886250227273 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: 2.304505276500685 --- Val Loss: 2.309549640340105 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3014646042936846 --- Val Loss: 2.302676170061878 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 710/1000 --- Train Loss: 2.3023063590824617 --- Val Loss: 2.3054578662864147 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 720/1000 --- Train Loss: 2.301538475349474 --- Val Loss: 2.3061169045838463 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.3034880001538447 --- Val Loss: 2.3051053548385982 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.301794503760716 --- Val Loss: 2.3052428175170525 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: 2.3030247503040933 --- Val Loss: 2.30363271764035 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.3014862892113928 --- Val Loss: 2.3008587602492634 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.301820197766742 --- Val Loss: 2.3029168595751033 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: 2.303166821941545 --- Val Loss: 2.30186540920065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.30156721118536 --- Val Loss: 2.3018694609455106 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.302201244046224 --- Val Loss: 2.306210847466265 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: 2.302385267809131 --- Val Loss: 2.304543547559336 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3022634791566445 --- Val Loss: 2.3074394900944957 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: 2.3034639297078745 --- Val Loss: 2.3026083293954343 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: 2.3014457464962126 --- Val Loss: 2.3021957005929625 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.304075244511229 --- Val Loss: 2.3037781562153477 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3018495846570284 --- Val Loss: 2.3015329571721646 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.303628385974151 --- Val Loss: 2.307304411333447 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3022634303514864 --- Val Loss: 2.3038593535814447 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.304759465672323 --- Val Loss: 2.303192529134952 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: 2.3023578152869826 --- Val Loss: 2.3070367142491905 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.302921204388645 --- Val Loss: 2.302106862617549 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.3023658654081536 --- Val Loss: 2.307491869865628 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.315016844290872 --- Val Loss: 2.307261915744826 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3031424266514953 --- Val Loss: 2.3049798029811006 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.30458476250352 --- Val Loss: 2.3071274540196027 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.302561271493817 --- Val Loss: 2.303498686556102 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: 2.3017618141102565 --- Val Loss: 2.3018399248862838 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: 2.3026738632475823 --- Val Loss: 2.305429970508966 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3021619846078427 --- Val Loss: 2.301234475159673 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09722222222222222\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.3018414658780753 --- Val Loss: 2.3008386390957294 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3016328337403533 --- Val Loss: 2.2993607858768317 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 20/500 --- Train Loss: 1.918390677696297 --- Val Loss: 1.9250259287756177 --- Train Acc: 0.21 --- Val Acc: 0.18\n",
      "Epoch 30/500 --- Train Loss: 0.09408487146971208 --- Val Loss: 0.05197493121398385 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/500 --- Train Loss: 0.11753486772548076 --- Val Loss: 0.01631708100285634 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 50/500 --- Train Loss: 3.3187678110366052 --- Val Loss: 3.0550951332580207 --- Train Acc: 0.71 --- Val Acc: 0.73\n",
      "Epoch 60/500 --- Train Loss: 0.8593462182689392 --- Val Loss: 0.5579982625014274 --- Train Acc: 0.77 --- Val Acc: 0.78\n",
      "Epoch 70/500 --- Train Loss: 2.023470032140475 --- Val Loss: 1.799898487397491 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 80/500 --- Train Loss: 2.3137532588441574 --- Val Loss: 2.2995739829503408 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3016931680349897 --- Val Loss: 2.2970542047231133 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 100/500 --- Train Loss: 2.301622126236371 --- Val Loss: 2.2997232803467482 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.301662223962331 --- Val Loss: 2.298225869684668 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.30168322955918 --- Val Loss: 2.2986551918412608 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 130/500 --- Train Loss: 2.3017084429070747 --- Val Loss: 2.2993452323033043 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3016234073018116 --- Val Loss: 2.297455760932887 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "\n",
      "Best Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Best Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomSearch(NeuralNetwork, param_grid, n_iter=20)\n",
    "best_params, best_accuracy = random_search.search(X, y)\n",
    "print(f\"\\nBest Params: {best_params}, Best Accuracy: {best_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
