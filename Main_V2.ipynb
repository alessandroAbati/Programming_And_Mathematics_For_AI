{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "236dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, l1=0.0, l2=0.0):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # Initialise (weights) velocity terms for momentum optimization\n",
    "        self.velocity_biases = np.zeros_like(self.biases) # Initialise (biases) velocity terms for momentum optimization\n",
    "        self.l1 = l1 # L1 regularization coefficient (default 0.0).\n",
    "        self.l2 = l2 # L2 regularization coefficient (default 0.0).\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        '''\n",
    "        # Regularization terms\n",
    "        l1_reg = self.l1 * np.sign(self.weights)\n",
    "        l2_reg = self.l2 * self.weights\n",
    "\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient) + l1_reg + l2_reg\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if optimizer == 'GD':\n",
    "            # Update weights and biases\n",
    "            self.weights += learning_rate * weights_gradient\n",
    "            self.biases += learning_rate * biases_gradient\n",
    "        elif optimizer == 'Momentum':\n",
    "            # Momentum update for weights and biases\n",
    "            self.velocity_weights = momentum * self.velocity_weights + learning_rate * weights_gradient\n",
    "            self.velocity_biases = momentum * self.velocity_biases + learning_rate * biases_gradient\n",
    "\n",
    "            # Update weights and biases using velocity\n",
    "            self.weights += self.velocity_weights\n",
    "            self.biases += self.velocity_biases\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for printing/plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for printing/plotting\n",
    "        self.accuracy_history = [] #placeholder to store the (train) accuracy for printing/plotting\n",
    "        self.val_accuracy_history = [] #placeholder to store the accuracy calculated on the validation set for printing/plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Computes the accuracy of predictions by comparing them with the true labels. \n",
    "        Accuracy is computed as the proportion of correct predictions to the total number of predictions.\n",
    "        '''\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate, optimizer, momentum)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, optimizer='GD', momentum=0.9, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate, optimizer, momentum) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_predictions = self.predict(X_train)\n",
    "            train_accuracy = self.compute_accuracy(train_predictions, np.argmax(y_train, axis=1))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_predictions = self.predict(X_val)\n",
    "            val_accuracy = self.compute_accuracy(val_predictions, np.argmax(y_val, axis=1))\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.plot(self.accuracy_history, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracy_history, label='Val Accuracy')\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 2.301149184562769 --- Val Loss: 2.300736748531308 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 0.26111784806899657 --- Val Loss: 0.1405624934320048 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 20/1000 --- Train Loss: 0.13999185954586968 --- Val Loss: 0.058701739610862155 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 30/1000 --- Train Loss: 0.09706131062311331 --- Val Loss: 0.03645762801623091 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 40/1000 --- Train Loss: 0.08681233868346758 --- Val Loss: 0.024422495935272907 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/1000 --- Train Loss: 0.08007441669030321 --- Val Loss: 0.017889867157877198 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 0.056523796581669034 --- Val Loss: 0.013773008041217718 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/1000 --- Train Loss: 0.039384841323872995 --- Val Loss: 0.009640593151864328 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/1000 --- Train Loss: 0.032169131504388576 --- Val Loss: 0.008009102546139492 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/1000 --- Train Loss: 0.0349203908158825 --- Val Loss: 0.006788541701033613 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/1000 --- Train Loss: 0.042492303278935606 --- Val Loss: 0.00582952800915414 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/1000 --- Train Loss: 0.036221405620722444 --- Val Loss: 0.00478179600254086 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/1000 --- Train Loss: 0.0255218128153392 --- Val Loss: 0.0034084338861329025 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/1000 --- Train Loss: 0.028490599852604395 --- Val Loss: 0.003300707941356346 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/1000 --- Train Loss: 0.035766017758400166 --- Val Loss: 0.0030742911752675455 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/1000 --- Train Loss: 0.02918846947466343 --- Val Loss: 0.0025595617357253024 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/1000 --- Train Loss: 0.03370229648194645 --- Val Loss: 0.0024418008120364967 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/1000 --- Train Loss: 0.03959814144718635 --- Val Loss: 0.0020697112434474745 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/1000 --- Train Loss: 0.02461502699672136 --- Val Loss: 0.002384183764952392 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/1000 --- Train Loss: 0.025569685499665443 --- Val Loss: 0.0020363779303753885 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/1000 --- Train Loss: 0.030912817799829448 --- Val Loss: 0.0019922253342707042 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/1000 --- Train Loss: 0.022192653382126233 --- Val Loss: 0.0018564808830204185 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/1000 --- Train Loss: 0.027160926631686004 --- Val Loss: 0.0016426977935421086 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/1000 --- Train Loss: 0.01921920550899374 --- Val Loss: 0.0014488754946857293 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.0182755531369718 --- Val Loss: 0.0013152238316496815 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.020606535041612976 --- Val Loss: 0.0012504898333053716 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.02315053677004692 --- Val Loss: 0.0012364171343871755 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.01737483618925521 --- Val Loss: 0.0011858283859603278 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.01603253002161945 --- Val Loss: 0.0011304735917028802 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.021276312936485037 --- Val Loss: 0.0010255579786339509 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.01554245894322484 --- Val Loss: 0.0009345691162941107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.027453803290041483 --- Val Loss: 0.000893140478753126 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.020212041653043702 --- Val Loss: 0.0007201949832149498 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.021835269921478762 --- Val Loss: 0.0007966317972637107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.01695814467219877 --- Val Loss: 0.0007627381988598538 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.02208793770394737 --- Val Loss: 0.0007881838792647893 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.01700434426269486 --- Val Loss: 0.0006929385464171313 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.02035076605183943 --- Val Loss: 0.000711565101804139 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.013976843782657499 --- Val Loss: 0.0005683391177353212 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.015516570732259012 --- Val Loss: 0.0005321693009274251 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.015814527438950475 --- Val Loss: 0.000612054627246974 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.014072468228879033 --- Val Loss: 0.0007043021720958765 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.01417895364049474 --- Val Loss: 0.0006287389266456734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.022316035820126697 --- Val Loss: 0.0005820773600488212 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.01084751310393034 --- Val Loss: 0.0005453577991555237 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.012369622186576555 --- Val Loss: 0.00048157079876443715 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.014947345394832634 --- Val Loss: 0.0005412261459339268 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.015691353126141883 --- Val Loss: 0.0004820761300242544 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.012681236488875433 --- Val Loss: 0.0004856422920563609 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.009331108804264129 --- Val Loss: 0.00048691175137224467 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.013806523509911458 --- Val Loss: 0.00048271667879163066 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.014629636098894077 --- Val Loss: 0.0004894845443558864 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.024888120425858595 --- Val Loss: 0.00041555518219004643 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.02430165906882027 --- Val Loss: 0.0004575886063918471 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.011571040763083806 --- Val Loss: 0.0004042786397581985 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.013173749654870493 --- Val Loss: 0.0004463287814565262 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.014736544295435727 --- Val Loss: 0.00038922844072265404 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.010683906514887143 --- Val Loss: 0.000462244034848092 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.00994539856517556 --- Val Loss: 0.0003997171403124016 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.010126718733335495 --- Val Loss: 0.0004029498433246194 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.005146152614553908 --- Val Loss: 0.00044567130565599235 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.011156940742265298 --- Val Loss: 0.000386328224529158 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.009955759787871213 --- Val Loss: 0.00041437498853785445 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.012122993318478733 --- Val Loss: 0.0003874100908162582 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.011293499854289558 --- Val Loss: 0.00035118542447613106 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.008416323597272634 --- Val Loss: 0.00034211841331357773 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.006898565018974534 --- Val Loss: 0.0003320754150354857 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.005204248861563753 --- Val Loss: 0.00033331598890143734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.013042389837679453 --- Val Loss: 0.0003679651899866131 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.015799983616968962 --- Val Loss: 0.00038690548989845253 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.012376713579528005 --- Val Loss: 0.0003176544425114073 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.01006731557853605 --- Val Loss: 0.0003439768192068196 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.010966206173191224 --- Val Loss: 0.00034181410170681324 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.005194238538013531 --- Val Loss: 0.00030105166805206336 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.00920874788207156 --- Val Loss: 0.00031239944375858146 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.009907853399309626 --- Val Loss: 0.00029689495397033433 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.007348592860860987 --- Val Loss: 0.00028732322024790615 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.008836156982651212 --- Val Loss: 0.00028739165424585943 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.008245290156513364 --- Val Loss: 0.0003285461006837708 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.0085127165857514 --- Val Loss: 0.0002993033209484686 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.01425201687337106 --- Val Loss: 0.00025203722286441126 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.0065115759109848235 --- Val Loss: 0.00022646787932803532 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.011673590410681455 --- Val Loss: 0.0002269391781153763 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.006269253589417593 --- Val Loss: 0.00026732612802629597 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.012757551588915086 --- Val Loss: 0.000256759948281947 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.011999255086971717 --- Val Loss: 0.0002478251057420131 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.005748036148269226 --- Val Loss: 0.00023849777941705705 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.01727894015770537 --- Val Loss: 0.00022080155951846142 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.004454980958639458 --- Val Loss: 0.00020740362667686306 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.005471773613507106 --- Val Loss: 0.00017236102774908563 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.00802460628742304 --- Val Loss: 0.00025641350730931986 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.009458112736762886 --- Val Loss: 0.0002138910239676511 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.0061986040772002045 --- Val Loss: 0.00021482980613783017 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.01702589359968339 --- Val Loss: 0.00024462389194513366 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.009919225905513022 --- Val Loss: 0.00026851570842986586 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.009878293924027727 --- Val Loss: 0.00018859741570415416 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.00946001869738558 --- Val Loss: 0.0002490753695582819 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.00427419105849347 --- Val Loss: 0.00022621931217192673 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.010842389729617504 --- Val Loss: 0.0001823595843642654 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.0113270884820269 --- Val Loss: 0.0001943961424473829 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 32))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "#network.add_layer(Layer(128, 64, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "#network.add_layer(Layer(64, 32, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=1000, learning_rate=0.1, optimizer='GD', momentum=0.5, batch_size=32)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 --- Train Loss: 0.692580580099927 --- Val Loss: 0.6925675160760685 --- Train Acc: 0.62 --- Val Acc: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10000 --- Train Loss: 0.6917478463225849 --- Val Loss: 0.6916211815475471 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 20/10000 --- Train Loss: 0.6909314623554517 --- Val Loss: 0.6906918476296456 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 30/10000 --- Train Loss: 0.6901410915624723 --- Val Loss: 0.6897906221033159 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 40/10000 --- Train Loss: 0.6893745344435445 --- Val Loss: 0.6889151167051457 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 50/10000 --- Train Loss: 0.6886290103390257 --- Val Loss: 0.6880622703332071 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 60/10000 --- Train Loss: 0.6878967168480163 --- Val Loss: 0.687223201092859 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 70/10000 --- Train Loss: 0.6871935328302484 --- Val Loss: 0.6864162533883401 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 80/10000 --- Train Loss: 0.6865073820049669 --- Val Loss: 0.6856276901563867 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 90/10000 --- Train Loss: 0.6858293598131011 --- Val Loss: 0.6848473097423348 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 100/10000 --- Train Loss: 0.6851654436432838 --- Val Loss: 0.6840820685521979 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 110/10000 --- Train Loss: 0.6845223297340443 --- Val Loss: 0.6833399039899097 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 120/10000 --- Train Loss: 0.6838934746598482 --- Val Loss: 0.6826133424503517 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 130/10000 --- Train Loss: 0.6832824850714763 --- Val Loss: 0.6819067067567809 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 140/10000 --- Train Loss: 0.6826794852071129 --- Val Loss: 0.6812085796341208 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 150/10000 --- Train Loss: 0.6820921933107486 --- Val Loss: 0.6805280993879088 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 160/10000 --- Train Loss: 0.6815108585584426 --- Val Loss: 0.679854054365297 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 170/10000 --- Train Loss: 0.6809390267048815 --- Val Loss: 0.6791906992772311 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 180/10000 --- Train Loss: 0.6803765682347783 --- Val Loss: 0.6785381412973328 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 190/10000 --- Train Loss: 0.6798175416321915 --- Val Loss: 0.6778895261202661 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 200/10000 --- Train Loss: 0.6792628475219745 --- Val Loss: 0.677246174802902 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 210/10000 --- Train Loss: 0.678709094220339 --- Val Loss: 0.6766043319932408 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 220/10000 --- Train Loss: 0.6781595696043137 --- Val Loss: 0.6759682032665301 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 230/10000 --- Train Loss: 0.6776071102479297 --- Val Loss: 0.6753295879852199 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 240/10000 --- Train Loss: 0.6770527238040897 --- Val Loss: 0.6746900052177224 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 250/10000 --- Train Loss: 0.6764986648775353 --- Val Loss: 0.6740523989729216 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 260/10000 --- Train Loss: 0.6759369180155183 --- Val Loss: 0.6734075880557414 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 270/10000 --- Train Loss: 0.675365368904512 --- Val Loss: 0.6727534389661713 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 280/10000 --- Train Loss: 0.6747791375975076 --- Val Loss: 0.6720847143364692 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 290/10000 --- Train Loss: 0.6741790065036143 --- Val Loss: 0.6714029478128866 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 300/10000 --- Train Loss: 0.6735579799906503 --- Val Loss: 0.670700335644152 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 310/10000 --- Train Loss: 0.672908609951958 --- Val Loss: 0.6699685300540672 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 320/10000 --- Train Loss: 0.6722295172330855 --- Val Loss: 0.6692067959059769 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 330/10000 --- Train Loss: 0.6715202623069937 --- Val Loss: 0.6684156401751512 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 340/10000 --- Train Loss: 0.6707706706399196 --- Val Loss: 0.6675839152363514 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 350/10000 --- Train Loss: 0.6699751297472879 --- Val Loss: 0.6667056571984534 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 360/10000 --- Train Loss: 0.6691269277786673 --- Val Loss: 0.6657744262392918 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 370/10000 --- Train Loss: 0.6682158787305132 --- Val Loss: 0.6647792231597637 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 380/10000 --- Train Loss: 0.6672338464398622 --- Val Loss: 0.6637119150148235 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 390/10000 --- Train Loss: 0.6661712787805866 --- Val Loss: 0.6625631545254413 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 400/10000 --- Train Loss: 0.665019037602814 --- Val Loss: 0.661322910392658 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 410/10000 --- Train Loss: 0.6637610275284478 --- Val Loss: 0.6599748714956856 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 420/10000 --- Train Loss: 0.662389068691106 --- Val Loss: 0.6585114842518731 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 430/10000 --- Train Loss: 0.6608866320398917 --- Val Loss: 0.656915321476146 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 440/10000 --- Train Loss: 0.6592363283024867 --- Val Loss: 0.6551686237498906 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 450/10000 --- Train Loss: 0.6574156291356105 --- Val Loss: 0.6532479404292564 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 460/10000 --- Train Loss: 0.6554127371560524 --- Val Loss: 0.65114121218501 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 470/10000 --- Train Loss: 0.6532017097274332 --- Val Loss: 0.648822950843958 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 480/10000 --- Train Loss: 0.6507606978179932 --- Val Loss: 0.6462700028872489 --- Train Acc: 0.62 --- Val Acc: 0.64\n",
      "Epoch 490/10000 --- Train Loss: 0.6480607694780791 --- Val Loss: 0.6434529413649074 --- Train Acc: 0.62 --- Val Acc: 0.65\n",
      "Epoch 500/10000 --- Train Loss: 0.6450816777541055 --- Val Loss: 0.6403506809982615 --- Train Acc: 0.63 --- Val Acc: 0.65\n",
      "Epoch 510/10000 --- Train Loss: 0.6417988279771518 --- Val Loss: 0.6369388212693675 --- Train Acc: 0.63 --- Val Acc: 0.65\n",
      "Epoch 520/10000 --- Train Loss: 0.6381773941776226 --- Val Loss: 0.6331801649438472 --- Train Acc: 0.64 --- Val Acc: 0.65\n",
      "Epoch 530/10000 --- Train Loss: 0.634195746802376 --- Val Loss: 0.6290537342829906 --- Train Acc: 0.65 --- Val Acc: 0.65\n",
      "Epoch 540/10000 --- Train Loss: 0.6298191222883293 --- Val Loss: 0.6245234230873576 --- Train Acc: 0.66 --- Val Acc: 0.67\n",
      "Epoch 550/10000 --- Train Loss: 0.625024644750826 --- Val Loss: 0.6195651832174212 --- Train Acc: 0.67 --- Val Acc: 0.69\n",
      "Epoch 560/10000 --- Train Loss: 0.6197836373225631 --- Val Loss: 0.6141485731414896 --- Train Acc: 0.68 --- Val Acc: 0.69\n",
      "Epoch 570/10000 --- Train Loss: 0.614082029187761 --- Val Loss: 0.6082591097454044 --- Train Acc: 0.71 --- Val Acc: 0.70\n",
      "Epoch 580/10000 --- Train Loss: 0.6079093231206275 --- Val Loss: 0.601885754630838 --- Train Acc: 0.73 --- Val Acc: 0.71\n",
      "Epoch 590/10000 --- Train Loss: 0.6012549243115174 --- Val Loss: 0.5950171562103501 --- Train Acc: 0.77 --- Val Acc: 0.80\n",
      "Epoch 600/10000 --- Train Loss: 0.5941067830536257 --- Val Loss: 0.5876406092430472 --- Train Acc: 0.80 --- Val Acc: 0.84\n",
      "Epoch 610/10000 --- Train Loss: 0.5864773785954691 --- Val Loss: 0.5797657013429123 --- Train Acc: 0.82 --- Val Acc: 0.86\n",
      "Epoch 620/10000 --- Train Loss: 0.5783993266807753 --- Val Loss: 0.571424108405647 --- Train Acc: 0.85 --- Val Acc: 0.89\n",
      "Epoch 630/10000 --- Train Loss: 0.5698888976606606 --- Val Loss: 0.5626341458467378 --- Train Acc: 0.86 --- Val Acc: 0.89\n",
      "Epoch 640/10000 --- Train Loss: 0.5609922047456952 --- Val Loss: 0.553436865742395 --- Train Acc: 0.87 --- Val Acc: 0.91\n",
      "Epoch 650/10000 --- Train Loss: 0.5517555296202278 --- Val Loss: 0.5438819109922887 --- Train Acc: 0.89 --- Val Acc: 0.92\n",
      "Epoch 660/10000 --- Train Loss: 0.542222784041672 --- Val Loss: 0.5340122905457567 --- Train Acc: 0.90 --- Val Acc: 0.93\n",
      "Epoch 670/10000 --- Train Loss: 0.5324459146647785 --- Val Loss: 0.5238790560309985 --- Train Acc: 0.91 --- Val Acc: 0.93\n",
      "Epoch 680/10000 --- Train Loss: 0.522476833781655 --- Val Loss: 0.5135331463896157 --- Train Acc: 0.92 --- Val Acc: 0.93\n",
      "Epoch 690/10000 --- Train Loss: 0.5123654321325415 --- Val Loss: 0.5030265775814835 --- Train Acc: 0.92 --- Val Acc: 0.95\n",
      "Epoch 700/10000 --- Train Loss: 0.5021668606178945 --- Val Loss: 0.4924154071664281 --- Train Acc: 0.93 --- Val Acc: 0.95\n",
      "Epoch 710/10000 --- Train Loss: 0.49192972868260293 --- Val Loss: 0.481750783762412 --- Train Acc: 0.93 --- Val Acc: 0.96\n",
      "Epoch 720/10000 --- Train Loss: 0.4817065983461955 --- Val Loss: 0.47108446650541386 --- Train Acc: 0.93 --- Val Acc: 0.96\n",
      "Epoch 730/10000 --- Train Loss: 0.47153580469827844 --- Val Loss: 0.46045947049723945 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 740/10000 --- Train Loss: 0.46145456004642205 --- Val Loss: 0.4499168790067028 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 750/10000 --- Train Loss: 0.4514770856617107 --- Val Loss: 0.4394705526148827 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 760/10000 --- Train Loss: 0.44163958320079505 --- Val Loss: 0.42916050892920704 --- Train Acc: 0.94 --- Val Acc: 0.96\n",
      "Epoch 770/10000 --- Train Loss: 0.43197022527234524 --- Val Loss: 0.4190159961212751 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 780/10000 --- Train Loss: 0.42249382537558927 --- Val Loss: 0.40906652723192966 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 790/10000 --- Train Loss: 0.413214619256545 --- Val Loss: 0.3993183816139269 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 800/10000 --- Train Loss: 0.40414566239026084 --- Val Loss: 0.3897838073402302 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 810/10000 --- Train Loss: 0.39531550503369456 --- Val Loss: 0.3804950605950781 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 820/10000 --- Train Loss: 0.38671569004547346 --- Val Loss: 0.371448155567284 --- Train Acc: 0.94 --- Val Acc: 0.97\n",
      "Epoch 830/10000 --- Train Loss: 0.37837457673280817 --- Val Loss: 0.3626721463377959 --- Train Acc: 0.94 --- Val Acc: 0.97\n",
      "Epoch 840/10000 --- Train Loss: 0.3702822538824661 --- Val Loss: 0.35415585163924235 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 850/10000 --- Train Loss: 0.3624275747189616 --- Val Loss: 0.34589260874774824 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 860/10000 --- Train Loss: 0.3548397733356485 --- Val Loss: 0.33791576457167916 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 870/10000 --- Train Loss: 0.34750564409160745 --- Val Loss: 0.3302060453465091 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 880/10000 --- Train Loss: 0.3404276048553259 --- Val Loss: 0.32276908902076645 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 890/10000 --- Train Loss: 0.333601463188258 --- Val Loss: 0.3156038239186401 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 900/10000 --- Train Loss: 0.3270338839552653 --- Val Loss: 0.3087158705099912 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 910/10000 --- Train Loss: 0.32070861475122986 --- Val Loss: 0.3020895786310252 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 920/10000 --- Train Loss: 0.3146207162560613 --- Val Loss: 0.2957181431691016 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 930/10000 --- Train Loss: 0.30875846981175337 --- Val Loss: 0.28959313431151074 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 940/10000 --- Train Loss: 0.30312699169202106 --- Val Loss: 0.2837163024270916 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 950/10000 --- Train Loss: 0.297708784915753 --- Val Loss: 0.2780689242850625 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 960/10000 --- Train Loss: 0.2924934717015237 --- Val Loss: 0.27263861606712053 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 970/10000 --- Train Loss: 0.2874839492596021 --- Val Loss: 0.2674280801097319 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 980/10000 --- Train Loss: 0.2826609108156461 --- Val Loss: 0.26241846184092554 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 990/10000 --- Train Loss: 0.27802224126870356 --- Val Loss: 0.2576044100151301 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1000/10000 --- Train Loss: 0.273555328446224 --- Val Loss: 0.25297509947462615 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1010/10000 --- Train Loss: 0.26925949385125253 --- Val Loss: 0.2485260143520963 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1020/10000 --- Train Loss: 0.26511533399667714 --- Val Loss: 0.2442421756664627 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1030/10000 --- Train Loss: 0.2611228221721171 --- Val Loss: 0.24012276058056484 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1040/10000 --- Train Loss: 0.25727561836835333 --- Val Loss: 0.23615902647783515 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1050/10000 --- Train Loss: 0.25356975194917625 --- Val Loss: 0.23234486842078808 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1060/10000 --- Train Loss: 0.24999651510656193 --- Val Loss: 0.22866959326124833 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1070/10000 --- Train Loss: 0.24654831671349456 --- Val Loss: 0.22512101058141162 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1080/10000 --- Train Loss: 0.24322753995724594 --- Val Loss: 0.22170486053976474 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1090/10000 --- Train Loss: 0.24001754636671302 --- Val Loss: 0.2184047351403179 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1100/10000 --- Train Loss: 0.2369156824400602 --- Val Loss: 0.2152165561416732 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1110/10000 --- Train Loss: 0.23392060556430266 --- Val Loss: 0.21214021060915858 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1120/10000 --- Train Loss: 0.2310239278915928 --- Val Loss: 0.20916949569638799 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1130/10000 --- Train Loss: 0.22822070579058149 --- Val Loss: 0.2062976581701305 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1140/10000 --- Train Loss: 0.22550729693004043 --- Val Loss: 0.20352229226288504 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1150/10000 --- Train Loss: 0.22288150306613796 --- Val Loss: 0.20083956361652436 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1160/10000 --- Train Loss: 0.2203336631051256 --- Val Loss: 0.1982388456269501 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1170/10000 --- Train Loss: 0.2178648764029781 --- Val Loss: 0.1957233602347207 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1180/10000 --- Train Loss: 0.21547410507735879 --- Val Loss: 0.19329008172031084 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1190/10000 --- Train Loss: 0.21315167623094025 --- Val Loss: 0.1909297585493953 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1200/10000 --- Train Loss: 0.21089589101635403 --- Val Loss: 0.18864007529715665 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1210/10000 --- Train Loss: 0.20870378040999588 --- Val Loss: 0.18641779052529758 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1220/10000 --- Train Loss: 0.20657263466633966 --- Val Loss: 0.18425988783568545 --- Train Acc: 0.95 --- Val Acc: 0.97\n",
      "Epoch 1230/10000 --- Train Loss: 0.20449524237791025 --- Val Loss: 0.18216088899840102 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1240/10000 --- Train Loss: 0.20247972710827813 --- Val Loss: 0.18012535753973766 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1250/10000 --- Train Loss: 0.20052008731165435 --- Val Loss: 0.17814916959913435 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1260/10000 --- Train Loss: 0.19861302367984454 --- Val Loss: 0.176228786333815 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1270/10000 --- Train Loss: 0.19675934707716736 --- Val Loss: 0.17436331377389538 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1280/10000 --- Train Loss: 0.1949544132712691 --- Val Loss: 0.1725497746481057 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1290/10000 --- Train Loss: 0.193198259943429 --- Val Loss: 0.17078797878571483 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1300/10000 --- Train Loss: 0.19148177981475273 --- Val Loss: 0.1690681561871066 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1310/10000 --- Train Loss: 0.1898136277291507 --- Val Loss: 0.16739593734390168 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1320/10000 --- Train Loss: 0.18818698249693575 --- Val Loss: 0.16577113291451734 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1330/10000 --- Train Loss: 0.18659822706747595 --- Val Loss: 0.16418842807459805 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1340/10000 --- Train Loss: 0.18504386370735013 --- Val Loss: 0.16264249429591124 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1350/10000 --- Train Loss: 0.18353196643695108 --- Val Loss: 0.16114053172039203 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1360/10000 --- Train Loss: 0.18205422785504277 --- Val Loss: 0.15967454436291764 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1370/10000 --- Train Loss: 0.18061246965725272 --- Val Loss: 0.15824690427289465 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1380/10000 --- Train Loss: 0.1792032746648222 --- Val Loss: 0.15685302487944042 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1390/10000 --- Train Loss: 0.17782547030356188 --- Val Loss: 0.15549216069052577 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1400/10000 --- Train Loss: 0.17647598360548816 --- Val Loss: 0.15416410786058588 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1410/10000 --- Train Loss: 0.1751595354227022 --- Val Loss: 0.15287034982697834 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1420/10000 --- Train Loss: 0.17387178910476841 --- Val Loss: 0.1516045275661987 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1430/10000 --- Train Loss: 0.1726113824876631 --- Val Loss: 0.15036483187322927 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1440/10000 --- Train Loss: 0.17137662828146805 --- Val Loss: 0.14914966090346013 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1450/10000 --- Train Loss: 0.17017038468358278 --- Val Loss: 0.14796363069992596 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1460/10000 --- Train Loss: 0.16898874471778613 --- Val Loss: 0.1468034458111596 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1470/10000 --- Train Loss: 0.1678290293165357 --- Val Loss: 0.14566422510413113 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1480/10000 --- Train Loss: 0.16669787992746724 --- Val Loss: 0.1445555493874255 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1490/10000 --- Train Loss: 0.16558780163527503 --- Val Loss: 0.1434670512511614 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1500/10000 --- Train Loss: 0.1645025052227409 --- Val Loss: 0.14240452420487226 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1510/10000 --- Train Loss: 0.16343775768913643 --- Val Loss: 0.14136176296904235 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1520/10000 --- Train Loss: 0.1623959121021514 --- Val Loss: 0.14034041898286412 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1530/10000 --- Train Loss: 0.16137230144695675 --- Val Loss: 0.13933656231638505 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1540/10000 --- Train Loss: 0.16036818663715224 --- Val Loss: 0.13835289500827594 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1550/10000 --- Train Loss: 0.15938627313706813 --- Val Loss: 0.1373902669729689 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1560/10000 --- Train Loss: 0.15842336738403764 --- Val Loss: 0.13644357011480548 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1570/10000 --- Train Loss: 0.15747711097881492 --- Val Loss: 0.13551381093357975 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1580/10000 --- Train Loss: 0.15655185192507987 --- Val Loss: 0.13460340214215014 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1590/10000 --- Train Loss: 0.1556423571779014 --- Val Loss: 0.1337074429885225 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1600/10000 --- Train Loss: 0.15474896104256644 --- Val Loss: 0.13283059661349964 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1610/10000 --- Train Loss: 0.1538693073967926 --- Val Loss: 0.1319603892928309 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1620/10000 --- Train Loss: 0.15300894234104914 --- Val Loss: 0.13111263399898204 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1630/10000 --- Train Loss: 0.152160466666456 --- Val Loss: 0.13027437687304697 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1640/10000 --- Train Loss: 0.15132796547579172 --- Val Loss: 0.1294535041087183 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1650/10000 --- Train Loss: 0.15051057489585343 --- Val Loss: 0.12864323561888746 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1660/10000 --- Train Loss: 0.14970283376755292 --- Val Loss: 0.1278438718864824 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1670/10000 --- Train Loss: 0.14891456610235712 --- Val Loss: 0.1270632149769994 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1680/10000 --- Train Loss: 0.14813780398853135 --- Val Loss: 0.12629356993755195 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1690/10000 --- Train Loss: 0.1473736471574654 --- Val Loss: 0.1255344853394182 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1700/10000 --- Train Loss: 0.14661854912651892 --- Val Loss: 0.12478149611557206 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1710/10000 --- Train Loss: 0.1458792603671962 --- Val Loss: 0.1240481218510036 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1720/10000 --- Train Loss: 0.14515116300259548 --- Val Loss: 0.12332315887650039 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1730/10000 --- Train Loss: 0.14443648711272397 --- Val Loss: 0.12261160712076624 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1740/10000 --- Train Loss: 0.1437334532064278 --- Val Loss: 0.12191053512510956 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1750/10000 --- Train Loss: 0.14304047781933707 --- Val Loss: 0.12121752397996119 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1760/10000 --- Train Loss: 0.14235683937736635 --- Val Loss: 0.12053213793179018 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1770/10000 --- Train Loss: 0.14168480595484464 --- Val Loss: 0.11985626549414154 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1780/10000 --- Train Loss: 0.14102351867965576 --- Val Loss: 0.11919307883606296 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1790/10000 --- Train Loss: 0.1403712399897536 --- Val Loss: 0.11853748495962402 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1800/10000 --- Train Loss: 0.13972978614856044 --- Val Loss: 0.11789464863939395 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1810/10000 --- Train Loss: 0.13909751285062913 --- Val Loss: 0.11725993427573574 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1820/10000 --- Train Loss: 0.13847436760813076 --- Val Loss: 0.11663103021488065 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1830/10000 --- Train Loss: 0.13786028881172158 --- Val Loss: 0.11600931861664843 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1840/10000 --- Train Loss: 0.13725763437732552 --- Val Loss: 0.11539941096224321 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1850/10000 --- Train Loss: 0.1366611090173059 --- Val Loss: 0.11479392901951387 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1860/10000 --- Train Loss: 0.1360771616468627 --- Val Loss: 0.11420068196423865 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 1870/10000 --- Train Loss: 0.1354958966612316 --- Val Loss: 0.1136083349707479 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1880/10000 --- Train Loss: 0.13492348297490447 --- Val Loss: 0.11302313529523333 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1890/10000 --- Train Loss: 0.13435912107665296 --- Val Loss: 0.11244438476242347 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1900/10000 --- Train Loss: 0.133801277674013 --- Val Loss: 0.11187174446473933 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1910/10000 --- Train Loss: 0.13325086410651207 --- Val Loss: 0.11130766167274543 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1920/10000 --- Train Loss: 0.13271221859408833 --- Val Loss: 0.11075651932280582 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1930/10000 --- Train Loss: 0.13217811706243418 --- Val Loss: 0.11020894647519947 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1940/10000 --- Train Loss: 0.13164992980194268 --- Val Loss: 0.10966909216558442 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1950/10000 --- Train Loss: 0.1311294722478552 --- Val Loss: 0.10913542064764162 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1960/10000 --- Train Loss: 0.13061730127090096 --- Val Loss: 0.10860946666546169 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1970/10000 --- Train Loss: 0.13011068058327074 --- Val Loss: 0.10809072984903197 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1980/10000 --- Train Loss: 0.1296108007354594 --- Val Loss: 0.10757679611953343 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 1990/10000 --- Train Loss: 0.12911719611189773 --- Val Loss: 0.1070685679180781 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2000/10000 --- Train Loss: 0.12862830873550385 --- Val Loss: 0.10656512345104557 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2010/10000 --- Train Loss: 0.12814843091986244 --- Val Loss: 0.10607316084466172 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2020/10000 --- Train Loss: 0.1276734259213404 --- Val Loss: 0.10558444038960617 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2030/10000 --- Train Loss: 0.12720395433247966 --- Val Loss: 0.10510059229787426 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2040/10000 --- Train Loss: 0.12674207457970643 --- Val Loss: 0.10462269654613811 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2050/10000 --- Train Loss: 0.12628482384611767 --- Val Loss: 0.10415271944930982 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2060/10000 --- Train Loss: 0.12583086308549482 --- Val Loss: 0.10368226281258715 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2070/10000 --- Train Loss: 0.12538312483962477 --- Val Loss: 0.10322004867144018 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2080/10000 --- Train Loss: 0.12494035439385966 --- Val Loss: 0.10276090769782616 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2090/10000 --- Train Loss: 0.12450443893769189 --- Val Loss: 0.1023102191559075 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2100/10000 --- Train Loss: 0.12407149482847483 --- Val Loss: 0.10186365706340329 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2110/10000 --- Train Loss: 0.12364443099522232 --- Val Loss: 0.10142307469373767 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2120/10000 --- Train Loss: 0.1232237147136799 --- Val Loss: 0.10098923247318119 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2130/10000 --- Train Loss: 0.12280874368549036 --- Val Loss: 0.10056053080864495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2140/10000 --- Train Loss: 0.12239812808289359 --- Val Loss: 0.10013748746345924 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2150/10000 --- Train Loss: 0.12199287321096569 --- Val Loss: 0.0997194577351518 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2160/10000 --- Train Loss: 0.12159219696493588 --- Val Loss: 0.09930508952388747 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2170/10000 --- Train Loss: 0.1211962219378329 --- Val Loss: 0.09889775060160128 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2180/10000 --- Train Loss: 0.12080362535325616 --- Val Loss: 0.09849516018833056 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2190/10000 --- Train Loss: 0.12041402708986461 --- Val Loss: 0.09809684029234912 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2200/10000 --- Train Loss: 0.12003072007521147 --- Val Loss: 0.09770533092823441 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2210/10000 --- Train Loss: 0.11965078345293997 --- Val Loss: 0.09731780123027127 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2220/10000 --- Train Loss: 0.11927335117125858 --- Val Loss: 0.09693436168167739 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2230/10000 --- Train Loss: 0.11890039871044653 --- Val Loss: 0.096557466045265 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2240/10000 --- Train Loss: 0.11853129111980973 --- Val Loss: 0.09618337481493863 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2250/10000 --- Train Loss: 0.11816760020754119 --- Val Loss: 0.0958199856267289 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2260/10000 --- Train Loss: 0.11780505773318226 --- Val Loss: 0.09545552361325119 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2270/10000 --- Train Loss: 0.11744624430147589 --- Val Loss: 0.09509651835930891 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2280/10000 --- Train Loss: 0.11709255835302172 --- Val Loss: 0.09474410503760115 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2290/10000 --- Train Loss: 0.11674291150257506 --- Val Loss: 0.09439743676326988 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2300/10000 --- Train Loss: 0.11639572582307206 --- Val Loss: 0.09405085892230546 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2310/10000 --- Train Loss: 0.11605137832797084 --- Val Loss: 0.09370703172295143 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2320/10000 --- Train Loss: 0.11571117323900128 --- Val Loss: 0.09336765691633964 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2330/10000 --- Train Loss: 0.1153754316540172 --- Val Loss: 0.09303379421125374 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2340/10000 --- Train Loss: 0.11504238683489285 --- Val Loss: 0.09269971864135464 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2350/10000 --- Train Loss: 0.11471434846558748 --- Val Loss: 0.09237415253318719 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2360/10000 --- Train Loss: 0.11439099779262542 --- Val Loss: 0.09205212803632895 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2370/10000 --- Train Loss: 0.11406888461964775 --- Val Loss: 0.09172771924105887 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2380/10000 --- Train Loss: 0.1137507522553922 --- Val Loss: 0.09141049992541364 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2390/10000 --- Train Loss: 0.11343612192449136 --- Val Loss: 0.09109461837536321 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2400/10000 --- Train Loss: 0.1131252651322759 --- Val Loss: 0.09078566313336088 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2410/10000 --- Train Loss: 0.11281795708945369 --- Val Loss: 0.09047695674533279 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2420/10000 --- Train Loss: 0.1125151347453133 --- Val Loss: 0.09017445658964747 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2430/10000 --- Train Loss: 0.11221445344206868 --- Val Loss: 0.08987300747113693 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2440/10000 --- Train Loss: 0.11191839354080185 --- Val Loss: 0.0895741287930308 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2450/10000 --- Train Loss: 0.11162382605440903 --- Val Loss: 0.0892781028429765 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2460/10000 --- Train Loss: 0.11133228284344526 --- Val Loss: 0.08898421877671864 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2470/10000 --- Train Loss: 0.11104426804718742 --- Val Loss: 0.08869453265173094 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2480/10000 --- Train Loss: 0.11075881376350065 --- Val Loss: 0.0884064153490323 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2490/10000 --- Train Loss: 0.11047486339571189 --- Val Loss: 0.08812142712252886 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2500/10000 --- Train Loss: 0.11019377197290825 --- Val Loss: 0.08783950909348186 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2510/10000 --- Train Loss: 0.10991756240476559 --- Val Loss: 0.0875592038611163 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2520/10000 --- Train Loss: 0.10964399019148294 --- Val Loss: 0.08728292559772635 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2530/10000 --- Train Loss: 0.10937318000253403 --- Val Loss: 0.08700695174098495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2540/10000 --- Train Loss: 0.10910522940532183 --- Val Loss: 0.08673805918562093 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2550/10000 --- Train Loss: 0.10883840536086532 --- Val Loss: 0.08646578902027838 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2560/10000 --- Train Loss: 0.1085753277227308 --- Val Loss: 0.0862000258891296 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2570/10000 --- Train Loss: 0.10831447233487508 --- Val Loss: 0.08593513278662704 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2580/10000 --- Train Loss: 0.10805714904910298 --- Val Loss: 0.08567458702872408 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2590/10000 --- Train Loss: 0.10780160131448006 --- Val Loss: 0.08541615808388339 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2600/10000 --- Train Loss: 0.10754812351509278 --- Val Loss: 0.08515782198119329 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2610/10000 --- Train Loss: 0.10729680703200802 --- Val Loss: 0.08490264624984915 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2620/10000 --- Train Loss: 0.10704877650601984 --- Val Loss: 0.08465255900318841 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2630/10000 --- Train Loss: 0.10680218787630671 --- Val Loss: 0.08440217812198171 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2640/10000 --- Train Loss: 0.10655864648493524 --- Val Loss: 0.08415605780532578 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2650/10000 --- Train Loss: 0.10631680298819804 --- Val Loss: 0.08391003977214462 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2660/10000 --- Train Loss: 0.10607761077275056 --- Val Loss: 0.0836653618029885 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2670/10000 --- Train Loss: 0.10584240356479453 --- Val Loss: 0.08342747630980585 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2680/10000 --- Train Loss: 0.10560703718237618 --- Val Loss: 0.08318612087360576 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2690/10000 --- Train Loss: 0.10537435021183343 --- Val Loss: 0.08294967073498369 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2700/10000 --- Train Loss: 0.10514448322954058 --- Val Loss: 0.0827147222926751 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2710/10000 --- Train Loss: 0.10491554018613293 --- Val Loss: 0.08248139608412142 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2720/10000 --- Train Loss: 0.10468788867744618 --- Val Loss: 0.0822467857663463 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2730/10000 --- Train Loss: 0.10446477271588278 --- Val Loss: 0.08201791389791717 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2740/10000 --- Train Loss: 0.104242136208723 --- Val Loss: 0.08179235026522516 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2750/10000 --- Train Loss: 0.10402147599059094 --- Val Loss: 0.08156641503602158 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2760/10000 --- Train Loss: 0.10380302576449313 --- Val Loss: 0.08134103822686906 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2770/10000 --- Train Loss: 0.1035857617038782 --- Val Loss: 0.08111885006676615 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2780/10000 --- Train Loss: 0.10337305318690815 --- Val Loss: 0.08090127631863159 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2790/10000 --- Train Loss: 0.10315954190829665 --- Val Loss: 0.08068319756813859 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2800/10000 --- Train Loss: 0.1029483111020732 --- Val Loss: 0.08046673948993409 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2810/10000 --- Train Loss: 0.10273936181633006 --- Val Loss: 0.08025201484659078 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2820/10000 --- Train Loss: 0.10253198866816693 --- Val Loss: 0.08004095929731604 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2830/10000 --- Train Loss: 0.10232614467019678 --- Val Loss: 0.07983001360000415 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2840/10000 --- Train Loss: 0.10212122626769278 --- Val Loss: 0.07961948555204075 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2850/10000 --- Train Loss: 0.10191888033916274 --- Val Loss: 0.07941257111880415 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2860/10000 --- Train Loss: 0.1017190331454936 --- Val Loss: 0.07920949774826244 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2870/10000 --- Train Loss: 0.10152027117370446 --- Val Loss: 0.07900687398511096 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2880/10000 --- Train Loss: 0.10132333966286619 --- Val Loss: 0.07880663943938862 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2890/10000 --- Train Loss: 0.10112677611821756 --- Val Loss: 0.07860470609067385 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2900/10000 --- Train Loss: 0.10093270927624659 --- Val Loss: 0.07840657523123609 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2910/10000 --- Train Loss: 0.1007415033734232 --- Val Loss: 0.07820945039689822 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2920/10000 --- Train Loss: 0.1005495867889166 --- Val Loss: 0.0780129601725731 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2930/10000 --- Train Loss: 0.10035943965444763 --- Val Loss: 0.07781858020262646 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2940/10000 --- Train Loss: 0.1001725086446534 --- Val Loss: 0.07762749577668829 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2950/10000 --- Train Loss: 0.09998694752382525 --- Val Loss: 0.07743685641723497 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2960/10000 --- Train Loss: 0.09980235953236367 --- Val Loss: 0.07724853703380756 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2970/10000 --- Train Loss: 0.09961976029446386 --- Val Loss: 0.07706305510659912 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2980/10000 --- Train Loss: 0.09943841463767077 --- Val Loss: 0.07687971708374812 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 2990/10000 --- Train Loss: 0.09925665357054066 --- Val Loss: 0.0766944145094962 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3000/10000 --- Train Loss: 0.09907846304506028 --- Val Loss: 0.07651333510444515 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3010/10000 --- Train Loss: 0.09890041107684353 --- Val Loss: 0.07633253923447846 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3020/10000 --- Train Loss: 0.09872313575813486 --- Val Loss: 0.07615050311102689 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3030/10000 --- Train Loss: 0.09854854639938776 --- Val Loss: 0.07597375845332523 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3040/10000 --- Train Loss: 0.09837483915788316 --- Val Loss: 0.07579693369726363 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3050/10000 --- Train Loss: 0.09820204558324772 --- Val Loss: 0.07562081916220655 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3060/10000 --- Train Loss: 0.09803249640017361 --- Val Loss: 0.07545080088286117 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3070/10000 --- Train Loss: 0.09786137667306812 --- Val Loss: 0.07527390838654008 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3080/10000 --- Train Loss: 0.09769313950235899 --- Val Loss: 0.07510308729419231 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3090/10000 --- Train Loss: 0.09752540707204807 --- Val Loss: 0.07493055511055799 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3100/10000 --- Train Loss: 0.09735930739637416 --- Val Loss: 0.07476212864017091 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3110/10000 --- Train Loss: 0.09719438386930329 --- Val Loss: 0.0745959044006651 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3120/10000 --- Train Loss: 0.09703159633508616 --- Val Loss: 0.07442912917241179 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3130/10000 --- Train Loss: 0.09686928283715816 --- Val Loss: 0.07426573648942092 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3140/10000 --- Train Loss: 0.09670743258375834 --- Val Loss: 0.07410358556551629 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3150/10000 --- Train Loss: 0.09654662383901058 --- Val Loss: 0.07393921572046457 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3160/10000 --- Train Loss: 0.09638881219383931 --- Val Loss: 0.07377922743284095 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3170/10000 --- Train Loss: 0.09623041763531968 --- Val Loss: 0.07361725787528199 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3180/10000 --- Train Loss: 0.09607298341806397 --- Val Loss: 0.07345888832124846 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3190/10000 --- Train Loss: 0.09591887984547043 --- Val Loss: 0.07330563255691132 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3200/10000 --- Train Loss: 0.09576398154319864 --- Val Loss: 0.0731495732837887 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3210/10000 --- Train Loss: 0.09561071458918827 --- Val Loss: 0.07299442555350077 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3220/10000 --- Train Loss: 0.09545740564878097 --- Val Loss: 0.07283921421535439 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3230/10000 --- Train Loss: 0.09530574968935665 --- Val Loss: 0.07268483954607219 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3240/10000 --- Train Loss: 0.09515647956958082 --- Val Loss: 0.072534193899058 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3250/10000 --- Train Loss: 0.09500722049117367 --- Val Loss: 0.0723839674098472 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3260/10000 --- Train Loss: 0.09486004125803385 --- Val Loss: 0.07223738964860953 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3270/10000 --- Train Loss: 0.09471348710674465 --- Val Loss: 0.0720922610843677 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3280/10000 --- Train Loss: 0.09456718982275654 --- Val Loss: 0.07194562338436476 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3290/10000 --- Train Loss: 0.09442225851356818 --- Val Loss: 0.07179988470156792 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3300/10000 --- Train Loss: 0.09427802823881824 --- Val Loss: 0.07165280986384037 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3310/10000 --- Train Loss: 0.09413652014177122 --- Val Loss: 0.07151182150634974 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3320/10000 --- Train Loss: 0.09399334347366084 --- Val Loss: 0.0713682527553013 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3330/10000 --- Train Loss: 0.09385169552147357 --- Val Loss: 0.07122441992048269 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3340/10000 --- Train Loss: 0.09371049442148463 --- Val Loss: 0.071082503297938 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3350/10000 --- Train Loss: 0.09357120742203102 --- Val Loss: 0.07094566101259858 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3360/10000 --- Train Loss: 0.09343238394426051 --- Val Loss: 0.07080671461476641 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3370/10000 --- Train Loss: 0.09329538275915865 --- Val Loss: 0.0706703528227081 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3380/10000 --- Train Loss: 0.09315822640028731 --- Val Loss: 0.07053444244107371 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3390/10000 --- Train Loss: 0.0930226023820366 --- Val Loss: 0.07039851339768158 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3400/10000 --- Train Loss: 0.09288822440191005 --- Val Loss: 0.07026429484942413 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3410/10000 --- Train Loss: 0.09275435207539284 --- Val Loss: 0.07013102367646273 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3420/10000 --- Train Loss: 0.0926220191197337 --- Val Loss: 0.06999983003589898 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3430/10000 --- Train Loss: 0.09248942878875786 --- Val Loss: 0.06986864055163225 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3440/10000 --- Train Loss: 0.09235872233292405 --- Val Loss: 0.06973846338393487 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3450/10000 --- Train Loss: 0.09222860804854444 --- Val Loss: 0.06960857980798942 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3460/10000 --- Train Loss: 0.09209981276781781 --- Val Loss: 0.06948208566711579 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3470/10000 --- Train Loss: 0.09197029045209561 --- Val Loss: 0.06935365847966786 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3480/10000 --- Train Loss: 0.09184286140751943 --- Val Loss: 0.06922878293955839 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3490/10000 --- Train Loss: 0.09171463458292824 --- Val Loss: 0.0691031923572489 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3500/10000 --- Train Loss: 0.09158864533403982 --- Val Loss: 0.06898085436781215 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3510/10000 --- Train Loss: 0.09146514495380709 --- Val Loss: 0.06885864905957388 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3520/10000 --- Train Loss: 0.0913409045181873 --- Val Loss: 0.0687379995896877 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3530/10000 --- Train Loss: 0.09121753015784612 --- Val Loss: 0.06861777934124678 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3540/10000 --- Train Loss: 0.0910946333138291 --- Val Loss: 0.06849833215648966 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3550/10000 --- Train Loss: 0.09097219792306971 --- Val Loss: 0.06837707803346492 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3560/10000 --- Train Loss: 0.09085025411556756 --- Val Loss: 0.06825782688982215 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3570/10000 --- Train Loss: 0.09073029574330971 --- Val Loss: 0.06814044865775606 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3580/10000 --- Train Loss: 0.09061083401606089 --- Val Loss: 0.06802434394022736 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3590/10000 --- Train Loss: 0.0904930508082912 --- Val Loss: 0.06790798799593396 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3600/10000 --- Train Loss: 0.09037488637960732 --- Val Loss: 0.06779473174913331 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3610/10000 --- Train Loss: 0.09025703876412018 --- Val Loss: 0.06767902736888122 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3620/10000 --- Train Loss: 0.09013976855396687 --- Val Loss: 0.06756398174486927 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3630/10000 --- Train Loss: 0.09002371458021344 --- Val Loss: 0.06745174780317796 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3640/10000 --- Train Loss: 0.08990849017242987 --- Val Loss: 0.06734004169579372 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3650/10000 --- Train Loss: 0.08979368817281626 --- Val Loss: 0.06722712225249887 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3660/10000 --- Train Loss: 0.08967948074684465 --- Val Loss: 0.06711723135255325 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3670/10000 --- Train Loss: 0.08956612706414147 --- Val Loss: 0.06700700479237068 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3680/10000 --- Train Loss: 0.08945306882298018 --- Val Loss: 0.06689795215734809 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3690/10000 --- Train Loss: 0.08934087245325709 --- Val Loss: 0.06679062534321654 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3700/10000 --- Train Loss: 0.08923013049061874 --- Val Loss: 0.0666835275365956 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3710/10000 --- Train Loss: 0.08911938664074188 --- Val Loss: 0.06657674111123969 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3720/10000 --- Train Loss: 0.08900939424749665 --- Val Loss: 0.0664685437205126 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3730/10000 --- Train Loss: 0.08889991028397926 --- Val Loss: 0.06636370935489544 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3740/10000 --- Train Loss: 0.08879151012663022 --- Val Loss: 0.06625912997424951 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3750/10000 --- Train Loss: 0.08868316241123148 --- Val Loss: 0.06615356143687003 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3760/10000 --- Train Loss: 0.08857562542621407 --- Val Loss: 0.06605054428737758 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3770/10000 --- Train Loss: 0.08846823750972316 --- Val Loss: 0.06594714133266823 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3780/10000 --- Train Loss: 0.08836197790832771 --- Val Loss: 0.06584574956992723 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3790/10000 --- Train Loss: 0.08825671644725791 --- Val Loss: 0.06574414931450878 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3800/10000 --- Train Loss: 0.08815249057112083 --- Val Loss: 0.06564278650884256 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3810/10000 --- Train Loss: 0.08804885568784701 --- Val Loss: 0.06554511100153158 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3820/10000 --- Train Loss: 0.08794543103713208 --- Val Loss: 0.06544565197326008 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3830/10000 --- Train Loss: 0.08784217734388017 --- Val Loss: 0.06534698089473533 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3840/10000 --- Train Loss: 0.08774002417863129 --- Val Loss: 0.06524829917090912 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3850/10000 --- Train Loss: 0.08763916688470802 --- Val Loss: 0.06515459940101678 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3860/10000 --- Train Loss: 0.0875375899813371 --- Val Loss: 0.06505659377900003 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3870/10000 --- Train Loss: 0.0874367230794912 --- Val Loss: 0.06496138010177484 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3880/10000 --- Train Loss: 0.0873374206208111 --- Val Loss: 0.06486677607455181 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3890/10000 --- Train Loss: 0.0872382033817865 --- Val Loss: 0.06477121905881011 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3900/10000 --- Train Loss: 0.087138982825586 --- Val Loss: 0.06467788534551541 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3910/10000 --- Train Loss: 0.08704103922119805 --- Val Loss: 0.06458647604870658 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3920/10000 --- Train Loss: 0.08694240982169996 --- Val Loss: 0.06449282704170178 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3930/10000 --- Train Loss: 0.0868456913640685 --- Val Loss: 0.06440290454808716 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3940/10000 --- Train Loss: 0.08674966063579653 --- Val Loss: 0.06431264788422 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3950/10000 --- Train Loss: 0.08665372787128112 --- Val Loss: 0.06422295561834579 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3960/10000 --- Train Loss: 0.08655935325202174 --- Val Loss: 0.06413688183574164 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3970/10000 --- Train Loss: 0.08646388318300516 --- Val Loss: 0.06404580508331773 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3980/10000 --- Train Loss: 0.08636942635818752 --- Val Loss: 0.06395840308674919 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 3990/10000 --- Train Loss: 0.08627498715412814 --- Val Loss: 0.06386969120510082 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4000/10000 --- Train Loss: 0.08618129258887097 --- Val Loss: 0.06378175934521525 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4010/10000 --- Train Loss: 0.08608852929393031 --- Val Loss: 0.06369318694633226 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4020/10000 --- Train Loss: 0.08599596779657609 --- Val Loss: 0.06360783867974226 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4030/10000 --- Train Loss: 0.08590355779904911 --- Val Loss: 0.06352112199023575 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4040/10000 --- Train Loss: 0.08581213773907752 --- Val Loss: 0.06343631745987964 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4050/10000 --- Train Loss: 0.08572018682720312 --- Val Loss: 0.06334789592681354 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4060/10000 --- Train Loss: 0.08562870978512978 --- Val Loss: 0.06326242222731986 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4070/10000 --- Train Loss: 0.085538435553752 --- Val Loss: 0.06317770658461543 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4080/10000 --- Train Loss: 0.08544888452305201 --- Val Loss: 0.06309467677385001 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4090/10000 --- Train Loss: 0.08536028804575663 --- Val Loss: 0.0630121976794806 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4100/10000 --- Train Loss: 0.0852715715309377 --- Val Loss: 0.06292897227212843 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4110/10000 --- Train Loss: 0.08518355313295993 --- Val Loss: 0.06284637122656676 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4120/10000 --- Train Loss: 0.0850961645849298 --- Val Loss: 0.06276498923724179 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4130/10000 --- Train Loss: 0.0850085599678517 --- Val Loss: 0.06268252178344925 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4140/10000 --- Train Loss: 0.084921143495108 --- Val Loss: 0.06259993514393739 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4150/10000 --- Train Loss: 0.08483529414161932 --- Val Loss: 0.06251888577746738 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4160/10000 --- Train Loss: 0.08474876244219937 --- Val Loss: 0.06243929744452337 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4170/10000 --- Train Loss: 0.08466316114673972 --- Val Loss: 0.06236048514628226 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4180/10000 --- Train Loss: 0.08457827269826741 --- Val Loss: 0.06228068449943798 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4190/10000 --- Train Loss: 0.08449373942745267 --- Val Loss: 0.06220164607459234 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4200/10000 --- Train Loss: 0.08441022789205263 --- Val Loss: 0.06212407229470871 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4210/10000 --- Train Loss: 0.08432713983540292 --- Val Loss: 0.062047325626947945 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4220/10000 --- Train Loss: 0.08424422838252237 --- Val Loss: 0.06197027362403151 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4230/10000 --- Train Loss: 0.08416178732168347 --- Val Loss: 0.061893777818431514 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4240/10000 --- Train Loss: 0.08407933548201862 --- Val Loss: 0.06181647969577551 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4250/10000 --- Train Loss: 0.08399741763188442 --- Val Loss: 0.06174127509512009 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4260/10000 --- Train Loss: 0.08391570678761551 --- Val Loss: 0.061665659107506876 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4270/10000 --- Train Loss: 0.08383427362878074 --- Val Loss: 0.06159013398800966 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4280/10000 --- Train Loss: 0.0837535207609587 --- Val Loss: 0.061512371628784535 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4290/10000 --- Train Loss: 0.083672986260443 --- Val Loss: 0.061438481624182126 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4300/10000 --- Train Loss: 0.0835928778549208 --- Val Loss: 0.061364215046309 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4310/10000 --- Train Loss: 0.0835134289459163 --- Val Loss: 0.06128911679637918 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4320/10000 --- Train Loss: 0.08343421479330126 --- Val Loss: 0.06121662569860923 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4330/10000 --- Train Loss: 0.08335571517356784 --- Val Loss: 0.061142267477135616 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4340/10000 --- Train Loss: 0.08327714708149969 --- Val Loss: 0.06106972679488619 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4350/10000 --- Train Loss: 0.08319899938443395 --- Val Loss: 0.06099739641319801 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4360/10000 --- Train Loss: 0.08312153475773794 --- Val Loss: 0.06092566014360045 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4370/10000 --- Train Loss: 0.08304372335804555 --- Val Loss: 0.06085469435336831 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4380/10000 --- Train Loss: 0.0829674108313677 --- Val Loss: 0.060784217677432924 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4390/10000 --- Train Loss: 0.0828903296636512 --- Val Loss: 0.060713464486534176 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4400/10000 --- Train Loss: 0.0828139845052274 --- Val Loss: 0.0606433515860532 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4410/10000 --- Train Loss: 0.08273842299017366 --- Val Loss: 0.060575104798626205 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4420/10000 --- Train Loss: 0.0826621361287107 --- Val Loss: 0.06050308945947174 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4430/10000 --- Train Loss: 0.08258743094369922 --- Val Loss: 0.060437089470693546 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4440/10000 --- Train Loss: 0.08251243628255528 --- Val Loss: 0.060365967598296114 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4450/10000 --- Train Loss: 0.08243819466829796 --- Val Loss: 0.060297567310970836 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4460/10000 --- Train Loss: 0.08236426283765594 --- Val Loss: 0.06022913958849635 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4470/10000 --- Train Loss: 0.08229055854634927 --- Val Loss: 0.06016116883281692 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4480/10000 --- Train Loss: 0.08221734429471238 --- Val Loss: 0.060093887071281084 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4490/10000 --- Train Loss: 0.08214445617693618 --- Val Loss: 0.0600268968903243 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4500/10000 --- Train Loss: 0.08207176118655526 --- Val Loss: 0.059960344601046545 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4510/10000 --- Train Loss: 0.08199931152733633 --- Val Loss: 0.0598939690600022 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4520/10000 --- Train Loss: 0.08192756353742305 --- Val Loss: 0.05982956992538031 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4530/10000 --- Train Loss: 0.08185576164481503 --- Val Loss: 0.05976295892184669 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4540/10000 --- Train Loss: 0.08178454395886502 --- Val Loss: 0.059698200600435 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4550/10000 --- Train Loss: 0.08171329860599914 --- Val Loss: 0.059634395842445495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4560/10000 --- Train Loss: 0.08164371959921321 --- Val Loss: 0.059571601359609115 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4570/10000 --- Train Loss: 0.08157322217025724 --- Val Loss: 0.05950693548334022 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4580/10000 --- Train Loss: 0.08150293061136861 --- Val Loss: 0.059443509409251534 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4590/10000 --- Train Loss: 0.0814333839796012 --- Val Loss: 0.05938021499909546 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4600/10000 --- Train Loss: 0.0813636570044118 --- Val Loss: 0.059314998235674855 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4610/10000 --- Train Loss: 0.08129403837374323 --- Val Loss: 0.05925310755794513 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4620/10000 --- Train Loss: 0.08122507493479639 --- Val Loss: 0.059190838237022134 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4630/10000 --- Train Loss: 0.08115575910860795 --- Val Loss: 0.059125438524095485 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4640/10000 --- Train Loss: 0.08108791981022244 --- Val Loss: 0.05906420445760266 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4650/10000 --- Train Loss: 0.08102023050513643 --- Val Loss: 0.0590023489177009 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4660/10000 --- Train Loss: 0.0809523948474484 --- Val Loss: 0.058940768229979006 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4670/10000 --- Train Loss: 0.08088539956589777 --- Val Loss: 0.058881173785836474 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4680/10000 --- Train Loss: 0.08081850984990332 --- Val Loss: 0.05882131759598688 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4690/10000 --- Train Loss: 0.08075216223529195 --- Val Loss: 0.058760685206626036 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4700/10000 --- Train Loss: 0.08068562733883601 --- Val Loss: 0.05870001177592986 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4710/10000 --- Train Loss: 0.08061903539731494 --- Val Loss: 0.05863910494130292 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4720/10000 --- Train Loss: 0.08055391049308983 --- Val Loss: 0.058582622029071066 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4730/10000 --- Train Loss: 0.08048860965786171 --- Val Loss: 0.058522455548732054 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4740/10000 --- Train Loss: 0.08042363916706982 --- Val Loss: 0.05846304445453864 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4750/10000 --- Train Loss: 0.08035854461836732 --- Val Loss: 0.05840434455747697 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4760/10000 --- Train Loss: 0.08029363828347166 --- Val Loss: 0.05834673776712289 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4770/10000 --- Train Loss: 0.08022923386899487 --- Val Loss: 0.05828798856683613 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4780/10000 --- Train Loss: 0.08016476463627333 --- Val Loss: 0.058228625979942536 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4790/10000 --- Train Loss: 0.08010165926504784 --- Val Loss: 0.058172651967585955 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4800/10000 --- Train Loss: 0.08003861212842223 --- Val Loss: 0.05811619341378131 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4810/10000 --- Train Loss: 0.07997550096458639 --- Val Loss: 0.05805915434741498 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4820/10000 --- Train Loss: 0.07991239928715446 --- Val Loss: 0.058003000052865974 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4830/10000 --- Train Loss: 0.07984945763440846 --- Val Loss: 0.05794615531914716 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4840/10000 --- Train Loss: 0.07978739333236523 --- Val Loss: 0.05788921912448955 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4850/10000 --- Train Loss: 0.07972472074320965 --- Val Loss: 0.05783248503799105 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4860/10000 --- Train Loss: 0.07966236406211455 --- Val Loss: 0.05777623580180762 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4870/10000 --- Train Loss: 0.07960086601191503 --- Val Loss: 0.05772004188004573 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4880/10000 --- Train Loss: 0.07954018409453469 --- Val Loss: 0.05766507755768302 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4890/10000 --- Train Loss: 0.07947925309658349 --- Val Loss: 0.05761236726236962 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4900/10000 --- Train Loss: 0.0794181969245739 --- Val Loss: 0.0575558630378262 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4910/10000 --- Train Loss: 0.07935732147611683 --- Val Loss: 0.057502166723375006 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4920/10000 --- Train Loss: 0.07929637474780235 --- Val Loss: 0.05744635635020617 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4930/10000 --- Train Loss: 0.0792370995308262 --- Val Loss: 0.057393054656929586 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4940/10000 --- Train Loss: 0.07917722926274925 --- Val Loss: 0.057339686655762924 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4950/10000 --- Train Loss: 0.07911804392614076 --- Val Loss: 0.05728709307301742 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4960/10000 --- Train Loss: 0.07905838286907688 --- Val Loss: 0.05723307336249921 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4970/10000 --- Train Loss: 0.07899877421970274 --- Val Loss: 0.05718013046907425 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4980/10000 --- Train Loss: 0.07893954405782691 --- Val Loss: 0.05712605480359689 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 4990/10000 --- Train Loss: 0.07888078727300601 --- Val Loss: 0.057073894419483004 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5000/10000 --- Train Loss: 0.07882246663934533 --- Val Loss: 0.05702117224764057 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5010/10000 --- Train Loss: 0.07876464457057068 --- Val Loss: 0.05697024162992583 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5020/10000 --- Train Loss: 0.07870684093220748 --- Val Loss: 0.056918592313595494 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5030/10000 --- Train Loss: 0.0786492212736018 --- Val Loss: 0.05686643255604165 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5040/10000 --- Train Loss: 0.07859147748615958 --- Val Loss: 0.05681341342655106 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5050/10000 --- Train Loss: 0.07853427419394295 --- Val Loss: 0.0567621243144134 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5060/10000 --- Train Loss: 0.07847756836962164 --- Val Loss: 0.05671040457246404 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5070/10000 --- Train Loss: 0.07842068691575398 --- Val Loss: 0.056658827348242374 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5080/10000 --- Train Loss: 0.07836418014495154 --- Val Loss: 0.05660712662002362 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5090/10000 --- Train Loss: 0.07830743062883497 --- Val Loss: 0.05655564554766236 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5100/10000 --- Train Loss: 0.07825118700970524 --- Val Loss: 0.056504795358341464 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5110/10000 --- Train Loss: 0.07819553365751561 --- Val Loss: 0.05645486952411093 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5120/10000 --- Train Loss: 0.07813944953274786 --- Val Loss: 0.05640356212934989 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5130/10000 --- Train Loss: 0.07808451408789692 --- Val Loss: 0.056354558283053566 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5140/10000 --- Train Loss: 0.07802952916898538 --- Val Loss: 0.05630546018701793 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5150/10000 --- Train Loss: 0.07797422315769557 --- Val Loss: 0.05625280343831121 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5160/10000 --- Train Loss: 0.07791997095448722 --- Val Loss: 0.056204615027230925 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5170/10000 --- Train Loss: 0.0778659147015415 --- Val Loss: 0.05615735109925449 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5180/10000 --- Train Loss: 0.07781161670922661 --- Val Loss: 0.05610695672197347 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5190/10000 --- Train Loss: 0.07775820123033748 --- Val Loss: 0.05606032550611159 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5200/10000 --- Train Loss: 0.07770378328321564 --- Val Loss: 0.056009202406814056 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5210/10000 --- Train Loss: 0.07764995781000507 --- Val Loss: 0.055959628509226075 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5220/10000 --- Train Loss: 0.07759617351986059 --- Val Loss: 0.05590931726142444 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5230/10000 --- Train Loss: 0.07754293563345305 --- Val Loss: 0.05586052824343492 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5240/10000 --- Train Loss: 0.07748973587749565 --- Val Loss: 0.055811977153857176 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5250/10000 --- Train Loss: 0.0774369220908545 --- Val Loss: 0.055764776383094024 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5260/10000 --- Train Loss: 0.07738439292006255 --- Val Loss: 0.05571702387220255 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5270/10000 --- Train Loss: 0.07733204648027783 --- Val Loss: 0.05566832369987257 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5280/10000 --- Train Loss: 0.07727964966068854 --- Val Loss: 0.055620331912177495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5290/10000 --- Train Loss: 0.07722772366928697 --- Val Loss: 0.05557414591293283 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5300/10000 --- Train Loss: 0.07717595917108465 --- Val Loss: 0.055527330742236904 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5310/10000 --- Train Loss: 0.0771244854016402 --- Val Loss: 0.05548101125488416 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5320/10000 --- Train Loss: 0.0770732178986209 --- Val Loss: 0.055434716026016594 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5330/10000 --- Train Loss: 0.07702245250240707 --- Val Loss: 0.05538875201887666 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5340/10000 --- Train Loss: 0.07697119965806504 --- Val Loss: 0.05534142167117661 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5350/10000 --- Train Loss: 0.07692051909618743 --- Val Loss: 0.05529632402927862 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5360/10000 --- Train Loss: 0.07686970788556743 --- Val Loss: 0.055249918151809366 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5370/10000 --- Train Loss: 0.07681969164394882 --- Val Loss: 0.055203921011718136 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5380/10000 --- Train Loss: 0.0767697693963833 --- Val Loss: 0.055159696689530094 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5390/10000 --- Train Loss: 0.07671935959937108 --- Val Loss: 0.05511413840776052 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5400/10000 --- Train Loss: 0.07666990175879655 --- Val Loss: 0.055070390432985256 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5410/10000 --- Train Loss: 0.07662013590319469 --- Val Loss: 0.05502498228207269 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5420/10000 --- Train Loss: 0.07657093755225046 --- Val Loss: 0.05498150537385484 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5430/10000 --- Train Loss: 0.0765211159893662 --- Val Loss: 0.0549364588226278 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5440/10000 --- Train Loss: 0.07647245503830183 --- Val Loss: 0.05489315746833879 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5450/10000 --- Train Loss: 0.07642322741178305 --- Val Loss: 0.054848420747847945 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5460/10000 --- Train Loss: 0.07637435402789079 --- Val Loss: 0.05480212569306517 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5470/10000 --- Train Loss: 0.07632530020173922 --- Val Loss: 0.05475972079218351 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5480/10000 --- Train Loss: 0.07627678960965917 --- Val Loss: 0.05471488628965948 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5490/10000 --- Train Loss: 0.07622813640035053 --- Val Loss: 0.05467228456194588 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5500/10000 --- Train Loss: 0.0761803688349774 --- Val Loss: 0.05462948346681178 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5510/10000 --- Train Loss: 0.07613239985686235 --- Val Loss: 0.054586933691058964 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5520/10000 --- Train Loss: 0.07608468015328652 --- Val Loss: 0.0545454540842669 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5530/10000 --- Train Loss: 0.07603693147142473 --- Val Loss: 0.054502717106661026 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5540/10000 --- Train Loss: 0.07598892472261348 --- Val Loss: 0.05445808416343894 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5550/10000 --- Train Loss: 0.07594139086638407 --- Val Loss: 0.05441581898245906 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5560/10000 --- Train Loss: 0.07589452927790212 --- Val Loss: 0.05437486993745795 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5570/10000 --- Train Loss: 0.07584719922901956 --- Val Loss: 0.0543319710543595 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5580/10000 --- Train Loss: 0.07580062371792648 --- Val Loss: 0.054291905311516675 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5590/10000 --- Train Loss: 0.07575432991408863 --- Val Loss: 0.05425041972996587 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5600/10000 --- Train Loss: 0.07570805423993666 --- Val Loss: 0.054209597264216855 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5610/10000 --- Train Loss: 0.07566171196315759 --- Val Loss: 0.05416927929986857 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5620/10000 --- Train Loss: 0.07561548102360281 --- Val Loss: 0.054128154227462415 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5630/10000 --- Train Loss: 0.07556912371364334 --- Val Loss: 0.05408712830749174 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5640/10000 --- Train Loss: 0.07552293743262954 --- Val Loss: 0.05404679721181841 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5650/10000 --- Train Loss: 0.0754769193536929 --- Val Loss: 0.054004520459112824 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5660/10000 --- Train Loss: 0.07543144359437103 --- Val Loss: 0.05396501166522831 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5670/10000 --- Train Loss: 0.07538642122048429 --- Val Loss: 0.053925849437830665 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5680/10000 --- Train Loss: 0.07534095144052858 --- Val Loss: 0.053885612185893016 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5690/10000 --- Train Loss: 0.07529508703109246 --- Val Loss: 0.05384580739286708 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5700/10000 --- Train Loss: 0.07525044905651247 --- Val Loss: 0.05380664548784507 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5710/10000 --- Train Loss: 0.07520517959122551 --- Val Loss: 0.05376629595934885 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5720/10000 --- Train Loss: 0.0751606624120355 --- Val Loss: 0.05372795393512337 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5730/10000 --- Train Loss: 0.07511572201051152 --- Val Loss: 0.053688435564223785 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5740/10000 --- Train Loss: 0.0750709773929672 --- Val Loss: 0.05364858683078427 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5750/10000 --- Train Loss: 0.07502715145939089 --- Val Loss: 0.05361188720085186 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5760/10000 --- Train Loss: 0.07498314032472214 --- Val Loss: 0.053572079573377264 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5770/10000 --- Train Loss: 0.07493884018796347 --- Val Loss: 0.05353258341996656 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5780/10000 --- Train Loss: 0.07489510608096658 --- Val Loss: 0.05349468992189331 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5790/10000 --- Train Loss: 0.07485100792904567 --- Val Loss: 0.05345475568227256 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5800/10000 --- Train Loss: 0.07480745818448403 --- Val Loss: 0.05341746488246706 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5810/10000 --- Train Loss: 0.07476425685383833 --- Val Loss: 0.05337982301456804 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5820/10000 --- Train Loss: 0.07472091082503492 --- Val Loss: 0.05334155697414495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5830/10000 --- Train Loss: 0.07467754030265442 --- Val Loss: 0.053303907994605125 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5840/10000 --- Train Loss: 0.07463470385989951 --- Val Loss: 0.05326710086710475 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5850/10000 --- Train Loss: 0.07459127512228705 --- Val Loss: 0.05322965192801343 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5860/10000 --- Train Loss: 0.07454859410729578 --- Val Loss: 0.05319308349274868 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5870/10000 --- Train Loss: 0.07450615939684457 --- Val Loss: 0.05315641053715644 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5880/10000 --- Train Loss: 0.07446406663542199 --- Val Loss: 0.053120727532465536 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5890/10000 --- Train Loss: 0.0744213240472274 --- Val Loss: 0.05308287732815414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5900/10000 --- Train Loss: 0.07437887626713197 --- Val Loss: 0.05304527781900196 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5910/10000 --- Train Loss: 0.07433660651744649 --- Val Loss: 0.0530081887947401 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5920/10000 --- Train Loss: 0.07429475462532485 --- Val Loss: 0.05297309260398119 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5930/10000 --- Train Loss: 0.07425360351708506 --- Val Loss: 0.05293749837731024 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5940/10000 --- Train Loss: 0.07421225068239039 --- Val Loss: 0.05290234997932414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5950/10000 --- Train Loss: 0.07417065719139478 --- Val Loss: 0.052865703628525725 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5960/10000 --- Train Loss: 0.0741292274025747 --- Val Loss: 0.0528298464150721 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5970/10000 --- Train Loss: 0.07408852053135855 --- Val Loss: 0.05279599717421632 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5980/10000 --- Train Loss: 0.07404731981249682 --- Val Loss: 0.05276028966471127 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 5990/10000 --- Train Loss: 0.07400615004659286 --- Val Loss: 0.0527257347582454 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6000/10000 --- Train Loss: 0.07396518350977008 --- Val Loss: 0.052690736813306334 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6010/10000 --- Train Loss: 0.07392509507328537 --- Val Loss: 0.05265668746313366 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6020/10000 --- Train Loss: 0.0738840291687225 --- Val Loss: 0.05261949008288735 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6030/10000 --- Train Loss: 0.07384393805959329 --- Val Loss: 0.05258551517763534 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6040/10000 --- Train Loss: 0.07380365975806519 --- Val Loss: 0.052550099300596786 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6050/10000 --- Train Loss: 0.07376326959367385 --- Val Loss: 0.0525151926497123 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6060/10000 --- Train Loss: 0.07372344101414773 --- Val Loss: 0.052481518947150634 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6070/10000 --- Train Loss: 0.073683246857013 --- Val Loss: 0.05244630676568193 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6080/10000 --- Train Loss: 0.07364366376187406 --- Val Loss: 0.052412350939358456 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6090/10000 --- Train Loss: 0.07360417409332831 --- Val Loss: 0.05237717274928843 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6100/10000 --- Train Loss: 0.07356426798721417 --- Val Loss: 0.052342764087163283 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6110/10000 --- Train Loss: 0.07352474307124224 --- Val Loss: 0.052308256773182765 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6120/10000 --- Train Loss: 0.0734855595414449 --- Val Loss: 0.0522743701304798 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6130/10000 --- Train Loss: 0.07344684587598087 --- Val Loss: 0.05224137696402232 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6140/10000 --- Train Loss: 0.07340768481416976 --- Val Loss: 0.05220747303826641 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6150/10000 --- Train Loss: 0.07336842473817465 --- Val Loss: 0.05217351539912357 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6160/10000 --- Train Loss: 0.07332967240903489 --- Val Loss: 0.05214178012486843 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6170/10000 --- Train Loss: 0.07329046308934807 --- Val Loss: 0.0521073114409074 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6180/10000 --- Train Loss: 0.07325214535735586 --- Val Loss: 0.052076639533551114 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6190/10000 --- Train Loss: 0.07321289424256114 --- Val Loss: 0.0520413515001042 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6200/10000 --- Train Loss: 0.0731744708588674 --- Val Loss: 0.05200965067890324 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6210/10000 --- Train Loss: 0.07313633407682854 --- Val Loss: 0.05197664431737501 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6220/10000 --- Train Loss: 0.07309811343371335 --- Val Loss: 0.05194414854301141 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6230/10000 --- Train Loss: 0.07305955690192147 --- Val Loss: 0.05191137747555924 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6240/10000 --- Train Loss: 0.07302122148624982 --- Val Loss: 0.05187901585103844 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6250/10000 --- Train Loss: 0.07298276895447761 --- Val Loss: 0.05184544516267714 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6260/10000 --- Train Loss: 0.07294486165324708 --- Val Loss: 0.051814118421983274 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6270/10000 --- Train Loss: 0.07290698518017079 --- Val Loss: 0.051782304595804525 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6280/10000 --- Train Loss: 0.0728692974423465 --- Val Loss: 0.05174991945657627 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6290/10000 --- Train Loss: 0.07283190487068858 --- Val Loss: 0.05172002851192597 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6300/10000 --- Train Loss: 0.07279417435705714 --- Val Loss: 0.05168811583561727 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6310/10000 --- Train Loss: 0.07275671085549368 --- Val Loss: 0.05165764094247207 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6320/10000 --- Train Loss: 0.0727189919707646 --- Val Loss: 0.05162423916611173 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6330/10000 --- Train Loss: 0.07268157376375832 --- Val Loss: 0.05159192872952986 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6340/10000 --- Train Loss: 0.07264401491300046 --- Val Loss: 0.0515609725446637 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6350/10000 --- Train Loss: 0.07260707274239099 --- Val Loss: 0.051529864119183394 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6360/10000 --- Train Loss: 0.07257069505397057 --- Val Loss: 0.05150018829007737 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6370/10000 --- Train Loss: 0.07253353665207155 --- Val Loss: 0.05146888955190982 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6380/10000 --- Train Loss: 0.07249695500313291 --- Val Loss: 0.05143754009685006 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6390/10000 --- Train Loss: 0.07245976953227987 --- Val Loss: 0.05140525592238626 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6400/10000 --- Train Loss: 0.07242288565076009 --- Val Loss: 0.05137414119077509 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6410/10000 --- Train Loss: 0.07238643457953031 --- Val Loss: 0.05134418421516257 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6420/10000 --- Train Loss: 0.07234983768204627 --- Val Loss: 0.05131339014986569 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6430/10000 --- Train Loss: 0.07231353067532403 --- Val Loss: 0.05128358883421434 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6440/10000 --- Train Loss: 0.07227717714380634 --- Val Loss: 0.051252881134450326 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6450/10000 --- Train Loss: 0.07224069914335786 --- Val Loss: 0.05122236646985461 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6460/10000 --- Train Loss: 0.07220466116419051 --- Val Loss: 0.051193235133114856 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6470/10000 --- Train Loss: 0.07216895590413817 --- Val Loss: 0.05116369473951854 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6480/10000 --- Train Loss: 0.0721330064543108 --- Val Loss: 0.05113361277976528 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6490/10000 --- Train Loss: 0.07209783842763175 --- Val Loss: 0.05110580953029983 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6500/10000 --- Train Loss: 0.07206232040029079 --- Val Loss: 0.0510760903544139 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6510/10000 --- Train Loss: 0.07202730321332369 --- Val Loss: 0.05104799053800751 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6520/10000 --- Train Loss: 0.07199144966090978 --- Val Loss: 0.051018582897746974 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6530/10000 --- Train Loss: 0.07195607504443284 --- Val Loss: 0.05099011343447164 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6540/10000 --- Train Loss: 0.07192115456624676 --- Val Loss: 0.05096119694651297 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6550/10000 --- Train Loss: 0.07188574282615463 --- Val Loss: 0.05093243938019005 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6560/10000 --- Train Loss: 0.07185052135885218 --- Val Loss: 0.05090390661004073 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6570/10000 --- Train Loss: 0.07181561669839995 --- Val Loss: 0.050874077997456565 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6580/10000 --- Train Loss: 0.0717807281408081 --- Val Loss: 0.05084526773371194 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6590/10000 --- Train Loss: 0.0717456904202577 --- Val Loss: 0.05081741346067989 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6600/10000 --- Train Loss: 0.07171091851732535 --- Val Loss: 0.05078896929188 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6610/10000 --- Train Loss: 0.07167614618417925 --- Val Loss: 0.050759252930892375 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6620/10000 --- Train Loss: 0.0716419124377648 --- Val Loss: 0.050731971634396446 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6630/10000 --- Train Loss: 0.07160772868152057 --- Val Loss: 0.05070422720289739 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6640/10000 --- Train Loss: 0.07157371491936276 --- Val Loss: 0.050676638977267235 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6650/10000 --- Train Loss: 0.07153964922449246 --- Val Loss: 0.05064988136151866 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6660/10000 --- Train Loss: 0.07150537013404783 --- Val Loss: 0.05062192824611867 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6670/10000 --- Train Loss: 0.0714710372933525 --- Val Loss: 0.05059363423126612 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6680/10000 --- Train Loss: 0.07143696675935476 --- Val Loss: 0.05056581813379506 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6690/10000 --- Train Loss: 0.0714027246590928 --- Val Loss: 0.050537328626876904 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6700/10000 --- Train Loss: 0.07136869830936741 --- Val Loss: 0.05051017904964604 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6710/10000 --- Train Loss: 0.07133473212211507 --- Val Loss: 0.05048286605669564 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6720/10000 --- Train Loss: 0.07130128201030386 --- Val Loss: 0.05045587807328 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6730/10000 --- Train Loss: 0.07126759688048423 --- Val Loss: 0.050428436933463244 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6740/10000 --- Train Loss: 0.07123444107596111 --- Val Loss: 0.05040228885287395 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6750/10000 --- Train Loss: 0.07120125952307037 --- Val Loss: 0.05037535169460459 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6760/10000 --- Train Loss: 0.07116787375923862 --- Val Loss: 0.050348023642188454 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6770/10000 --- Train Loss: 0.07113458513658119 --- Val Loss: 0.050319822982312155 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6780/10000 --- Train Loss: 0.07110138648485079 --- Val Loss: 0.050292652876292414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6790/10000 --- Train Loss: 0.07106801311325985 --- Val Loss: 0.05026442411868347 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6800/10000 --- Train Loss: 0.07103512378503105 --- Val Loss: 0.05023685989609392 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6810/10000 --- Train Loss: 0.0710022172031009 --- Val Loss: 0.05021033031316239 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6820/10000 --- Train Loss: 0.07096877267990635 --- Val Loss: 0.050182529779136245 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6830/10000 --- Train Loss: 0.07093601624797014 --- Val Loss: 0.050154737599944814 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6840/10000 --- Train Loss: 0.07090312305891915 --- Val Loss: 0.05012746640824894 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6850/10000 --- Train Loss: 0.0708706002456043 --- Val Loss: 0.05010032183800679 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6860/10000 --- Train Loss: 0.07083773420482174 --- Val Loss: 0.05007303098386908 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6870/10000 --- Train Loss: 0.0708051610667335 --- Val Loss: 0.05004664944933276 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6880/10000 --- Train Loss: 0.07077308635312572 --- Val Loss: 0.05002138684009101 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6890/10000 --- Train Loss: 0.07074091256029952 --- Val Loss: 0.049994430624357884 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6900/10000 --- Train Loss: 0.0707087436379377 --- Val Loss: 0.04996860530384647 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6910/10000 --- Train Loss: 0.07067660858151205 --- Val Loss: 0.04994214552485107 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6920/10000 --- Train Loss: 0.07064448512033603 --- Val Loss: 0.04991607153573866 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6930/10000 --- Train Loss: 0.07061281594058197 --- Val Loss: 0.049890166002252434 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6940/10000 --- Train Loss: 0.07058057250961851 --- Val Loss: 0.049863149224182536 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6950/10000 --- Train Loss: 0.07054925694419503 --- Val Loss: 0.049839133154220595 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6960/10000 --- Train Loss: 0.0705172322893659 --- Val Loss: 0.049812075981196365 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6970/10000 --- Train Loss: 0.07048564732603596 --- Val Loss: 0.04978732806162871 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6980/10000 --- Train Loss: 0.0704537008517945 --- Val Loss: 0.049760384218897113 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 6990/10000 --- Train Loss: 0.07042181783011366 --- Val Loss: 0.04973360958556516 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7000/10000 --- Train Loss: 0.07038993508526187 --- Val Loss: 0.04970759780283324 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7010/10000 --- Train Loss: 0.07035844474018997 --- Val Loss: 0.04968236532643481 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7020/10000 --- Train Loss: 0.070327073722837 --- Val Loss: 0.049656753640026675 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7030/10000 --- Train Loss: 0.07029561374505797 --- Val Loss: 0.049630929883199185 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7040/10000 --- Train Loss: 0.07026463809651363 --- Val Loss: 0.04960585766263579 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7050/10000 --- Train Loss: 0.07023379301913253 --- Val Loss: 0.04958099409894596 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7060/10000 --- Train Loss: 0.07020282625474074 --- Val Loss: 0.049555845025440894 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7070/10000 --- Train Loss: 0.07017208337686215 --- Val Loss: 0.0495304078485863 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7080/10000 --- Train Loss: 0.07014137104728073 --- Val Loss: 0.049505662821990724 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7090/10000 --- Train Loss: 0.07011021380298368 --- Val Loss: 0.049479732600114304 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7100/10000 --- Train Loss: 0.07007941516166963 --- Val Loss: 0.049455010180944135 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7110/10000 --- Train Loss: 0.07004858885293183 --- Val Loss: 0.049429843430932874 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7120/10000 --- Train Loss: 0.07001756351667746 --- Val Loss: 0.04940407444353598 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7130/10000 --- Train Loss: 0.06998696453807696 --- Val Loss: 0.04937847859104845 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7140/10000 --- Train Loss: 0.06995651637034143 --- Val Loss: 0.04935371045165734 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7150/10000 --- Train Loss: 0.06992633789998175 --- Val Loss: 0.049329045546986715 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7160/10000 --- Train Loss: 0.06989623703518501 --- Val Loss: 0.049305444451029 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7170/10000 --- Train Loss: 0.06986589665778631 --- Val Loss: 0.049282087217131315 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7180/10000 --- Train Loss: 0.06983608678285601 --- Val Loss: 0.04925636584584941 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7190/10000 --- Train Loss: 0.06980582404362214 --- Val Loss: 0.04923244015456455 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7200/10000 --- Train Loss: 0.0697758251596289 --- Val Loss: 0.049207674298021545 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7210/10000 --- Train Loss: 0.06974561916245671 --- Val Loss: 0.04918216854353566 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7220/10000 --- Train Loss: 0.06971556170653515 --- Val Loss: 0.04915866016581609 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7230/10000 --- Train Loss: 0.06968513920720855 --- Val Loss: 0.049132402931769056 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7240/10000 --- Train Loss: 0.06965555406734793 --- Val Loss: 0.0491078238212888 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7250/10000 --- Train Loss: 0.06962541570215812 --- Val Loss: 0.049083489584408994 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7260/10000 --- Train Loss: 0.0695954897517395 --- Val Loss: 0.04906001360319344 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7270/10000 --- Train Loss: 0.06956612021342692 --- Val Loss: 0.04903646524176124 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7280/10000 --- Train Loss: 0.06953615546385794 --- Val Loss: 0.04901189196959963 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7290/10000 --- Train Loss: 0.06950681563106474 --- Val Loss: 0.048987802189046574 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7300/10000 --- Train Loss: 0.06947701435181909 --- Val Loss: 0.04896319213150257 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7310/10000 --- Train Loss: 0.06944735442849023 --- Val Loss: 0.04893928523430094 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7320/10000 --- Train Loss: 0.06941773623124409 --- Val Loss: 0.04891507615567664 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7330/10000 --- Train Loss: 0.06938822396687418 --- Val Loss: 0.048890838181428156 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7340/10000 --- Train Loss: 0.06935929858790066 --- Val Loss: 0.04886815118546271 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7350/10000 --- Train Loss: 0.06932937563124841 --- Val Loss: 0.048843065285945245 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7360/10000 --- Train Loss: 0.06930082940604271 --- Val Loss: 0.048821395782671084 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7370/10000 --- Train Loss: 0.06927178134955558 --- Val Loss: 0.048797409591600054 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7380/10000 --- Train Loss: 0.06924266429468355 --- Val Loss: 0.048773370580118325 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7390/10000 --- Train Loss: 0.0692137945957912 --- Val Loss: 0.04874978819054645 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7400/10000 --- Train Loss: 0.06918507072699374 --- Val Loss: 0.04872661860951538 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7410/10000 --- Train Loss: 0.06915672175472558 --- Val Loss: 0.04870352923152139 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7420/10000 --- Train Loss: 0.06912774258006782 --- Val Loss: 0.048679370300623574 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7430/10000 --- Train Loss: 0.06909904323178927 --- Val Loss: 0.048655699947537906 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7440/10000 --- Train Loss: 0.06907038908430135 --- Val Loss: 0.04863182016225351 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7450/10000 --- Train Loss: 0.06904214909040045 --- Val Loss: 0.0486089091844984 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7460/10000 --- Train Loss: 0.06901353527094574 --- Val Loss: 0.04858492144813261 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7470/10000 --- Train Loss: 0.06898518928430725 --- Val Loss: 0.04856175844361786 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7480/10000 --- Train Loss: 0.06895632804301566 --- Val Loss: 0.04853811745570571 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7490/10000 --- Train Loss: 0.06892846844675139 --- Val Loss: 0.048513869969787876 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7500/10000 --- Train Loss: 0.06889975923412256 --- Val Loss: 0.04849164557560003 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7510/10000 --- Train Loss: 0.06887129577594592 --- Val Loss: 0.04846855388863211 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7520/10000 --- Train Loss: 0.06884319095361532 --- Val Loss: 0.04844540234545617 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7530/10000 --- Train Loss: 0.06881482288107771 --- Val Loss: 0.04842164244400681 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7540/10000 --- Train Loss: 0.06878720828848088 --- Val Loss: 0.048399667806169076 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7550/10000 --- Train Loss: 0.06875937803880099 --- Val Loss: 0.04837709453481589 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7560/10000 --- Train Loss: 0.06873213014587606 --- Val Loss: 0.048355121904557465 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7570/10000 --- Train Loss: 0.06870436837736714 --- Val Loss: 0.04833217419034387 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7580/10000 --- Train Loss: 0.06867682011831835 --- Val Loss: 0.048311004482858834 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7590/10000 --- Train Loss: 0.0686491731353148 --- Val Loss: 0.048288332612113204 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7600/10000 --- Train Loss: 0.06862131400153745 --- Val Loss: 0.04826606822183951 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7610/10000 --- Train Loss: 0.06859368860573346 --- Val Loss: 0.048243092170478924 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7620/10000 --- Train Loss: 0.06856575603620381 --- Val Loss: 0.04821978580270488 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7630/10000 --- Train Loss: 0.06853842475677085 --- Val Loss: 0.048197174575961205 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7640/10000 --- Train Loss: 0.0685114444289554 --- Val Loss: 0.04817517588825502 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7650/10000 --- Train Loss: 0.06848370287866966 --- Val Loss: 0.048152064474985795 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7660/10000 --- Train Loss: 0.06845639133652291 --- Val Loss: 0.04813046570933214 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7670/10000 --- Train Loss: 0.06842883037867194 --- Val Loss: 0.048107072184776015 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7680/10000 --- Train Loss: 0.06840124100310192 --- Val Loss: 0.04808535766253403 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7690/10000 --- Train Loss: 0.06837369863702378 --- Val Loss: 0.04806310977893344 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7700/10000 --- Train Loss: 0.06834652903758823 --- Val Loss: 0.048041694020618864 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7710/10000 --- Train Loss: 0.06831919918839971 --- Val Loss: 0.048018536835298255 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7720/10000 --- Train Loss: 0.06829134311323794 --- Val Loss: 0.047996388391423714 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7730/10000 --- Train Loss: 0.06826444291156605 --- Val Loss: 0.047974310955232545 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7740/10000 --- Train Loss: 0.06823744558428706 --- Val Loss: 0.047953608198708794 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7750/10000 --- Train Loss: 0.06821055457813878 --- Val Loss: 0.0479322881360007 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7760/10000 --- Train Loss: 0.0681830785372879 --- Val Loss: 0.04790993419205047 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7770/10000 --- Train Loss: 0.06815604190129247 --- Val Loss: 0.04788817882574536 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7780/10000 --- Train Loss: 0.06812957379312665 --- Val Loss: 0.047867430290893444 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7790/10000 --- Train Loss: 0.06810292893467211 --- Val Loss: 0.04784483246746854 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7800/10000 --- Train Loss: 0.06807585919624905 --- Val Loss: 0.047823350477566536 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7810/10000 --- Train Loss: 0.06804881135952008 --- Val Loss: 0.047800978887478224 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7820/10000 --- Train Loss: 0.06802217192431197 --- Val Loss: 0.04777933148893257 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7830/10000 --- Train Loss: 0.06799537290242286 --- Val Loss: 0.0477589281468684 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7840/10000 --- Train Loss: 0.06796849313729954 --- Val Loss: 0.04773707968584086 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7850/10000 --- Train Loss: 0.0679420562218531 --- Val Loss: 0.047714191999024644 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7860/10000 --- Train Loss: 0.06791515405937452 --- Val Loss: 0.047692055701346436 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7870/10000 --- Train Loss: 0.06788862097329733 --- Val Loss: 0.04767005295066221 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7880/10000 --- Train Loss: 0.06786193791352893 --- Val Loss: 0.04764782943750226 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7890/10000 --- Train Loss: 0.06783515352157606 --- Val Loss: 0.0476252677663233 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7900/10000 --- Train Loss: 0.06780912324224032 --- Val Loss: 0.04760392330909865 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7910/10000 --- Train Loss: 0.06778260653373583 --- Val Loss: 0.047581763758873005 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7920/10000 --- Train Loss: 0.06775630912238348 --- Val Loss: 0.04756051968181356 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7930/10000 --- Train Loss: 0.06773048172621476 --- Val Loss: 0.04753896300034531 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7940/10000 --- Train Loss: 0.06770379986437584 --- Val Loss: 0.04751704235606161 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7950/10000 --- Train Loss: 0.06767775825826065 --- Val Loss: 0.04749608891633237 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7960/10000 --- Train Loss: 0.06765186434748043 --- Val Loss: 0.0474749666042142 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7970/10000 --- Train Loss: 0.06762556171601174 --- Val Loss: 0.04745303447678414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7980/10000 --- Train Loss: 0.06759946681668255 --- Val Loss: 0.047432011300716344 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 7990/10000 --- Train Loss: 0.06757336219702184 --- Val Loss: 0.047410301409904605 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8000/10000 --- Train Loss: 0.06754724142363461 --- Val Loss: 0.047387911471409416 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8010/10000 --- Train Loss: 0.06752121243804547 --- Val Loss: 0.04736536072928772 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8020/10000 --- Train Loss: 0.06749552887039065 --- Val Loss: 0.04734427858919695 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8030/10000 --- Train Loss: 0.06746927245851504 --- Val Loss: 0.04732200343869036 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8040/10000 --- Train Loss: 0.06744387332395965 --- Val Loss: 0.0473016790428651 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8050/10000 --- Train Loss: 0.06741779100864514 --- Val Loss: 0.04727912724313183 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8060/10000 --- Train Loss: 0.06739214876306594 --- Val Loss: 0.047258598130263105 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8070/10000 --- Train Loss: 0.06736632552115684 --- Val Loss: 0.047237453608637216 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8080/10000 --- Train Loss: 0.06734089978604337 --- Val Loss: 0.04721629373193314 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8090/10000 --- Train Loss: 0.06731530101093224 --- Val Loss: 0.04719411346640189 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8100/10000 --- Train Loss: 0.06728982410617258 --- Val Loss: 0.04717316042620233 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8110/10000 --- Train Loss: 0.06726493341405312 --- Val Loss: 0.04715360500921332 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8120/10000 --- Train Loss: 0.06723935682728953 --- Val Loss: 0.047131815334045195 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8130/10000 --- Train Loss: 0.0672141541336026 --- Val Loss: 0.04711098568245045 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8140/10000 --- Train Loss: 0.06718910161934327 --- Val Loss: 0.047090402850101847 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8150/10000 --- Train Loss: 0.0671634338736805 --- Val Loss: 0.04706966370503278 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8160/10000 --- Train Loss: 0.06713835901463445 --- Val Loss: 0.047049359518217586 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8170/10000 --- Train Loss: 0.06711302216550571 --- Val Loss: 0.04702797489523284 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8180/10000 --- Train Loss: 0.0670876969070622 --- Val Loss: 0.04700647184224516 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8190/10000 --- Train Loss: 0.06706288973874952 --- Val Loss: 0.046987012028265995 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8200/10000 --- Train Loss: 0.06703791033377324 --- Val Loss: 0.04696585833639744 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8210/10000 --- Train Loss: 0.06701224726411524 --- Val Loss: 0.04694352264263474 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8220/10000 --- Train Loss: 0.0669868506359889 --- Val Loss: 0.046922285947534066 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8230/10000 --- Train Loss: 0.06696172239570027 --- Val Loss: 0.04690108359855459 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8240/10000 --- Train Loss: 0.06693645937909747 --- Val Loss: 0.04687988388998512 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8250/10000 --- Train Loss: 0.06691226818882957 --- Val Loss: 0.04686078322351211 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8260/10000 --- Train Loss: 0.06688728101576995 --- Val Loss: 0.04684015890677502 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8270/10000 --- Train Loss: 0.06686248910127268 --- Val Loss: 0.04681913198521714 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8280/10000 --- Train Loss: 0.0668376560404922 --- Val Loss: 0.04679807248546578 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8290/10000 --- Train Loss: 0.06681354756191905 --- Val Loss: 0.046777649452871745 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8300/10000 --- Train Loss: 0.06678946430057336 --- Val Loss: 0.046757997815390626 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8310/10000 --- Train Loss: 0.06676492313024554 --- Val Loss: 0.04673799282465131 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8320/10000 --- Train Loss: 0.06674027573017323 --- Val Loss: 0.04671759286942414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8330/10000 --- Train Loss: 0.06671578127706532 --- Val Loss: 0.046696635760884356 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8340/10000 --- Train Loss: 0.06669128689159046 --- Val Loss: 0.04667592912709926 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8350/10000 --- Train Loss: 0.06666744218952428 --- Val Loss: 0.046655982700397405 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8360/10000 --- Train Loss: 0.06664281202345228 --- Val Loss: 0.04663546999600055 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8370/10000 --- Train Loss: 0.06661871373184364 --- Val Loss: 0.04661545817404586 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8380/10000 --- Train Loss: 0.06659422770518349 --- Val Loss: 0.04659455583615951 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8390/10000 --- Train Loss: 0.06657085035691264 --- Val Loss: 0.04657647477092947 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8400/10000 --- Train Loss: 0.06654688307732295 --- Val Loss: 0.04655614917889027 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8410/10000 --- Train Loss: 0.06652294486635345 --- Val Loss: 0.04653730932884276 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8420/10000 --- Train Loss: 0.06649864387599524 --- Val Loss: 0.0465166517356315 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8430/10000 --- Train Loss: 0.06647452035383168 --- Val Loss: 0.04649561669972185 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8440/10000 --- Train Loss: 0.0664500730842445 --- Val Loss: 0.04647418514709664 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8450/10000 --- Train Loss: 0.06642610145919836 --- Val Loss: 0.046453573119668905 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8460/10000 --- Train Loss: 0.06640259082708443 --- Val Loss: 0.04643481582733761 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8470/10000 --- Train Loss: 0.06637832772952974 --- Val Loss: 0.04641428562451437 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8480/10000 --- Train Loss: 0.06635491726286284 --- Val Loss: 0.04639557848582192 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8490/10000 --- Train Loss: 0.06633138578805449 --- Val Loss: 0.04637539938024523 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8500/10000 --- Train Loss: 0.06630819635609778 --- Val Loss: 0.046357301934250306 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8510/10000 --- Train Loss: 0.06628441641799279 --- Val Loss: 0.04633772927955204 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8520/10000 --- Train Loss: 0.06626060596866216 --- Val Loss: 0.046317255147352084 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8530/10000 --- Train Loss: 0.06623687609976285 --- Val Loss: 0.04629687810948249 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8540/10000 --- Train Loss: 0.06621302547949853 --- Val Loss: 0.046275835317750154 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8550/10000 --- Train Loss: 0.06618971061985153 --- Val Loss: 0.04625627615356756 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8560/10000 --- Train Loss: 0.06616616041998981 --- Val Loss: 0.046237050188378384 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8570/10000 --- Train Loss: 0.06614222069777036 --- Val Loss: 0.04621665856940686 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8580/10000 --- Train Loss: 0.06611860147678802 --- Val Loss: 0.046197307357686984 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8590/10000 --- Train Loss: 0.06609519799438965 --- Val Loss: 0.04617726397803084 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8600/10000 --- Train Loss: 0.06607165837056453 --- Val Loss: 0.04615696432598712 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8610/10000 --- Train Loss: 0.06604872869978126 --- Val Loss: 0.046137955594619806 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8620/10000 --- Train Loss: 0.0660251066676129 --- Val Loss: 0.04611713344690801 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8630/10000 --- Train Loss: 0.06600187121529591 --- Val Loss: 0.04609775610150217 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8640/10000 --- Train Loss: 0.06597879984945225 --- Val Loss: 0.04607902773736218 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8650/10000 --- Train Loss: 0.06595598043128799 --- Val Loss: 0.04606009444059275 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8660/10000 --- Train Loss: 0.0659328387741211 --- Val Loss: 0.0460413089394445 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8670/10000 --- Train Loss: 0.06590986004421356 --- Val Loss: 0.046021674172799426 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8680/10000 --- Train Loss: 0.06588742861116899 --- Val Loss: 0.04600326369458642 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8690/10000 --- Train Loss: 0.06586469194294757 --- Val Loss: 0.04598430688783932 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8700/10000 --- Train Loss: 0.0658414097032532 --- Val Loss: 0.045964664365187424 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8710/10000 --- Train Loss: 0.06581831121615153 --- Val Loss: 0.045945574749942986 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8720/10000 --- Train Loss: 0.06579480586170525 --- Val Loss: 0.04592591560647632 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8730/10000 --- Train Loss: 0.06577189008663381 --- Val Loss: 0.045905873922141495 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8740/10000 --- Train Loss: 0.06574844395681023 --- Val Loss: 0.04588692567039437 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8750/10000 --- Train Loss: 0.06572619481276495 --- Val Loss: 0.04586821383356892 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8760/10000 --- Train Loss: 0.06570386406901656 --- Val Loss: 0.04584958898885963 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8770/10000 --- Train Loss: 0.06568148478376191 --- Val Loss: 0.04583061658476089 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8780/10000 --- Train Loss: 0.0656588609425639 --- Val Loss: 0.045811887265907576 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8790/10000 --- Train Loss: 0.06563662803368575 --- Val Loss: 0.04579369134701449 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8800/10000 --- Train Loss: 0.06561376175350639 --- Val Loss: 0.04577396860254875 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8810/10000 --- Train Loss: 0.06559123480233822 --- Val Loss: 0.045755262420960285 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8820/10000 --- Train Loss: 0.06556840164291126 --- Val Loss: 0.04573693811912271 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8830/10000 --- Train Loss: 0.06554673419229072 --- Val Loss: 0.045719224500059816 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8840/10000 --- Train Loss: 0.06552471380392837 --- Val Loss: 0.045701935122100336 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8850/10000 --- Train Loss: 0.06550141736365475 --- Val Loss: 0.04568167405701775 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8860/10000 --- Train Loss: 0.06547905380621916 --- Val Loss: 0.04566284870540076 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8870/10000 --- Train Loss: 0.06545657371748073 --- Val Loss: 0.045644271719130006 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8880/10000 --- Train Loss: 0.06543415848335798 --- Val Loss: 0.04562495895277873 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8890/10000 --- Train Loss: 0.06541173994493737 --- Val Loss: 0.04560656784684913 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8900/10000 --- Train Loss: 0.0653896690993641 --- Val Loss: 0.0455876739213833 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8910/10000 --- Train Loss: 0.06536762935884846 --- Val Loss: 0.0455707324028772 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8920/10000 --- Train Loss: 0.06534603623896393 --- Val Loss: 0.045553612105297125 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8930/10000 --- Train Loss: 0.06532383322342233 --- Val Loss: 0.04553493788710198 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8940/10000 --- Train Loss: 0.06530173373068775 --- Val Loss: 0.04551657028425783 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8950/10000 --- Train Loss: 0.06528001872089774 --- Val Loss: 0.04549864768461839 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8960/10000 --- Train Loss: 0.06525732545631337 --- Val Loss: 0.045479030868735705 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8970/10000 --- Train Loss: 0.06523537654643008 --- Val Loss: 0.045461826203159396 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8980/10000 --- Train Loss: 0.06521345191720801 --- Val Loss: 0.0454433035337506 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 8990/10000 --- Train Loss: 0.06519202300816894 --- Val Loss: 0.04542543688430289 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9000/10000 --- Train Loss: 0.06516956553805446 --- Val Loss: 0.04540636348734278 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9010/10000 --- Train Loss: 0.065147959588414 --- Val Loss: 0.045388213238646086 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9020/10000 --- Train Loss: 0.06512652709211483 --- Val Loss: 0.04537049380209873 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9030/10000 --- Train Loss: 0.06510516997017833 --- Val Loss: 0.045353265779588266 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9040/10000 --- Train Loss: 0.06508381358874711 --- Val Loss: 0.04533583001690193 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9050/10000 --- Train Loss: 0.0650617730437848 --- Val Loss: 0.045317157455933094 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9060/10000 --- Train Loss: 0.0650396661074351 --- Val Loss: 0.04529905115056053 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9070/10000 --- Train Loss: 0.06501839269459177 --- Val Loss: 0.04528159841627884 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9080/10000 --- Train Loss: 0.06499633803494034 --- Val Loss: 0.04526321398579598 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9090/10000 --- Train Loss: 0.06497515413871874 --- Val Loss: 0.045246676731066436 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9100/10000 --- Train Loss: 0.06495408872435084 --- Val Loss: 0.04522962181106926 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9110/10000 --- Train Loss: 0.06493236135255465 --- Val Loss: 0.04521183214984814 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9120/10000 --- Train Loss: 0.06491139705843277 --- Val Loss: 0.045194904785751305 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9130/10000 --- Train Loss: 0.06488978357344187 --- Val Loss: 0.04517634313731458 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9140/10000 --- Train Loss: 0.06486797618338937 --- Val Loss: 0.045157526823358586 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9150/10000 --- Train Loss: 0.0648466410039177 --- Val Loss: 0.04514018967357785 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9160/10000 --- Train Loss: 0.06482539262374785 --- Val Loss: 0.04512260474444735 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9170/10000 --- Train Loss: 0.06480490569473414 --- Val Loss: 0.04510612240684349 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9180/10000 --- Train Loss: 0.06478385431443445 --- Val Loss: 0.04508812833737336 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9190/10000 --- Train Loss: 0.06476276702163265 --- Val Loss: 0.045071399939452333 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9200/10000 --- Train Loss: 0.06474150676046515 --- Val Loss: 0.04505397238757223 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9210/10000 --- Train Loss: 0.06472018639037729 --- Val Loss: 0.04503536180824547 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9220/10000 --- Train Loss: 0.06469910896836571 --- Val Loss: 0.045018321864223415 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9230/10000 --- Train Loss: 0.0646779555219729 --- Val Loss: 0.045000505620659915 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9240/10000 --- Train Loss: 0.06465694721296662 --- Val Loss: 0.044983791379675696 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9250/10000 --- Train Loss: 0.06463566846228667 --- Val Loss: 0.044965245942405994 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9260/10000 --- Train Loss: 0.06461417311891954 --- Val Loss: 0.04494761003870252 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9270/10000 --- Train Loss: 0.0645930330421109 --- Val Loss: 0.04492985441373207 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9280/10000 --- Train Loss: 0.06457178908080216 --- Val Loss: 0.04491111595917681 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9290/10000 --- Train Loss: 0.06455120218047967 --- Val Loss: 0.04489514231554022 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9300/10000 --- Train Loss: 0.06452968375493898 --- Val Loss: 0.044877382692216077 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9310/10000 --- Train Loss: 0.06450837969803883 --- Val Loss: 0.044858839786647786 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9320/10000 --- Train Loss: 0.06448783612650767 --- Val Loss: 0.044841362171349965 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9330/10000 --- Train Loss: 0.06446785915249029 --- Val Loss: 0.04482624986210631 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9340/10000 --- Train Loss: 0.0644469725768284 --- Val Loss: 0.04480850755382791 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9350/10000 --- Train Loss: 0.06442594579645321 --- Val Loss: 0.04479082310057754 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9360/10000 --- Train Loss: 0.0644053545575811 --- Val Loss: 0.04477401876558837 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9370/10000 --- Train Loss: 0.06438518244973437 --- Val Loss: 0.04475854864787271 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9380/10000 --- Train Loss: 0.0643643057193616 --- Val Loss: 0.044740600339332415 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9390/10000 --- Train Loss: 0.06434358799269847 --- Val Loss: 0.044723524717091116 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9400/10000 --- Train Loss: 0.06432276939441567 --- Val Loss: 0.04470485001562047 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9410/10000 --- Train Loss: 0.06430198573581826 --- Val Loss: 0.04468779516180306 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9420/10000 --- Train Loss: 0.06428167010316925 --- Val Loss: 0.044671293185943164 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9430/10000 --- Train Loss: 0.06426069702842839 --- Val Loss: 0.044653012867101674 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9440/10000 --- Train Loss: 0.06423973153597744 --- Val Loss: 0.04463509341168116 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9450/10000 --- Train Loss: 0.0642202755787999 --- Val Loss: 0.04461963982219643 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9460/10000 --- Train Loss: 0.06419947417669741 --- Val Loss: 0.04460248273763193 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9470/10000 --- Train Loss: 0.06417869995319928 --- Val Loss: 0.04458365678654915 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9480/10000 --- Train Loss: 0.06415833518353693 --- Val Loss: 0.044565986790293444 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9490/10000 --- Train Loss: 0.0641371385006067 --- Val Loss: 0.04454806132723413 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9500/10000 --- Train Loss: 0.06411612517990747 --- Val Loss: 0.04452859170645088 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9510/10000 --- Train Loss: 0.0640955264322912 --- Val Loss: 0.044510387915232735 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9520/10000 --- Train Loss: 0.06407474670269446 --- Val Loss: 0.04449298359671689 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9530/10000 --- Train Loss: 0.06405430218372937 --- Val Loss: 0.04447536872072663 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9540/10000 --- Train Loss: 0.06403370850143093 --- Val Loss: 0.04445693489034682 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9550/10000 --- Train Loss: 0.06401307803247554 --- Val Loss: 0.04443940596412539 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9560/10000 --- Train Loss: 0.06399276336350956 --- Val Loss: 0.044421542713332644 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9570/10000 --- Train Loss: 0.06397227534573441 --- Val Loss: 0.04440141934293338 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9580/10000 --- Train Loss: 0.06395173175949408 --- Val Loss: 0.04438336093234334 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9590/10000 --- Train Loss: 0.06393131829523878 --- Val Loss: 0.04436509688873354 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9600/10000 --- Train Loss: 0.06391090445270114 --- Val Loss: 0.04434774199978442 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9610/10000 --- Train Loss: 0.06389039291434664 --- Val Loss: 0.044328618293285056 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9620/10000 --- Train Loss: 0.06387009207971103 --- Val Loss: 0.044310405428849446 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9630/10000 --- Train Loss: 0.06385047815705665 --- Val Loss: 0.044293177531726506 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9640/10000 --- Train Loss: 0.0638300680467693 --- Val Loss: 0.04427541451621435 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9650/10000 --- Train Loss: 0.06380988925515887 --- Val Loss: 0.04425773676489165 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9660/10000 --- Train Loss: 0.06378969509081539 --- Val Loss: 0.04423897487580889 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9670/10000 --- Train Loss: 0.0637698032310246 --- Val Loss: 0.04422191245158436 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9680/10000 --- Train Loss: 0.06374934821944937 --- Val Loss: 0.044204302674385444 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9690/10000 --- Train Loss: 0.06372914442253957 --- Val Loss: 0.044186515330266414 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9700/10000 --- Train Loss: 0.06370892111152958 --- Val Loss: 0.04416791258829057 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9710/10000 --- Train Loss: 0.06368841428304216 --- Val Loss: 0.04414939681504397 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9720/10000 --- Train Loss: 0.06366803548313117 --- Val Loss: 0.04413071587668622 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9730/10000 --- Train Loss: 0.06364848160981051 --- Val Loss: 0.04411323402808357 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9740/10000 --- Train Loss: 0.06362836938088759 --- Val Loss: 0.04409553544800079 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9750/10000 --- Train Loss: 0.06360856050561954 --- Val Loss: 0.04407787544958969 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9760/10000 --- Train Loss: 0.0635882544281177 --- Val Loss: 0.04405916741565532 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9770/10000 --- Train Loss: 0.0635684783267207 --- Val Loss: 0.044041047481777625 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9780/10000 --- Train Loss: 0.0635480538117003 --- Val Loss: 0.04402177151753261 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9790/10000 --- Train Loss: 0.06352803073847138 --- Val Loss: 0.04400308349411048 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9800/10000 --- Train Loss: 0.06350834642387931 --- Val Loss: 0.04398703739046909 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9810/10000 --- Train Loss: 0.06348831064645163 --- Val Loss: 0.043968337428016854 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9820/10000 --- Train Loss: 0.06346882429721507 --- Val Loss: 0.04395060867299515 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9830/10000 --- Train Loss: 0.06344880267566191 --- Val Loss: 0.0439315918609923 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9840/10000 --- Train Loss: 0.06342900497203312 --- Val Loss: 0.04391456415221403 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9850/10000 --- Train Loss: 0.06340915205322116 --- Val Loss: 0.04389538601627696 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9860/10000 --- Train Loss: 0.06338958758451003 --- Val Loss: 0.043878362653301466 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9870/10000 --- Train Loss: 0.06336958151529767 --- Val Loss: 0.04386089088003742 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9880/10000 --- Train Loss: 0.06334965210742013 --- Val Loss: 0.04384267174768275 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9890/10000 --- Train Loss: 0.06332958856330125 --- Val Loss: 0.04382445942244631 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9900/10000 --- Train Loss: 0.06330999238325528 --- Val Loss: 0.04380672568876878 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9910/10000 --- Train Loss: 0.06329055591856833 --- Val Loss: 0.04378769190652205 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9920/10000 --- Train Loss: 0.06327051819982192 --- Val Loss: 0.04376994958496139 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9930/10000 --- Train Loss: 0.06325121004443254 --- Val Loss: 0.043752751418402965 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9940/10000 --- Train Loss: 0.06323158710329597 --- Val Loss: 0.043734404046911084 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9950/10000 --- Train Loss: 0.06321177532747504 --- Val Loss: 0.04371648536676616 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9960/10000 --- Train Loss: 0.06319185693573941 --- Val Loss: 0.04369826573766731 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9970/10000 --- Train Loss: 0.06317257984120571 --- Val Loss: 0.04368188649610701 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9980/10000 --- Train Loss: 0.06315293846102468 --- Val Loss: 0.04366343622356647 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 9990/10000 --- Train Loss: 0.06313367307996257 --- Val Loss: 0.043645123738874636 --- Train Acc: 0.98 --- Val Acc: 0.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=10000, learning_rate=0.001, batch_size=16)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "* optimization of hyperparameters (random search and grid search function?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
