{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380f9332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, l1=0.0, l2=0.0):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # Initialise (weights) velocity terms for momentum optimization\n",
    "        self.velocity_biases = np.zeros_like(self.biases) # Initialise (biases) velocity terms for momentum optimization\n",
    "        self.l1 = l1 # L1 regularization coefficient (default 0.0).\n",
    "        self.l2 = l2 # L2 regularization coefficient (default 0.0).\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        '''\n",
    "        # Regularization terms\n",
    "        l1_reg = self.l1 * np.sign(self.weights)\n",
    "        l2_reg = self.l2 * self.weights\n",
    "\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient) + l1_reg + l2_reg\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if optimizer == 'GD':\n",
    "            # Update weights and biases\n",
    "            self.weights += learning_rate * weights_gradient\n",
    "            self.biases += learning_rate * biases_gradient\n",
    "        elif optimizer == 'Momentum':\n",
    "            # Momentum update for weights and biases\n",
    "            self.velocity_weights = momentum * self.velocity_weights + learning_rate * weights_gradient\n",
    "            self.velocity_biases = momentum * self.velocity_biases + learning_rate * biases_gradient\n",
    "\n",
    "            # Update weights and biases using velocity\n",
    "            self.weights += self.velocity_weights\n",
    "            self.biases += self.velocity_biases\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for printing/plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for printing/plotting\n",
    "        self.accuracy_history = [] #placeholder to store the (train) accuracy for printing/plotting\n",
    "        self.val_accuracy_history = [] #placeholder to store the accuracy calculated on the validation set for printing/plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Computes the accuracy of predictions by comparing them with the true labels. \n",
    "        Accuracy is computed as the proportion of correct predictions to the total number of predictions.\n",
    "        '''\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate, optimizer, momentum)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, optimizer='GD', momentum=0.9, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate, optimizer, momentum) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_predictions = self.predict(X_train)\n",
    "            train_accuracy = self.compute_accuracy(train_predictions, np.argmax(y_train, axis=1))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_predictions = self.predict(X_val)\n",
    "            val_accuracy = self.compute_accuracy(val_predictions, np.argmax(y_val, axis=1))\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.plot(self.accuracy_history, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracy_history, label='Val Accuracy')\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 2.302356789917726 --- Val Loss: 2.3030626335665714 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/1000 --- Train Loss: 2.3015546697331475 --- Val Loss: 2.30481692923134 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: 2.301546929308975 --- Val Loss: 2.305424323508293 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 30/1000 --- Train Loss: 2.301547701748579 --- Val Loss: 2.305904673126399 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/1000 --- Train Loss: 2.3015448387861417 --- Val Loss: 2.3056093337588055 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.301543182229616 --- Val Loss: 2.305386636037665 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: 2.3015400500602885 --- Val Loss: 2.305506964970304 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.301538943521579 --- Val Loss: 2.305403722661314 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: 2.301537585140498 --- Val Loss: 2.3056023931103287 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: 2.301536702818009 --- Val Loss: 2.3054550596727497 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: 2.3015300454707375 --- Val Loss: 2.3049256533644016 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: 2.301500815838859 --- Val Loss: 2.305293016883942 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 120/1000 --- Train Loss: 2.301421575468958 --- Val Loss: 2.3054974024198724 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.300949523726798 --- Val Loss: 2.30534957909564 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.280722795618599 --- Val Loss: 2.2849282117767706 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: 1.7324776785074736 --- Val Loss: 1.722695006580204 --- Train Acc: 0.22 --- Val Acc: 0.17\n",
      "Epoch 160/1000 --- Train Loss: 1.4034969542349074 --- Val Loss: 1.3528349579525638 --- Train Acc: 0.39 --- Val Acc: 0.45\n",
      "Epoch 170/1000 --- Train Loss: 0.9899853801118286 --- Val Loss: 0.9272678464693561 --- Train Acc: 0.63 --- Val Acc: 0.66\n",
      "Epoch 180/1000 --- Train Loss: 0.553095782550537 --- Val Loss: 0.5042225102117264 --- Train Acc: 0.84 --- Val Acc: 0.86\n",
      "Epoch 190/1000 --- Train Loss: 0.22172539375370087 --- Val Loss: 0.18661181479609912 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 200/1000 --- Train Loss: 0.09969235728766364 --- Val Loss: 0.08041118952129303 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 210/1000 --- Train Loss: 0.06783669950894512 --- Val Loss: 0.045012039149903364 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/1000 --- Train Loss: 0.03871483257211618 --- Val Loss: 0.02822491526566302 --- Train Acc: 1.00 --- Val Acc: 0.99\n",
      "Epoch 230/1000 --- Train Loss: 0.026162935593576145 --- Val Loss: 0.01792051409748029 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.0242229818939071 --- Val Loss: 0.01251708405900228 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.019439589168431477 --- Val Loss: 0.01117575869537181 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.01219480616676006 --- Val Loss: 0.006884608368049146 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.015328853038733428 --- Val Loss: 0.0054689126116652 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.014122756433136147 --- Val Loss: 0.004574034532210348 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.008425846235401544 --- Val Loss: 0.0037746497542826734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.0073959206236514545 --- Val Loss: 0.0030728385877774596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.007148098688519604 --- Val Loss: 0.0027991784951494074 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.005041958753974565 --- Val Loss: 0.002223121051152566 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.00427818696316838 --- Val Loss: 0.002362246874743891 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.005700404077166819 --- Val Loss: 0.0019793487116419733 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.004117177801307024 --- Val Loss: 0.001875360851975013 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.003404584846800598 --- Val Loss: 0.0015054740243684794 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.00750153412091846 --- Val Loss: 0.0013681528863190265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.004900327180073538 --- Val Loss: 0.0011217439874369693 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.002478677574025675 --- Val Loss: 0.0011604415584896607 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.005181552439081183 --- Val Loss: 0.001112844646963107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.0030060344231055067 --- Val Loss: 0.0009976813638938025 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.003257692725594857 --- Val Loss: 0.0009015471290758989 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.002989100781547583 --- Val Loss: 0.0008064031307479407 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.004705266474253968 --- Val Loss: 0.0007353573855329655 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.0037917525446124903 --- Val Loss: 0.0008154231319784656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.0018817097202069246 --- Val Loss: 0.0006896236236265254 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.003273891234296671 --- Val Loss: 0.0006231060851387359 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.002841324631376345 --- Val Loss: 0.0005826234265036463 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.005529645241276387 --- Val Loss: 0.0006025376748158585 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.004834511829244008 --- Val Loss: 0.0005841302590456079 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.0022551512676501147 --- Val Loss: 0.0005929074988992282 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.0014258305202364972 --- Val Loss: 0.0005568942918209986 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.0033415880469512297 --- Val Loss: 0.0005428013721999217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.0016716506092732447 --- Val Loss: 0.0005140428000073823 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.0022087375274295155 --- Val Loss: 0.0005806397741862596 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.0023205312700000213 --- Val Loss: 0.00044902562972978663 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.0013265668251402868 --- Val Loss: 0.0004182700302532922 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.0029719674853805884 --- Val Loss: 0.00043381965071879265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.0018615445910901175 --- Val Loss: 0.00038591409212441476 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.002673591095321845 --- Val Loss: 0.00038047702107641795 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.0015927166734335384 --- Val Loss: 0.0003390874075411512 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.001403086630539543 --- Val Loss: 0.0003395927081047969 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.0016298534285919969 --- Val Loss: 0.0003043140790131179 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.003188343838256794 --- Val Loss: 0.0003037961554159207 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.001178879428725212 --- Val Loss: 0.0002998157748673638 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.0013581980257668803 --- Val Loss: 0.0003163654897775714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.0032236103888950932 --- Val Loss: 0.0002784704854040845 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.0009284679583418629 --- Val Loss: 0.0003285367483168252 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.001204313131240686 --- Val Loss: 0.00028002351251090753 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.002579338606128365 --- Val Loss: 0.00026544339880157984 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.0011147655446382071 --- Val Loss: 0.00026953880353518803 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.0013907567637035344 --- Val Loss: 0.00025357650085694217 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.0017643192046140138 --- Val Loss: 0.0002833933806050634 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.0024717046315138494 --- Val Loss: 0.00023481440871528158 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.0009597840813464966 --- Val Loss: 0.00023771964402890644 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.0012800187834710287 --- Val Loss: 0.00022990928746920869 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.0006377793507080582 --- Val Loss: 0.00023173974704575266 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.0006142630087686835 --- Val Loss: 0.00019827361253252528 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.006025131591577447 --- Val Loss: 0.00018225709909021223 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.0019101733602403523 --- Val Loss: 0.00017738431519360998 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.0005648173150589995 --- Val Loss: 0.0002014665609429143 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.00595106788479372 --- Val Loss: 0.014795724022187469 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.0008271741996441107 --- Val Loss: 0.0002189994529216031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.0027625114994478286 --- Val Loss: 0.00020335966110294188 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.002559200368868788 --- Val Loss: 0.00019554454017426436 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.0009409726239936969 --- Val Loss: 0.00017363034437281849 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.0010633287691086287 --- Val Loss: 0.000162436666650023 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.000742021167402009 --- Val Loss: 0.00016039636398033078 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.0009433387329315694 --- Val Loss: 0.00014595128575259982 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.0005527412346128654 --- Val Loss: 0.00017904441613359568 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.0006894446656948526 --- Val Loss: 0.00016361388608799403 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.0006268102119439098 --- Val Loss: 0.00013366206697259205 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.0009014159312917142 --- Val Loss: 0.00012425043769076892 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.000971177890405463 --- Val Loss: 0.00015154412213958562 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.0008286063959864463 --- Val Loss: 0.00012741082209208763 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.0007453002922626374 --- Val Loss: 0.00011777075926920235 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.0006321569835548771 --- Val Loss: 0.00011387439406768384 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.0006641458102533699 --- Val Loss: 0.00010477575096066011 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.0004588805706631513 --- Val Loss: 0.00011414369091215687 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32)) \n",
    "network.add_layer(ReLU())\n",
    "#network.add_layer(Layer(64, 32, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=1000, learning_rate=0.01, optimizer='Momentum', momentum=0.9, batch_size=64)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (166,214) (166,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aless\\Desktop\\Uni\\Math and Programming for AI\\Programming_And_Mathematics_For_AI\\Main_V2.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m network\u001b[39m.\u001b[39madd_layer(Sigmoid())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Train the network\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m network\u001b[39m.\u001b[39;49mtrain(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, optimizer\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMomentum\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m network\u001b[39m.\u001b[39mplot_loss()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Evaluate the performance of the model\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\aless\\Desktop\\Uni\\Math and Programming for AI\\Programming_And_Mathematics_For_AI\\Main_V2.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     batch_y \u001b[39m=\u001b[39m y_train[start_idx:end_idx]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_pass(batch_x) \u001b[39m# forward pass to get the output predictions\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m     loss_gradient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_categorical_cross_entropy_gradient(batch_y, output)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_pass(loss_gradient, learning_rate, optimizer, momentum) \u001b[39m# backward pass to update the network's weights\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m \u001b[39m# Calculate training loss for the epoch\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\aless\\Desktop\\Uni\\Math and Programming for AI\\Programming_And_Mathematics_For_AI\\Main_V2.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mCalculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m- y_true: One-hot encoded label array.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Assuming y_true is one-hot encoded and y_pred is the output of softmax\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m y_pred_gradient \u001b[39m=\u001b[39m (y_pred \u001b[39m-\u001b[39;49m y_true) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y_pred)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aless/Desktop/Uni/Math%20and%20Programming%20for%20AI/Programming_And_Mathematics_For_AI/Main_V2.ipynb#X12sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_pred_gradient\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (166,214) (166,2) "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer, load_iris, load_wine\n",
    "\n",
    "#dataset = load_breast_cancer()\n",
    "dataset = load_wine()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=5000, learning_rate=0.1, optimizer='Momentum', batch_size=1024, validation_split=0.25)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 0.6922619966589287 --- Val Loss: 0.6922396163263533 --- Train Acc: 0.62 --- Val Acc: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000 --- Train Loss: 0.6776416862420167 --- Val Loss: 0.6766434703079923 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 20/1000 --- Train Loss: 0.6654140453792818 --- Val Loss: 0.6630997029951565 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 30/1000 --- Train Loss: 0.6618505537608961 --- Val Loss: 0.6586073804399666 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 40/1000 --- Train Loss: 0.661598694577384 --- Val Loss: 0.6579011946087174 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 50/1000 --- Train Loss: 0.6617006651952264 --- Val Loss: 0.6578733502779768 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 60/1000 --- Train Loss: 0.6616692422317545 --- Val Loss: 0.6578686599182977 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 70/1000 --- Train Loss: 0.6616072643214732 --- Val Loss: 0.6578792713770116 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 80/1000 --- Train Loss: 0.6615734808487239 --- Val Loss: 0.6579096463611654 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 90/1000 --- Train Loss: 0.6615628926962265 --- Val Loss: 0.657938126134997 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 100/1000 --- Train Loss: 0.6615604993598827 --- Val Loss: 0.657953192424454 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 110/1000 --- Train Loss: 0.6615602497922433 --- Val Loss: 0.6579567318512795 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 120/1000 --- Train Loss: 0.6615609656353209 --- Val Loss: 0.6579546614673633 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 130/1000 --- Train Loss: 0.6615602974323128 --- Val Loss: 0.6579513415236413 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 140/1000 --- Train Loss: 0.6615603979212497 --- Val Loss: 0.6579485974332747 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 150/1000 --- Train Loss: 0.6615619009976349 --- Val Loss: 0.6579468499137245 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 160/1000 --- Train Loss: 0.6615600179831354 --- Val Loss: 0.6579458714136832 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 170/1000 --- Train Loss: 0.6615591793853592 --- Val Loss: 0.6579453670411131 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 180/1000 --- Train Loss: 0.6615604012332369 --- Val Loss: 0.657945037379244 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 190/1000 --- Train Loss: 0.6615576153790991 --- Val Loss: 0.6579446402937501 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 200/1000 --- Train Loss: 0.6615587628600574 --- Val Loss: 0.6579440866632068 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 210/1000 --- Train Loss: 0.6615567790049932 --- Val Loss: 0.6579434003068132 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 220/1000 --- Train Loss: 0.6615560372624015 --- Val Loss: 0.6579425492836108 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 230/1000 --- Train Loss: 0.6615535453627244 --- Val Loss: 0.6579411980391153 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 240/1000 --- Train Loss: 0.6615506982057939 --- Val Loss: 0.6579395721427144 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 250/1000 --- Train Loss: 0.6615500869362798 --- Val Loss: 0.657937560961364 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 260/1000 --- Train Loss: 0.6615464463644293 --- Val Loss: 0.6579349116385409 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 270/1000 --- Train Loss: 0.6615449061373578 --- Val Loss: 0.6579316736229933 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 280/1000 --- Train Loss: 0.6615398736436087 --- Val Loss: 0.6579272757231871 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 290/1000 --- Train Loss: 0.6615328029074646 --- Val Loss: 0.6579212450092441 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 300/1000 --- Train Loss: 0.6615226956718373 --- Val Loss: 0.6579130465599904 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 310/1000 --- Train Loss: 0.6615095967798578 --- Val Loss: 0.6579021311072566 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 320/1000 --- Train Loss: 0.6614895623720021 --- Val Loss: 0.6578865206054774 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 330/1000 --- Train Loss: 0.6614661838293855 --- Val Loss: 0.6578634560154272 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 340/1000 --- Train Loss: 0.661430206057488 --- Val Loss: 0.6578286599283011 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 350/1000 --- Train Loss: 0.6613565102203566 --- Val Loss: 0.6577732672112897 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 360/1000 --- Train Loss: 0.6612666560014113 --- Val Loss: 0.6576826831571605 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 370/1000 --- Train Loss: 0.6611057899610759 --- Val Loss: 0.6575302859253549 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 380/1000 --- Train Loss: 0.6607792397243031 --- Val Loss: 0.657255203489552 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 390/1000 --- Train Loss: 0.660205294266291 --- Val Loss: 0.6567212367315816 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 400/1000 --- Train Loss: 0.6590432909508168 --- Val Loss: 0.655627360223175 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 410/1000 --- Train Loss: 0.6563806638028208 --- Val Loss: 0.6531416899031446 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 420/1000 --- Train Loss: 0.6498130310245411 --- Val Loss: 0.6466376009130942 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 430/1000 --- Train Loss: 0.6300748981185915 --- Val Loss: 0.6265755767074916 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 440/1000 --- Train Loss: 0.5562329463278101 --- Val Loss: 0.5543896843210462 --- Train Acc: 0.62 --- Val Acc: 0.63\n",
      "Epoch 450/1000 --- Train Loss: 0.3387085039308963 --- Val Loss: 0.3422082758606675 --- Train Acc: 0.93 --- Val Acc: 0.92\n",
      "Epoch 460/1000 --- Train Loss: 0.14517505950707776 --- Val Loss: 0.12578846452083703 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 470/1000 --- Train Loss: 0.09658425785501043 --- Val Loss: 0.07059038353207615 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 480/1000 --- Train Loss: 0.07070401158280057 --- Val Loss: 0.051518124205020624 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 490/1000 --- Train Loss: 0.07172468105621359 --- Val Loss: 0.04393701353047818 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 500/1000 --- Train Loss: 0.0640722688784371 --- Val Loss: 0.03988138650760891 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 510/1000 --- Train Loss: 0.06006127938386032 --- Val Loss: 0.036867238615599165 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 520/1000 --- Train Loss: 0.06482461429625799 --- Val Loss: 0.03418969994735595 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 530/1000 --- Train Loss: 0.06212189526126797 --- Val Loss: 0.03168308534094711 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 540/1000 --- Train Loss: 0.054954838147924866 --- Val Loss: 0.02920131091881682 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 550/1000 --- Train Loss: 0.057159377474404985 --- Val Loss: 0.02678992255415833 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 560/1000 --- Train Loss: 0.05503426594152257 --- Val Loss: 0.024636585484491037 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 570/1000 --- Train Loss: 0.05716905300691649 --- Val Loss: 0.022784396884967226 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 580/1000 --- Train Loss: 0.03807711467146595 --- Val Loss: 0.021109978068462106 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 590/1000 --- Train Loss: 0.04865942225515498 --- Val Loss: 0.019668330507460308 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 600/1000 --- Train Loss: 0.045915823118563015 --- Val Loss: 0.018489860144909945 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.04966604318428225 --- Val Loss: 0.0176031179394852 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.04305133361823942 --- Val Loss: 0.016526097703397767 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.040647949922270034 --- Val Loss: 0.015517489900863627 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.03735630518839812 --- Val Loss: 0.014421537435217536 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.03663951301516084 --- Val Loss: 0.013686755537560777 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.04433051757353027 --- Val Loss: 0.013038377970544931 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.03281969514290285 --- Val Loss: 0.012237539980711549 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.04454937301554189 --- Val Loss: 0.01149612325022974 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.03250179271174717 --- Val Loss: 0.011000095577318288 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.041227237430558505 --- Val Loss: 0.01060574067122555 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.0356414961614495 --- Val Loss: 0.010191281427587849 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.035233756729246356 --- Val Loss: 0.009633952218283587 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.03375276672519478 --- Val Loss: 0.008992135604463438 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.03213135068605425 --- Val Loss: 0.008533555135356625 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.03491800189044099 --- Val Loss: 0.008266291517760762 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.03207387803999988 --- Val Loss: 0.008074191961827078 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.035634227752666246 --- Val Loss: 0.007715090739387799 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.03651945960678183 --- Val Loss: 0.007546979354722776 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.04148627882605407 --- Val Loss: 0.00731383100973496 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.045141349658938595 --- Val Loss: 0.007020108874015193 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.0392186863379184 --- Val Loss: 0.006722584833919036 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.04193264589194548 --- Val Loss: 0.006540001636309599 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.044919132321107655 --- Val Loss: 0.0064481147247787375 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.03580547159764834 --- Val Loss: 0.006206975346814137 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.04073872173043068 --- Val Loss: 0.005855648370544053 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.03906380244008059 --- Val Loss: 0.005693214701850399 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.041014075079139914 --- Val Loss: 0.0056755509565240714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.028072324085365842 --- Val Loss: 0.005557050150735805 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.041363292823412404 --- Val Loss: 0.005275447078551795 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.03962532409687292 --- Val Loss: 0.005154623898213301 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.028447542074448946 --- Val Loss: 0.005041325940775647 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.04288338219772949 --- Val Loss: 0.0049182104563851555 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.03557791601888013 --- Val Loss: 0.00474617720188754 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.030493887307950274 --- Val Loss: 0.004637215714603734 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.025367235841132936 --- Val Loss: 0.0044821120393918655 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.03398916953925918 --- Val Loss: 0.0044410114513562995 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.04533275883831239 --- Val Loss: 0.00434145956425943 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.03583497537056784 --- Val Loss: 0.004223807717702591 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.02487164849549706 --- Val Loss: 0.004039109010843616 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 32))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(32, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=10000, learning_rate=0.01, optimizer='Momentum', batch_size=256, validation_split=0.25)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49815a38",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "* optimization of hyperparameters (random search and grid search function?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702fa55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21dae1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    def __init__(self, network, param_grid, n_iter=10):\n",
    "        self.network = NeuralNetwork\n",
    "        self.param_grid = param_grid\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def sample_params(self):\n",
    "        sampled_params = {}\n",
    "        for param, values in self.param_grid.items():\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        return sampled_params\n",
    "\n",
    "    def evaluate(self, X_train, y_train, X_val, y_val, params):\n",
    "        network = self.network()\n",
    "\n",
    "        config = params['layer_configs']\n",
    "        # Add the first Dense layer\n",
    "        network.add_layer(Layer(64, config['layer1_nodes'], l1=config['layer1_l1'], l2=config['layer1_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        network.add_layer(Dropout(0.25))\n",
    "\n",
    "        print(config['layer1_nodes'])\n",
    "        # Add the second Dense layer\n",
    "        network.add_layer(Layer(config['layer1_nodes'], config['layer2_nodes'], l1=config['layer2_l1'], l2=config['layer2_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        # Add the output Softmax layer\n",
    "        network.add_layer(Layer(config['layer2_nodes'], 10))\n",
    "        network.add_layer(Softmax())\n",
    "        \n",
    "        network.train(X_train, y_train, epochs=params['epochs'], learning_rate=params['learning_rate'],\n",
    "                      optimizer=params['optimizer'], momentum=params['momentum'], batch_size=params['batch_size'])\n",
    "        \n",
    "        y_pred = network.predict(X_val)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def search(self, X, y):\n",
    "        best_params = None\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            params = self.sample_params()\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            accuracy = self.evaluate(X_train, y_train, X_val, y_val, params)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "\n",
    "            print(f\"Params: {params}, Accuracy: {accuracy}\")\n",
    "\n",
    "        return best_params, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20145077",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'layer_configs': [\n",
    "        {\n",
    "            'layer1_nodes': layer1_nodes,\n",
    "            'layer1_l1': layer1_l1,\n",
    "            'layer1_l2': layer1_l2,\n",
    "            'layer2_nodes': layer2_nodes,\n",
    "            'layer2_l1': layer2_l1,\n",
    "            'layer2_l2': layer2_l2\n",
    "        }\n",
    "        for layer1_nodes in [32, 64, 128]  # Possible node counts for the first Dense layer\n",
    "        for layer1_l1 in [0.0, 0.01]       # L1 regularization for the first Dense layer\n",
    "        for layer1_l2 in [0.0, 0.01]       # L2 regularization for the first Dense layer\n",
    "        for layer2_nodes in [16, 32, 64]   # Possible node counts for the second Dense layer\n",
    "        for layer2_l1 in [0.0, 0.01]       # L1 regularization for the second Dense layer\n",
    "        for layer2_l2 in [0.0, 0.01]       # L2 regularization for the second Dense layer\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'epochs': [100, 500, 1000],\n",
    "    'optimizer': ['GD', 'Momentum'],\n",
    "    'momentum': [0.5, 0.9],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd34073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "Epoch 0/100 --- Train Loss: 0.5999855043392593 --- Val Loss: 0.5184779946092196 --- Train Acc: 0.86 --- Val Acc: 0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 --- Train Loss: 1.9736622173200513 --- Val Loss: 2.012554687860109 --- Train Acc: 0.24 --- Val Acc: 0.22\n",
      "Epoch 20/100 --- Train Loss: 2.3141052452932476 --- Val Loss: 2.298881316877556 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3282237266348615 --- Val Loss: 2.2995982299973954 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.298121232053494 --- Val Loss: 2.2992258521342626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.292297161109781 --- Val Loss: 2.298238102510764 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3004727544799395 --- Val Loss: 2.299817073251843 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.3003352799871983 --- Val Loss: 2.2998766647194833 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.300441803492836 --- Val Loss: 2.3011424278194994 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3004233171236357 --- Val Loss: 2.299077081469976 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.302011439297 --- Val Loss: 2.301463499549136 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/100 --- Train Loss: 0.5533321718646385 --- Val Loss: 0.12474962415653879 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 20/100 --- Train Loss: 1.2847330655422775 --- Val Loss: 0.3337924367947385 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 30/100 --- Train Loss: 1.104460411902192 --- Val Loss: 0.7148682143465206 --- Train Acc: 0.81 --- Val Acc: 0.81\n",
      "Epoch 40/100 --- Train Loss: 1.2544141055452498 --- Val Loss: 1.1906429752114784 --- Train Acc: 0.57 --- Val Acc: 0.53\n",
      "Epoch 50/100 --- Train Loss: 1.4329557856159247 --- Val Loss: 1.3555572379482936 --- Train Acc: 0.49 --- Val Acc: 0.48\n",
      "Epoch 60/100 --- Train Loss: 1.6705781734101837 --- Val Loss: 1.6787415160638648 --- Train Acc: 0.38 --- Val Acc: 0.37\n",
      "Epoch 70/100 --- Train Loss: 1.904660286675092 --- Val Loss: 1.9395355515722228 --- Train Acc: 0.25 --- Val Acc: 0.27\n",
      "Epoch 80/100 --- Train Loss: 2.042517618982838 --- Val Loss: 2.084516381192997 --- Train Acc: 0.20 --- Val Acc: 0.20\n",
      "Epoch 90/100 --- Train Loss: 2.0002692730447706 --- Val Loss: 2.0409641084053183 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.15555555555555556\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3023536416955546 --- Val Loss: 2.3021031078838643 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.3010480771203943 --- Val Loss: 2.2988832015841267 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.2962047993790025 --- Val Loss: 2.2926800135154677 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: 1.906779527904955 --- Val Loss: 1.8477939110032422 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 40/1000 --- Train Loss: 0.8690203796503909 --- Val Loss: 0.7640393863106343 --- Train Acc: 0.81 --- Val Acc: 0.78\n",
      "Epoch 50/1000 --- Train Loss: 0.5804793707466696 --- Val Loss: 0.404953989767038 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 60/1000 --- Train Loss: 0.4676990972591807 --- Val Loss: 0.27910152873454264 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 70/1000 --- Train Loss: 0.4047974339644763 --- Val Loss: 0.20938465203519094 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 80/1000 --- Train Loss: 0.43265457298336246 --- Val Loss: 0.18925654561706096 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 90/1000 --- Train Loss: 0.4112709103291219 --- Val Loss: 0.14644963284246734 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 100/1000 --- Train Loss: 0.3736640090911676 --- Val Loss: 0.12871976663552107 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 110/1000 --- Train Loss: 0.41731556316051893 --- Val Loss: 0.13335678653630445 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 120/1000 --- Train Loss: 0.3700259444295674 --- Val Loss: 0.10267966669729102 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 130/1000 --- Train Loss: 0.30402003789853577 --- Val Loss: 0.11411608108398984 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 140/1000 --- Train Loss: 0.3074919217964152 --- Val Loss: 0.07896390299625522 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 150/1000 --- Train Loss: 0.3062936171332492 --- Val Loss: 0.06122248847638451 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 160/1000 --- Train Loss: 0.28849106582764866 --- Val Loss: 0.0421664031235748 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 170/1000 --- Train Loss: 0.35130463757807806 --- Val Loss: 0.06483612222228208 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 180/1000 --- Train Loss: 0.24612433818288754 --- Val Loss: 0.04039685763134246 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 190/1000 --- Train Loss: 0.36573350512471325 --- Val Loss: 0.05679325565324692 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 200/1000 --- Train Loss: 0.39528474658775886 --- Val Loss: 0.09257984374566992 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 210/1000 --- Train Loss: 0.3817251066432929 --- Val Loss: 0.069144551572881 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 220/1000 --- Train Loss: 0.3657215390554863 --- Val Loss: 0.04049989657503687 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 230/1000 --- Train Loss: 0.3942020671226752 --- Val Loss: 0.0749687470388529 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 240/1000 --- Train Loss: 0.4955173879715996 --- Val Loss: 0.07628850421042482 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 250/1000 --- Train Loss: 0.42323900479936427 --- Val Loss: 0.09831136528728791 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 260/1000 --- Train Loss: 0.43750396845702455 --- Val Loss: 0.08749922006524734 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 270/1000 --- Train Loss: 0.43739503895455506 --- Val Loss: 0.11965295617503051 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 280/1000 --- Train Loss: 0.49594128995367237 --- Val Loss: 0.1493883003347014 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 290/1000 --- Train Loss: 0.5016012880798135 --- Val Loss: 0.1230567714475073 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 300/1000 --- Train Loss: 0.6512328946511692 --- Val Loss: 0.2329874140115493 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 310/1000 --- Train Loss: 0.5639004940166822 --- Val Loss: 0.21682628337663448 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 320/1000 --- Train Loss: 0.5061423808399905 --- Val Loss: 0.2170878860874055 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 330/1000 --- Train Loss: 0.5854231367331595 --- Val Loss: 0.21540238594991865 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 340/1000 --- Train Loss: 0.694266232715664 --- Val Loss: 0.3004553110234194 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 350/1000 --- Train Loss: 0.6741147293675224 --- Val Loss: 0.3798765291373849 --- Train Acc: 0.90 --- Val Acc: 0.88\n",
      "Epoch 360/1000 --- Train Loss: 0.7486388615209506 --- Val Loss: 0.3721572862420508 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 370/1000 --- Train Loss: 0.8700336648749093 --- Val Loss: 0.4862872224209113 --- Train Acc: 0.85 --- Val Acc: 0.84\n",
      "Epoch 380/1000 --- Train Loss: 0.7844041523234694 --- Val Loss: 0.44855290838146855 --- Train Acc: 0.85 --- Val Acc: 0.86\n",
      "Epoch 390/1000 --- Train Loss: 0.840254205042708 --- Val Loss: 0.48979599677549723 --- Train Acc: 0.86 --- Val Acc: 0.85\n",
      "Epoch 400/1000 --- Train Loss: 0.9717231761297369 --- Val Loss: 0.5447470688310894 --- Train Acc: 0.84 --- Val Acc: 0.84\n",
      "Epoch 410/1000 --- Train Loss: 1.1241927115637182 --- Val Loss: 0.7308472460517437 --- Train Acc: 0.78 --- Val Acc: 0.76\n",
      "Epoch 420/1000 --- Train Loss: 1.1109110837083507 --- Val Loss: 0.6573709391272455 --- Train Acc: 0.79 --- Val Acc: 0.79\n",
      "Epoch 430/1000 --- Train Loss: 1.1092782276437365 --- Val Loss: 0.8456695031041861 --- Train Acc: 0.70 --- Val Acc: 0.70\n",
      "Epoch 440/1000 --- Train Loss: 1.1056604920597561 --- Val Loss: 1.0107929917858105 --- Train Acc: 0.68 --- Val Acc: 0.63\n",
      "Epoch 450/1000 --- Train Loss: 1.3088755309579407 --- Val Loss: 1.1720137439663736 --- Train Acc: 0.60 --- Val Acc: 0.57\n",
      "Epoch 460/1000 --- Train Loss: 1.3630937529537166 --- Val Loss: 1.0551885053871655 --- Train Acc: 0.58 --- Val Acc: 0.59\n",
      "Epoch 470/1000 --- Train Loss: 1.493226265555102 --- Val Loss: 1.4037680781204804 --- Train Acc: 0.51 --- Val Acc: 0.48\n",
      "Epoch 480/1000 --- Train Loss: 1.6196060348324015 --- Val Loss: 1.2742844255957622 --- Train Acc: 0.52 --- Val Acc: 0.51\n",
      "Epoch 490/1000 --- Train Loss: 1.6753967634783449 --- Val Loss: 1.5987840592921796 --- Train Acc: 0.41 --- Val Acc: 0.39\n",
      "Epoch 500/1000 --- Train Loss: 1.8161143099728883 --- Val Loss: 1.8168899673659316 --- Train Acc: 0.30 --- Val Acc: 0.30\n",
      "Epoch 510/1000 --- Train Loss: 2.0024067549120517 --- Val Loss: 2.094660613463582 --- Train Acc: 0.20 --- Val Acc: 0.18\n",
      "Epoch 520/1000 --- Train Loss: 1.891485981174709 --- Val Loss: 1.959807966129462 --- Train Acc: 0.27 --- Val Acc: 0.24\n",
      "Epoch 530/1000 --- Train Loss: 1.854391358853092 --- Val Loss: 1.8697344929766795 --- Train Acc: 0.29 --- Val Acc: 0.27\n",
      "Epoch 540/1000 --- Train Loss: 2.1644445651580724 --- Val Loss: 2.16495329722956 --- Train Acc: 0.17 --- Val Acc: 0.16\n",
      "Epoch 550/1000 --- Train Loss: 2.1986014789582855 --- Val Loss: 2.2202033395273437 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 560/1000 --- Train Loss: 2.2195838934975645 --- Val Loss: 2.240315243430494 --- Train Acc: 0.14 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: 2.14171296131725 --- Val Loss: 2.2551909469175824 --- Train Acc: 0.14 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.1650520284079784 --- Val Loss: 2.1963389917096343 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 590/1000 --- Train Loss: 2.2012972607699672 --- Val Loss: 2.2352501618508094 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.2080880234018783 --- Val Loss: 2.270271910972144 --- Train Acc: 0.13 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.211144503314433 --- Val Loss: 2.2327242665953135 --- Train Acc: 0.14 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.2845683109252453 --- Val Loss: 2.3045481577977616 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.2476129754668923 --- Val Loss: 2.292103643612468 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 640/1000 --- Train Loss: 2.2563995883577186 --- Val Loss: 2.2911755112735004 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 650/1000 --- Train Loss: 2.2957304949967337 --- Val Loss: 2.298260662929979 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: 2.2876082206255486 --- Val Loss: 2.29774389686969 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: 2.2827575892155836 --- Val Loss: 2.2974479512916512 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 680/1000 --- Train Loss: 2.288531629670692 --- Val Loss: 2.297387638131499 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: 2.300654765713967 --- Val Loss: 2.297150121038427 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.294781860980208 --- Val Loss: 2.2970163523178835 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.2906591400200877 --- Val Loss: 2.2970039359061447 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.2968077569680423 --- Val Loss: 2.296956765883378 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: 2.2987714637936585 --- Val Loss: 2.2969239512574706 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 740/1000 --- Train Loss: 2.322852890106748 --- Val Loss: 2.2968327710839898 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: 2.294831654923094 --- Val Loss: 2.296942805174748 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: 2.294778292567759 --- Val Loss: 2.2968447921161506 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: 2.2965853429432337 --- Val Loss: 2.2968933224199453 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 780/1000 --- Train Loss: 2.2986046771328392 --- Val Loss: 2.2968712655730257 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 790/1000 --- Train Loss: 2.2947793989444447 --- Val Loss: 2.2968245428334817 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.2967972244055783 --- Val Loss: 2.2967994878439466 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.296797890300276 --- Val Loss: 2.2968609087023117 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: 2.3007373513491953 --- Val Loss: 2.2968364539001414 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: 2.2987667101817375 --- Val Loss: 2.296824489298127 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: 2.300736059338258 --- Val Loss: 2.2967979713433593 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: 2.2948793415556508 --- Val Loss: 2.296879593231434 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.294851917996861 --- Val Loss: 2.2969588921169835 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: 2.3007430107939775 --- Val Loss: 2.2969405576287363 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: 2.306849411019448 --- Val Loss: 2.2969761916184446 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: 2.298802233007638 --- Val Loss: 2.2969776843302507 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: 2.30074172173645 --- Val Loss: 2.296945761490134 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: 2.2987667467993127 --- Val Loss: 2.2968799292871895 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.300736720295268 --- Val Loss: 2.2968589901091754 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.2987676316427996 --- Val Loss: 2.296811170317952 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: 2.3007355156020304 --- Val Loss: 2.2968570491321123 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 950/1000 --- Train Loss: 2.300735246247712 --- Val Loss: 2.2968375799696723 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.3007362935551083 --- Val Loss: 2.2968905448342545 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.296795692456576 --- Val Loss: 2.2968549942192253 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.300736067504765 --- Val Loss: 2.2968825477983543 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: 2.300736042840048 --- Val Loss: 2.296835811278673 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3079016500216394 --- Val Loss: 2.3179280098472628 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.314706901143255 --- Val Loss: 2.30971631985025 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 20/1000 --- Train Loss: 2.3190682773562368 --- Val Loss: 2.3270914563915204 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 2.306794816289979 --- Val Loss: 2.3217365690308673 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 40/1000 --- Train Loss: 2.3051949700519825 --- Val Loss: 2.3044400874058235 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: 2.311083936943365 --- Val Loss: 2.320281143050674 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: 2.310083000328314 --- Val Loss: 2.3281317116287887 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: 2.311286900269716 --- Val Loss: 2.300009301491421 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.3119802678089454 --- Val Loss: 2.3315156818427063 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 90/1000 --- Train Loss: 2.3085519811798347 --- Val Loss: 2.3142906813440325 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 100/1000 --- Train Loss: 2.3077470832622957 --- Val Loss: 2.3105846943191337 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.314289228275405 --- Val Loss: 2.3010916679312525 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.307485037618217 --- Val Loss: 2.312057274360391 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.307816377670478 --- Val Loss: 2.301867928949315 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.3028812422783482 --- Val Loss: 2.3035557518047063 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.311806280752387 --- Val Loss: 2.3163197623731127 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3120115995199506 --- Val Loss: 2.3030083665727057 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 170/1000 --- Train Loss: 2.3066803909374434 --- Val Loss: 2.30698601380861 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: 2.307382487349041 --- Val Loss: 2.319135699928359 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.320248040068271 --- Val Loss: 2.3322740500468373 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: 2.3064106789500265 --- Val Loss: 2.311708915681398 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.321231541049538 --- Val Loss: 2.331280740513725 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: 2.3076362430324724 --- Val Loss: 2.316861780374755 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 230/1000 --- Train Loss: 2.3125172070520024 --- Val Loss: 2.3069413370924354 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.310137016224422 --- Val Loss: 2.3196620485431354 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 250/1000 --- Train Loss: 2.3146483037665244 --- Val Loss: 2.3218746003023587 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: 2.310402029515372 --- Val Loss: 2.2998827769616725 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.3170869327254175 --- Val Loss: 2.321161311980031 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 280/1000 --- Train Loss: 2.3150583372866405 --- Val Loss: 2.3206020838988795 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.3109760159412107 --- Val Loss: 2.3116105695503686 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.314861485009709 --- Val Loss: 2.314809833696721 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: 2.30514548378756 --- Val Loss: 2.3069596696953663 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: 2.3092546369745146 --- Val Loss: 2.322076875372099 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.309039407721538 --- Val Loss: 2.3228252250560124 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: 2.3163540245506633 --- Val Loss: 2.3292008540618934 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: 2.3274066089849104 --- Val Loss: 2.361710538557727 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: 2.3112942741821523 --- Val Loss: 2.315116293850105 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: 2.312801694251867 --- Val Loss: 2.3209951926514987 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3084528090756753 --- Val Loss: 2.300131791048612 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.309384206796736 --- Val Loss: 2.3167022954489016 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.3126248286672166 --- Val Loss: 2.3197655010770046 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 410/1000 --- Train Loss: 2.3147938913267323 --- Val Loss: 2.3159805785273564 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.3105418999736327 --- Val Loss: 2.306687834653238 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 430/1000 --- Train Loss: 2.309892563952587 --- Val Loss: 2.29501636077944 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3060181856605944 --- Val Loss: 2.315894369426802 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.320633366273046 --- Val Loss: 2.3142189082467524 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: 2.3077003291761353 --- Val Loss: 2.3116506602947196 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.313201961594777 --- Val Loss: 2.311227768672081 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.308311713034914 --- Val Loss: 2.314464988950519 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.3113639379109974 --- Val Loss: 2.311405552232979 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 500/1000 --- Train Loss: 2.316853222805209 --- Val Loss: 2.315811676633905 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: 2.305704753983222 --- Val Loss: 2.3020612146751733 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.302884235549997 --- Val Loss: 2.305913199950074 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.323927989801645 --- Val Loss: 2.3035471031708954 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.3183905793632156 --- Val Loss: 2.3195693589379696 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: 2.3232541609776014 --- Val Loss: 2.319533322825822 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: 2.313475085658839 --- Val Loss: 2.3172437388457885 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.303926383880064 --- Val Loss: 2.307856055789743 --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 580/1000 --- Train Loss: 2.3066471639782162 --- Val Loss: 2.3146454206932727 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 590/1000 --- Train Loss: 2.3095307496853783 --- Val Loss: 2.3034475467399873 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: 2.310024251939036 --- Val Loss: 2.305694014402898 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.309040178403386 --- Val Loss: 2.3106541259085853 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.3131906103057003 --- Val Loss: 2.3040994272399358 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: 2.3058761965713828 --- Val Loss: 2.3055994179865267 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 640/1000 --- Train Loss: 2.309175249698966 --- Val Loss: 2.3046501165021698 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.3073945121382415 --- Val Loss: 2.318194578070759 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: 2.3134874262601994 --- Val Loss: 2.30460181472953 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 670/1000 --- Train Loss: 2.317020275047714 --- Val Loss: 2.330591846310178 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: 2.3150715420408954 --- Val Loss: 2.301817614773947 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 690/1000 --- Train Loss: 2.3077492251588017 --- Val Loss: 2.3009641214959315 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: 2.3109953914281576 --- Val Loss: 2.3132649557531684 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.3112434592456133 --- Val Loss: 2.3101359481906334 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 720/1000 --- Train Loss: 2.311157754721371 --- Val Loss: 2.3112589142985143 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.320564602563599 --- Val Loss: 2.3179536312211884 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: 2.3148509994547775 --- Val Loss: 2.3202447536767723 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: 2.313486267998186 --- Val Loss: 2.326804208994547 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.315343528872116 --- Val Loss: 2.3147724674086874 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: 2.307222596888231 --- Val Loss: 2.310154327440256 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.309902933868237 --- Val Loss: 2.3154747372041347 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: 2.317210430177777 --- Val Loss: 2.3155896396854807 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: 2.3084365133364817 --- Val Loss: 2.300257920206204 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.3101096306367306 --- Val Loss: 2.31224050176899 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3133962723545727 --- Val Loss: 2.3091048667478145 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.3161300504565507 --- Val Loss: 2.3307291395274943 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 840/1000 --- Train Loss: 2.3112598311358328 --- Val Loss: 2.307557979440662 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.3085038171502315 --- Val Loss: 2.300274025655105 --- Train Acc: 0.10 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.315208880608467 --- Val Loss: 2.3168244436678993 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.315341954337449 --- Val Loss: 2.3181053683335664 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 880/1000 --- Train Loss: 2.3140043457701225 --- Val Loss: 2.3112647148487326 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.318113782554869 --- Val Loss: 2.321478999478225 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: 2.3086930286429146 --- Val Loss: 2.308835269561117 --- Train Acc: 0.10 --- Val Acc: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_420\\3559256763.py:50: RuntimeWarning: overflow encountered in add\n",
      "  self.weights += self.velocity_weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3024344876416922 --- Val Loss: 2.3021912933696695 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/100 --- Train Loss: 2.301681666811276 --- Val Loss: 2.299747642532327 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.301503065841283 --- Val Loss: 2.298699752287707 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3014632419332925 --- Val Loss: 2.298285811514586 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 40/100 --- Train Loss: 2.3014529990065884 --- Val Loss: 2.2980495560357594 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.301450036768018 --- Val Loss: 2.297936859392635 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.301449478439546 --- Val Loss: 2.297891434175459 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3014492178851906 --- Val Loss: 2.297874548024523 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.301449111903596 --- Val Loss: 2.2978452897230675 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.301449060396747 --- Val Loss: 2.297852239994448 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.30252938798153 --- Val Loss: 2.302594856313988 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3021236681026425 --- Val Loss: 2.3027294257359445 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.3017445501357052 --- Val Loss: 2.3027276805770454 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.298657125898102 --- Val Loss: 2.299742400201479 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.1773059572918583 --- Val Loss: 2.173858331368432 --- Train Acc: 0.45 --- Val Acc: 0.46\n",
      "Epoch 50/500 --- Train Loss: 1.1544251555628493 --- Val Loss: 1.0756584713617825 --- Train Acc: 0.69 --- Val Acc: 0.72\n",
      "Epoch 60/500 --- Train Loss: 0.6066128745448404 --- Val Loss: 0.5058514659024163 --- Train Acc: 0.85 --- Val Acc: 0.88\n",
      "Epoch 70/500 --- Train Loss: 0.3473520627372894 --- Val Loss: 0.24220784715380186 --- Train Acc: 0.92 --- Val Acc: 0.94\n",
      "Epoch 80/500 --- Train Loss: 0.23273151749392873 --- Val Loss: 0.12917514589786305 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 90/500 --- Train Loss: 0.17055177503116337 --- Val Loss: 0.0728079571222616 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 100/500 --- Train Loss: 0.1439644583452351 --- Val Loss: 0.04580009069683281 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 110/500 --- Train Loss: 0.13234624833632522 --- Val Loss: 0.03431761908906942 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 120/500 --- Train Loss: 0.10162717610757434 --- Val Loss: 0.016506001707777163 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.1308142843830718 --- Val Loss: 0.014326456969895183 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.09149902100346127 --- Val Loss: 0.01114809135548031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.0928328852480408 --- Val Loss: 0.008465869413484244 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.07673135216525566 --- Val Loss: 0.004241925294150241 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.11488737921913673 --- Val Loss: 0.004339940411328046 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/500 --- Train Loss: 0.12572736401898604 --- Val Loss: 0.004395749172997894 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.08487609098392794 --- Val Loss: 0.0018669085473684295 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/500 --- Train Loss: 0.08550078578640498 --- Val Loss: 0.0011630467465396168 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.11214327886556467 --- Val Loss: 0.002441182502368521 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/500 --- Train Loss: 0.11607601626467637 --- Val Loss: 0.001405985013071934 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.12678921112833444 --- Val Loss: 0.0014094536069758905 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.1176917713315986 --- Val Loss: 0.0005222075167877477 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/500 --- Train Loss: 0.10632967942546218 --- Val Loss: 0.0017897031599918027 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/500 --- Train Loss: 0.08825923059355603 --- Val Loss: 0.0008521165025935108 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/500 --- Train Loss: 0.3031337677963663 --- Val Loss: 0.0964157160730606 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 280/500 --- Train Loss: 0.11895076861160724 --- Val Loss: 0.00013576059459881462 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/500 --- Train Loss: 0.12787221309999244 --- Val Loss: 0.0019136231939271618 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/500 --- Train Loss: 0.08573067099849244 --- Val Loss: 0.0004760352027019031 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/500 --- Train Loss: 0.11016445296790733 --- Val Loss: 0.002662311619808393 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/500 --- Train Loss: 0.1044791921569125 --- Val Loss: 0.0007811995365826746 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/500 --- Train Loss: 0.2155786662377698 --- Val Loss: 0.0037252742673046036 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/500 --- Train Loss: 0.1401652108719765 --- Val Loss: 0.0059992527949094395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/500 --- Train Loss: 0.11468329203222434 --- Val Loss: 0.0003465379718781125 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/500 --- Train Loss: 0.12842745380967283 --- Val Loss: 6.0477680204401365e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/500 --- Train Loss: 0.06770394223701703 --- Val Loss: 1.0689218199930784e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/500 --- Train Loss: 0.1507566332045444 --- Val Loss: 3.100668394314715e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/500 --- Train Loss: 0.17047735383330428 --- Val Loss: 2.599463798879381e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/500 --- Train Loss: 0.2299921863092159 --- Val Loss: 1.0208237070604912e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/500 --- Train Loss: 0.17981615816835161 --- Val Loss: 3.569483659336487e-06 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/500 --- Train Loss: 0.13163716447399784 --- Val Loss: 3.011401966519195e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/500 --- Train Loss: 0.0610077876328023 --- Val Loss: 1.5345635276368304e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/500 --- Train Loss: 0.05753368897355579 --- Val Loss: 7.504603310418118e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/500 --- Train Loss: 0.20955462980006956 --- Val Loss: 2.401593057904207e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/500 --- Train Loss: 0.2721928039482032 --- Val Loss: 0.00019973937228098243 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/500 --- Train Loss: 0.20712930935203297 --- Val Loss: 0.009191896858880931 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/500 --- Train Loss: 0.19675035194047383 --- Val Loss: 0.0008674641414779185 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/500 --- Train Loss: 0.28342253269460177 --- Val Loss: 1.245371941771986e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.95\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3018828725257885 --- Val Loss: 2.302453495189011 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 0.31543521299481325 --- Val Loss: 0.23000306820187355 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 20/500 --- Train Loss: 0.28162063805117815 --- Val Loss: 0.07903203244733437 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 30/500 --- Train Loss: 0.44933823889845054 --- Val Loss: 0.07707138241310396 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 40/500 --- Train Loss: 0.8177367465809571 --- Val Loss: 0.1428833314649073 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 50/500 --- Train Loss: 0.5885931320724652 --- Val Loss: 0.07104632097746696 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 60/500 --- Train Loss: 0.5966001695390973 --- Val Loss: 0.05616178994667823 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/500 --- Train Loss: 0.3684624237206901 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/500 --- Train Loss: 0.33311951479899554 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/500 --- Train Loss: 0.32778386054284797 --- Val Loss: 0.022559042687607615 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/500 --- Train Loss: 0.35637186590344594 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/500 --- Train Loss: 0.26482514756049974 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/500 --- Train Loss: 0.19175724108985084 --- Val Loss: 0.056160711675817904 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.275598290899644 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/500 --- Train Loss: 0.13361550414937468 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/500 --- Train Loss: 0.16818892314043954 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.25228333471065684 --- Val Loss: 0.056160711675817904 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 0.3784249520659828 --- Val Loss: 0.11232132335163082 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 180/500 --- Train Loss: 0.26629906997235975 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/500 --- Train Loss: 0.714802598346852 --- Val Loss: 0.39312438173069564 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 200/500 --- Train Loss: 0.2720844829118859 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/500 --- Train Loss: 0.43448789311279434 --- Val Loss: 0.16848193502744374 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 220/500 --- Train Loss: 0.3363777462808742 --- Val Loss: 0.05616071167581789 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 230/500 --- Train Loss: 0.3223620110191712 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/500 --- Train Loss: 0.2564643831944011 --- Val Loss: 0.00837676508067734 --- Train Acc: 0.99 --- Val Acc: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_420\\118437895.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.5167547562343593 --- Val Loss: 2.2408856609171965 --- Train Acc: 0.50 --- Val Acc: 0.47\n",
      "Epoch 10/1000 --- Train Loss: 2.3221897760613817 --- Val Loss: 2.3245191099430147 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.300557251880854 --- Val Loss: 2.2977351475338077 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.7691132313867866 --- Val Loss: 1.8083260961901564 --- Train Acc: 0.48 --- Val Acc: 0.46\n",
      "Epoch 20/1000 --- Train Loss: 14.492270360600786 --- Val Loss: 14.433277300683933 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.302405248945103 --- Val Loss: 2.302358059663533 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3015468924304647 --- Val Loss: 2.301193756669537 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.3013487105860464 --- Val Loss: 2.3008368441740488 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.301303064176355 --- Val Loss: 2.300722489192698 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3012911560577556 --- Val Loss: 2.300663771351669 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/100 --- Train Loss: 2.301288100515383 --- Val Loss: 2.300641122002933 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3012870281671534 --- Val Loss: 2.300615017168025 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.30128648042876 --- Val Loss: 2.3006084458477147 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/100 --- Train Loss: 2.3012858299749577 --- Val Loss: 2.300598917551925 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.3012850540375682 --- Val Loss: 2.3006091391741443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 3.3724016434240367 --- Val Loss: 2.5750967610273827 --- Train Acc: 0.63 --- Val Acc: 0.60\n",
      "Epoch 10/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.301262078540388 --- Val Loss: 2.2986256661832574 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 10/1000 --- Train Loss: 0.35163245720691816 --- Val Loss: 0.19420962200064637 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 20/1000 --- Train Loss: 0.4325578724716512 --- Val Loss: 0.2169743361483166 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 30/1000 --- Train Loss: 0.5835001035710169 --- Val Loss: 0.46008017483499747 --- Train Acc: 0.88 --- Val Acc: 0.84\n",
      "Epoch 40/1000 --- Train Loss: 0.8809123291397629 --- Val Loss: 0.7896499885433507 --- Train Acc: 0.72 --- Val Acc: 0.72\n",
      "Epoch 50/1000 --- Train Loss: 1.6772141988718272 --- Val Loss: 1.2559712375145187 --- Train Acc: 0.58 --- Val Acc: 0.61\n",
      "Epoch 60/1000 --- Train Loss: 1.7508617305693113 --- Val Loss: 1.5362985731538765 --- Train Acc: 0.41 --- Val Acc: 0.46\n",
      "Epoch 70/1000 --- Train Loss: 1.8728721701633193 --- Val Loss: 1.8374234733434358 --- Train Acc: 0.30 --- Val Acc: 0.34\n",
      "Epoch 80/1000 --- Train Loss: 2.209296842718724 --- Val Loss: 2.275676327049414 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 90/1000 --- Train Loss: 2.1248908433257516 --- Val Loss: 2.1131756004258455 --- Train Acc: 0.17 --- Val Acc: 0.21\n",
      "Epoch 100/1000 --- Train Loss: 2.260954349389411 --- Val Loss: 2.2570404361663567 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.310953230228709 --- Val Loss: 2.2983826205095776 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3108898253311554 --- Val Loss: 2.2977196495854475 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.298863197055025 --- Val Loss: 2.298108660559207 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.30117346797277 --- Val Loss: 2.2973178362089897 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3009862581187317 --- Val Loss: 2.296916322339282 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3009661459763144 --- Val Loss: 2.296536064768543 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 170/1000 --- Train Loss: 2.3009313367109314 --- Val Loss: 2.297693465681572 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3009177839421167 --- Val Loss: 2.297075980153106 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.30271135900108 --- Val Loss: 2.2975002319304316 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.3009379422821454 --- Val Loss: 2.298195727531903 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.300953481574311 --- Val Loss: 2.296471828724529 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.300941379852613 --- Val Loss: 2.2982898843824024 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.313016692209793 --- Val Loss: 2.297530477171048 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.300913310038245 --- Val Loss: 2.297716594286915 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.300951895197969 --- Val Loss: 2.2979222711730722 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.300932899702762 --- Val Loss: 2.2974288389978583 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 270/1000 --- Train Loss: 2.3009472919870193 --- Val Loss: 2.2973065008732942 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.300934701424807 --- Val Loss: 2.297737647663254 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.300921099699788 --- Val Loss: 2.297279472428036 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3009134739965016 --- Val Loss: 2.2970270827362076 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3009180183484466 --- Val Loss: 2.29793464549308 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.298900547890098 --- Val Loss: 2.297638299184465 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 330/1000 --- Train Loss: 2.300931485435679 --- Val Loss: 2.2985761414849737 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.301001276607487 --- Val Loss: 2.2966960623671144 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3009889399633474 --- Val Loss: 2.2973973918602297 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 360/1000 --- Train Loss: 2.3009638101052574 --- Val Loss: 2.2983962599425243 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3130384959726484 --- Val Loss: 2.2980052639549995 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.301033209329296 --- Val Loss: 2.2976581805170726 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3009653364201403 --- Val Loss: 2.2975629839724028 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3009625752547467 --- Val Loss: 2.2976487796290592 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.30091340708154 --- Val Loss: 2.2968861670761607 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3009955395073587 --- Val Loss: 2.297883383421162 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.3009582136561795 --- Val Loss: 2.2973442701289772 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3009105867090094 --- Val Loss: 2.2967475499307417 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 450/1000 --- Train Loss: 2.3009590693579995 --- Val Loss: 2.2976345422151274 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3009062315008526 --- Val Loss: 2.2979601995066243 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3009328832399527 --- Val Loss: 2.2974429183890677 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.300921683198684 --- Val Loss: 2.2978363039815353 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 490/1000 --- Train Loss: 2.3010117086573305 --- Val Loss: 2.2983590317818683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.3009550698248002 --- Val Loss: 2.297715134795542 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.3009396215723728 --- Val Loss: 2.2979803109992445 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 520/1000 --- Train Loss: 2.3009487215634703 --- Val Loss: 2.2981470429881647 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 530/1000 --- Train Loss: 2.300921443734486 --- Val Loss: 2.2981546400923567 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3009398747107803 --- Val Loss: 2.297511920777055 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.300956271188749 --- Val Loss: 2.296481596375842 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 560/1000 --- Train Loss: 2.3009513113891256 --- Val Loss: 2.298113029648751 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.300904177232254 --- Val Loss: 2.29781123400925 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.300902059874445 --- Val Loss: 2.298079800439229 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.300932868455381 --- Val Loss: 2.2967400705992422 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.300939789445803 --- Val Loss: 2.297337460572678 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.3009561225721282 --- Val Loss: 2.2973370695557587 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.300991081940134 --- Val Loss: 2.297490023147301 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.3009914938987523 --- Val Loss: 2.29867801092359 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.300949316492304 --- Val Loss: 2.29688509708272 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 650/1000 --- Train Loss: 2.30100399645512 --- Val Loss: 2.296943007789764 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 660/1000 --- Train Loss: 2.3009086853377574 --- Val Loss: 2.297839501320914 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3009100373886167 --- Val Loss: 2.2971382628982298 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.300907820892586 --- Val Loss: 2.297166462050309 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.300899810443929 --- Val Loss: 2.2972549526858437 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.300991386500565 --- Val Loss: 2.2963389209056473 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 710/1000 --- Train Loss: 2.3009047032757315 --- Val Loss: 2.297858662381733 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3009558057308874 --- Val Loss: 2.29751044697387 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3009346036136575 --- Val Loss: 2.2976438803205395 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.301070626327392 --- Val Loss: 2.2969228525688696 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.300974154914746 --- Val Loss: 2.2982892393898693 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.300937899093978 --- Val Loss: 2.297535511966879 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 770/1000 --- Train Loss: 2.3009208671162797 --- Val Loss: 2.2976209833831174 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.3008924588434456 --- Val Loss: 2.2974050065307434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.3009845047114137 --- Val Loss: 2.2976338274856527 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 800/1000 --- Train Loss: 2.300913109051609 --- Val Loss: 2.2967684289324923 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 810/1000 --- Train Loss: 2.3009413384718203 --- Val Loss: 2.297982807150421 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3009321714927586 --- Val Loss: 2.2969612028718207 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 830/1000 --- Train Loss: 2.3009599189392365 --- Val Loss: 2.2973090687863906 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3009221095814842 --- Val Loss: 2.2975232067555065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.300921912555487 --- Val Loss: 2.297548135797392 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.3010335000671134 --- Val Loss: 2.2979937151085563 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.300998152429528 --- Val Loss: 2.297327319144117 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 880/1000 --- Train Loss: 2.3009056801992616 --- Val Loss: 2.297376579380831 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 890/1000 --- Train Loss: 2.2989050737803343 --- Val Loss: 2.2974422921438107 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.3009742537978566 --- Val Loss: 2.297191717415453 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.300970573351162 --- Val Loss: 2.2978484724150663 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.3009466161318777 --- Val Loss: 2.2971486728937784 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.3009233541843708 --- Val Loss: 2.298214125416691 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.300942632056455 --- Val Loss: 2.2979722249899086 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3009509788505387 --- Val Loss: 2.2973055298253744 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.298875915737164 --- Val Loss: 2.2975817139702808 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 970/1000 --- Train Loss: 2.300910328523414 --- Val Loss: 2.298097911418154 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3009906097617963 --- Val Loss: 2.2975532270388785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3009018260350036 --- Val Loss: 2.2976874774278104 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302526906686371 --- Val Loss: 2.3025518511520136 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.301978903883389 --- Val Loss: 2.3022282233728135 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.3005023693324116 --- Val Loss: 2.3008479333254743 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.292958570218565 --- Val Loss: 2.293076767326175 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/500 --- Train Loss: 2.262117153819159 --- Val Loss: 2.260466617678956 --- Train Acc: 0.29 --- Val Acc: 0.31\n",
      "Epoch 50/500 --- Train Loss: 2.1502582621758592 --- Val Loss: 2.140462195338187 --- Train Acc: 0.35 --- Val Acc: 0.37\n",
      "Epoch 60/500 --- Train Loss: 1.8209358479213853 --- Val Loss: 1.7885714629071912 --- Train Acc: 0.51 --- Val Acc: 0.52\n",
      "Epoch 70/500 --- Train Loss: 1.2938711990349245 --- Val Loss: 1.2300380559675053 --- Train Acc: 0.73 --- Val Acc: 0.75\n",
      "Epoch 80/500 --- Train Loss: 0.9034576957134695 --- Val Loss: 0.8046382558148106 --- Train Acc: 0.84 --- Val Acc: 0.83\n",
      "Epoch 90/500 --- Train Loss: 0.7040731762239106 --- Val Loss: 0.5399072920363669 --- Train Acc: 0.89 --- Val Acc: 0.88\n",
      "Epoch 100/500 --- Train Loss: 0.5453943118642831 --- Val Loss: 0.3967108665383717 --- Train Acc: 0.91 --- Val Acc: 0.89\n",
      "Epoch 110/500 --- Train Loss: 0.47513153182279916 --- Val Loss: 0.3096307803958356 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 120/500 --- Train Loss: 0.411120348847773 --- Val Loss: 0.24898563333693793 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 130/500 --- Train Loss: 0.3968231236038502 --- Val Loss: 0.20909081668040358 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 140/500 --- Train Loss: 0.3701736846324492 --- Val Loss: 0.1795514831331852 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 150/500 --- Train Loss: 0.34573206664123635 --- Val Loss: 0.15603305173086546 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 160/500 --- Train Loss: 0.3270611667824682 --- Val Loss: 0.13453704242374692 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 170/500 --- Train Loss: 0.37479466879322304 --- Val Loss: 0.11552994302434554 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 180/500 --- Train Loss: 0.29482190018467264 --- Val Loss: 0.1109481978979421 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 190/500 --- Train Loss: 0.3411837428244217 --- Val Loss: 0.09073170937302594 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 200/500 --- Train Loss: 0.3059127890915407 --- Val Loss: 0.08259467227808961 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 210/500 --- Train Loss: 0.29745649144902203 --- Val Loss: 0.08257845559546735 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 220/500 --- Train Loss: 0.3086468419770901 --- Val Loss: 0.06799913298910015 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 230/500 --- Train Loss: 0.3638914363551733 --- Val Loss: 0.06491525059290187 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 240/500 --- Train Loss: 0.32247294966835643 --- Val Loss: 0.060641768395267207 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 250/500 --- Train Loss: 0.2931212448982924 --- Val Loss: 0.0622591507207879 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 260/500 --- Train Loss: 0.32861872218506416 --- Val Loss: 0.0466298764583375 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 270/500 --- Train Loss: 0.29133016809007173 --- Val Loss: 0.06557990165203816 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 280/500 --- Train Loss: 0.31592690490701897 --- Val Loss: 0.04601842174704226 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 290/500 --- Train Loss: 0.27059008943807017 --- Val Loss: 0.04472801654161403 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 300/500 --- Train Loss: 0.3789351517794367 --- Val Loss: 0.05084986977995003 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 310/500 --- Train Loss: 0.3067741677898673 --- Val Loss: 0.04596261993723374 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 320/500 --- Train Loss: 0.3535239132039332 --- Val Loss: 0.04907175081147382 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 330/500 --- Train Loss: 0.43183218220240266 --- Val Loss: 0.06538912033780865 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 340/500 --- Train Loss: 0.37400060027846416 --- Val Loss: 0.052933434049042584 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 350/500 --- Train Loss: 0.44063899618813135 --- Val Loss: 0.09542802368877923 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 360/500 --- Train Loss: 0.4693429460439701 --- Val Loss: 0.07880397967864303 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 370/500 --- Train Loss: 0.5984084401981835 --- Val Loss: 0.09322062143181147 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 380/500 --- Train Loss: 1.0023625071638684 --- Val Loss: 0.45244174189504 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 390/500 --- Train Loss: 0.5947106336452417 --- Val Loss: 0.12944401784556794 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 400/500 --- Train Loss: 0.6084098670863674 --- Val Loss: 0.13208686027151406 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 410/500 --- Train Loss: 0.971088184392748 --- Val Loss: 0.28961087031289434 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 420/500 --- Train Loss: 1.0832871693697743 --- Val Loss: 0.2926877673094404 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 430/500 --- Train Loss: 1.0954031232706316 --- Val Loss: 0.3214142497594526 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 440/500 --- Train Loss: 2.0459604402174616 --- Val Loss: 0.9764198135126778 --- Train Acc: 0.89 --- Val Acc: 0.89\n",
      "Epoch 450/500 --- Train Loss: 0.5929830319873766 --- Val Loss: 0.06414099707384904 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 460/500 --- Train Loss: 1.4016152301638394 --- Val Loss: 0.6877549287236069 --- Train Acc: 0.89 --- Val Acc: 0.91\n",
      "Epoch 470/500 --- Train Loss: 0.8270613769349037 --- Val Loss: 0.049663098301868186 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 480/500 --- Train Loss: 1.0581077542671689 --- Val Loss: 0.13908235922702636 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 490/500 --- Train Loss: 0.9529473721256403 --- Val Loss: 0.1906742550040357 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.9277777777777778\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.558653084698243 --- Val Loss: 2.310765893605998 --- Train Acc: 0.09 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 20/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 110/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.302485594177745 --- Val Loss: 2.3023787487389127 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.301772013537767 --- Val Loss: 2.301134406827238 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/500 --- Train Loss: 2.301428079321185 --- Val Loss: 2.300384420843406 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.301253367091234 --- Val Loss: 2.299925931122994 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.3011708637914707 --- Val Loss: 2.299667926051119 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/500 --- Train Loss: 2.301128388128477 --- Val Loss: 2.29946834995315 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.3011079636871203 --- Val Loss: 2.2993621657584047 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3010959216514872 --- Val Loss: 2.2992328055026796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.301090931125964 --- Val Loss: 2.299173872514016 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3010884773653753 --- Val Loss: 2.299166622353942 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3010867810750106 --- Val Loss: 2.2991147871341475 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.301085572977698 --- Val Loss: 2.2990588727692716 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.3010854658653837 --- Val Loss: 2.2990516848129965 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.301085340439316 --- Val Loss: 2.299105461244191 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3010852579487446 --- Val Loss: 2.299031677753801 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.301085017751158 --- Val Loss: 2.2990267996683653 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/500 --- Train Loss: 2.3010848260748555 --- Val Loss: 2.299030696860871 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/500 --- Train Loss: 2.3010847736344835 --- Val Loss: 2.2989977921514773 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.30108464310949 --- Val Loss: 2.299015256097193 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/500 --- Train Loss: 2.3010846625903563 --- Val Loss: 2.299017492699495 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.3010845278967853 --- Val Loss: 2.2990007558295265 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3010844293013766 --- Val Loss: 2.298979437470668 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.3010843654431357 --- Val Loss: 2.299039800297368 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.3010841809746845 --- Val Loss: 2.299030125787642 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.3010841937544475 --- Val Loss: 2.299045627711285 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.3010839980656015 --- Val Loss: 2.2990117703889386 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.3010838743823743 --- Val Loss: 2.2990250065760174 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.3010837604395538 --- Val Loss: 2.2990183775234083 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.3010835460383627 --- Val Loss: 2.2990436942185064 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.301083410812158 --- Val Loss: 2.2990208652217796 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.3010832303514994 --- Val Loss: 2.299017557507811 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.301083211912685 --- Val Loss: 2.299044441969539 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/500 --- Train Loss: 2.3010828738257625 --- Val Loss: 2.2990132578493445 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3010827007086077 --- Val Loss: 2.2990415524964383 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.3010824388204214 --- Val Loss: 2.2989995439600763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: 2.3010821525603267 --- Val Loss: 2.299025951145306 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.3010819986972932 --- Val Loss: 2.299059772047861 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.3010816916279193 --- Val Loss: 2.2990110093788783 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.3010813462926007 --- Val Loss: 2.299027548756217 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.301080943848594 --- Val Loss: 2.2989728643336416 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: 2.3010805421358183 --- Val Loss: 2.299018586630666 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.3010800120443817 --- Val Loss: 2.2990760701958988 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.30107955774741 --- Val Loss: 2.29898209999726 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.3010791667730004 --- Val Loss: 2.2989811417218986 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.3010782147323514 --- Val Loss: 2.29903600567404 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.301077707768909 --- Val Loss: 2.299003950560114 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: 2.3010767704450132 --- Val Loss: 2.2990178868941973 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: 2.301075951851791 --- Val Loss: 2.299016640974325 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.3010750315806567 --- Val Loss: 2.298985802655977 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.301073547885436 --- Val Loss: 2.2990151926426257 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09444444444444444\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 1.3535577567515735 --- Val Loss: 1.301917617356367 --- Train Acc: 0.60 --- Val Acc: 0.58\n",
      "Epoch 10/100 --- Train Loss: 2.304318855403745 --- Val Loss: 2.309308228675639 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.305433313540744 --- Val Loss: 2.299091188351344 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.3093627402212973 --- Val Loss: 2.302842031527007 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3042988327190095 --- Val Loss: 2.2991436204943523 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.3144866592005964 --- Val Loss: 2.2989312526400902 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/100 --- Train Loss: 2.3063074054852897 --- Val Loss: 2.30097981940824 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/100 --- Train Loss: 2.3064758150757356 --- Val Loss: 2.307785161355592 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.311387590287514 --- Val Loss: 2.307940319724163 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.304562824473739 --- Val Loss: 2.2997408060765467 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.307846925449628 --- Val Loss: 2.308599831928564 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 10/500 --- Train Loss: 2.3126656761415 --- Val Loss: 2.3221430644717165 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 20/500 --- Train Loss: 2.313151005301703 --- Val Loss: 2.3083619049467043 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/500 --- Train Loss: 2.308254763057268 --- Val Loss: 2.3174080561801564 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/500 --- Train Loss: 2.31241211426111 --- Val Loss: 2.316958916796222 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 50/500 --- Train Loss: 2.3289104056364276 --- Val Loss: 2.3203620406231775 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 60/500 --- Train Loss: 2.317430211006766 --- Val Loss: 2.3127005434905334 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: 2.310040242335666 --- Val Loss: 2.3123906049066396 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.3073099760690416 --- Val Loss: 2.3134867511664794 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: 2.3131793309944904 --- Val Loss: 2.306522414016731 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3166206389829758 --- Val Loss: 2.3155955746235186 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: 2.30908152517267 --- Val Loss: 2.303608928644491 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: 2.3158504345944952 --- Val Loss: 2.3294986956692187 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.315925036328178 --- Val Loss: 2.316524274938793 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3198320730633504 --- Val Loss: 2.3162359718648866 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.306204051923323 --- Val Loss: 2.3111831690503366 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: 2.3089353181102674 --- Val Loss: 2.3095253636822237 --- Train Acc: 0.09 --- Val Acc: 0.08\n",
      "Epoch 170/500 --- Train Loss: 2.3133940161340623 --- Val Loss: 2.3180640363507536 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 180/500 --- Train Loss: 2.3074924735818074 --- Val Loss: 2.3017032781902373 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.3216588805208507 --- Val Loss: 2.3377635213766483 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: 2.310666774644831 --- Val Loss: 2.3169954301290403 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3121604030878493 --- Val Loss: 2.3220435190051933 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 220/500 --- Train Loss: 2.3109703871766656 --- Val Loss: 2.3055948947968847 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.3122190849023454 --- Val Loss: 2.322608243416174 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 240/500 --- Train Loss: 2.306777385780402 --- Val Loss: 2.3100430734736053 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 250/500 --- Train Loss: 2.3089377879193838 --- Val Loss: 2.3138847948697934 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 260/500 --- Train Loss: 2.314154533785585 --- Val Loss: 2.330154332010174 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 270/500 --- Train Loss: 2.3153279352306755 --- Val Loss: 2.323882600820275 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: 2.3231942045154 --- Val Loss: 2.3149734775503745 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.317712042648707 --- Val Loss: 2.3138630413 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: 2.3190966666890342 --- Val Loss: 2.326373023299379 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: 2.324451507961057 --- Val Loss: 2.3241770144604925 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: 2.3068739402153198 --- Val Loss: 2.301670725300154 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3221470151552976 --- Val Loss: 2.3379262932158915 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.311428732682658 --- Val Loss: 2.3110818144210152 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.312714770458515 --- Val Loss: 2.3089133430245448 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: 2.325192210430295 --- Val Loss: 2.330976223087449 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.310439723048147 --- Val Loss: 2.314804148693406 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: 2.311893322771993 --- Val Loss: 2.320214017273359 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: 2.314306632576996 --- Val Loss: 2.3087207644434735 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: 2.3143769749880456 --- Val Loss: 2.3197393447174335 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 410/500 --- Train Loss: 2.30374434483464 --- Val Loss: 2.304710625208447 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 420/500 --- Train Loss: 2.311467042129436 --- Val Loss: 2.3078209160608485 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.3121142750555252 --- Val Loss: 2.311287170783942 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.3188195765545068 --- Val Loss: 2.314553311258272 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: 2.31085636231234 --- Val Loss: 2.3150528514298236 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: 2.3085192509054906 --- Val Loss: 2.3198467586255327 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 470/500 --- Train Loss: 2.3104168139858596 --- Val Loss: 2.311662758203702 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.3145970143427963 --- Val Loss: 2.3127963100597624 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.3124267208447242 --- Val Loss: 2.310022714887891 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.12777777777777777\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 3.0140924006157856 --- Val Loss: 2.8394976249533816 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.3567093077452452 --- Val Loss: 2.3341307212884357 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.322757668223267 --- Val Loss: 2.30767427984912 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/100 --- Train Loss: 2.3252859730108404 --- Val Loss: 2.3216491271011472 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/100 --- Train Loss: 2.3388693701635694 --- Val Loss: 2.3626355610444216 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 50/100 --- Train Loss: 2.3296595679078056 --- Val Loss: 2.316369633683837 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/100 --- Train Loss: 2.3173218062452023 --- Val Loss: 2.316543617701126 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/100 --- Train Loss: 2.312288684398315 --- Val Loss: 2.320670486429323 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/100 --- Train Loss: 2.331641428519821 --- Val Loss: 2.3030858295841914 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 90/100 --- Train Loss: 2.3259300623553694 --- Val Loss: 2.3174896177097524 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 1.923990047286862 --- Val Loss: 1.9438492289925833 --- Train Acc: 0.31 --- Val Acc: 0.29\n",
      "Epoch 10/1000 --- Train Loss: 2.2830156788163727 --- Val Loss: 2.2666531064955393 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 20/1000 --- Train Loss: 2.311157989489715 --- Val Loss: 2.3023651918265156 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.316287278312946 --- Val Loss: 2.3035885300389083 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 40/1000 --- Train Loss: 2.2975301067188023 --- Val Loss: 2.302056166335434 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.2981298814926596 --- Val Loss: 2.3000405852384223 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.3014473173987358 --- Val Loss: 2.3021916389133206 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 70/1000 --- Train Loss: 2.302790717063921 --- Val Loss: 2.306063378661122 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: 2.299360035566055 --- Val Loss: 2.303031186872245 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.302006745330923 --- Val Loss: 2.310629732482613 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.2965801841799247 --- Val Loss: 2.3097662939340378 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3026338152338885 --- Val Loss: 2.3028404698493103 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3024128957758005 --- Val Loss: 2.301068260573312 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.3021400052056062 --- Val Loss: 2.3048152440395793 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.3020844241141547 --- Val Loss: 2.3027661509715998 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.3032470157471607 --- Val Loss: 2.3020925810907347 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.3028620067931844 --- Val Loss: 2.3069330235063212 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/1000 --- Train Loss: 2.3025571849790962 --- Val Loss: 2.3019602985235856 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/1000 --- Train Loss: 2.3021693976798603 --- Val Loss: 2.300369727609632 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/1000 --- Train Loss: 2.304041157170586 --- Val Loss: 2.29822485050719 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.3025235390635275 --- Val Loss: 2.303085429136741 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.3022902345206284 --- Val Loss: 2.3051939763815565 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/1000 --- Train Loss: 2.3031029314157787 --- Val Loss: 2.305087271061883 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.3034093968648883 --- Val Loss: 2.3104164656785344 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: 2.3035514027225807 --- Val Loss: 2.304099413856719 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: 2.3023420842444278 --- Val Loss: 2.30543243482179 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.3016780562843424 --- Val Loss: 2.302080030313019 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.30329411577976 --- Val Loss: 2.301914284494217 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.3032812253717676 --- Val Loss: 2.3082390052510044 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.302035765643277 --- Val Loss: 2.305392282867552 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/1000 --- Train Loss: 2.302565563866403 --- Val Loss: 2.3052953244147325 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.3033776845499307 --- Val Loss: 2.3078510116193978 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/1000 --- Train Loss: 2.301840112535388 --- Val Loss: 2.3078054075471757 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: 2.3034830104067807 --- Val Loss: 2.3049093530468223 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: 2.3017996351434853 --- Val Loss: 2.3013530976222443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3015545084986773 --- Val Loss: 2.302491571290176 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.302200534862563 --- Val Loss: 2.3059577647513785 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.302873315008996 --- Val Loss: 2.305941608536129 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/1000 --- Train Loss: 2.3021748641616204 --- Val Loss: 2.3032447231729236 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.3037532588574337 --- Val Loss: 2.301509395963957 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/1000 --- Train Loss: 2.3028134616378657 --- Val Loss: 2.3059125274521923 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.3016091961978242 --- Val Loss: 2.305823387542258 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: 2.303863118244021 --- Val Loss: 2.3016469217363245 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/1000 --- Train Loss: 2.30343022728772 --- Val Loss: 2.3089271635678825 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.3029181622284898 --- Val Loss: 2.3021979496215246 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.3024332294343934 --- Val Loss: 2.308438266273924 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: 2.301582961670353 --- Val Loss: 2.3048734829279955 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/1000 --- Train Loss: 2.3026062533029528 --- Val Loss: 2.302130065263441 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.3042288812865674 --- Val Loss: 2.301478895602671 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.302719969138863 --- Val Loss: 2.3061703536000038 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: 2.3023230675537936 --- Val Loss: 2.3026210380650403 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.302894854905443 --- Val Loss: 2.302537792522683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.304004859884797 --- Val Loss: 2.2978743612374526 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.302908549576294 --- Val Loss: 2.3009298491711405 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.302572939100537 --- Val Loss: 2.305036632593471 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.303301109231989 --- Val Loss: 2.310211515507988 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: 2.3016514108653103 --- Val Loss: 2.3058611073315443 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3022021733472284 --- Val Loss: 2.3059106484733527 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.302244716249101 --- Val Loss: 2.3045705278960185 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.3016842386235505 --- Val Loss: 2.298906905098527 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.3027676099519794 --- Val Loss: 2.3052608730092987 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 610/1000 --- Train Loss: 2.3033384609348593 --- Val Loss: 2.304245565418042 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: 2.3019131618377755 --- Val Loss: 2.304130340849055 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.302002943373641 --- Val Loss: 2.3004146974685016 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3017479358064743 --- Val Loss: 2.3040827506287243 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 650/1000 --- Train Loss: 2.301920827313197 --- Val Loss: 2.3044412544389394 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3022994316924663 --- Val Loss: 2.304267594327226 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3025097756823496 --- Val Loss: 2.302508427637741 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 680/1000 --- Train Loss: 2.3029399026351998 --- Val Loss: 2.3054886250227273 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: 2.304505276500685 --- Val Loss: 2.309549640340105 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3014646042936846 --- Val Loss: 2.302676170061878 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 710/1000 --- Train Loss: 2.3023063590824617 --- Val Loss: 2.3054578662864147 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 720/1000 --- Train Loss: 2.301538475349474 --- Val Loss: 2.3061169045838463 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.3034880001538447 --- Val Loss: 2.3051053548385982 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.301794503760716 --- Val Loss: 2.3052428175170525 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 750/1000 --- Train Loss: 2.3030247503040933 --- Val Loss: 2.30363271764035 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.3014862892113928 --- Val Loss: 2.3008587602492634 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 770/1000 --- Train Loss: 2.301820197766742 --- Val Loss: 2.3029168595751033 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 780/1000 --- Train Loss: 2.303166821941545 --- Val Loss: 2.30186540920065 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.30156721118536 --- Val Loss: 2.3018694609455106 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.302201244046224 --- Val Loss: 2.306210847466265 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 810/1000 --- Train Loss: 2.302385267809131 --- Val Loss: 2.304543547559336 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3022634791566445 --- Val Loss: 2.3074394900944957 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: 2.3034639297078745 --- Val Loss: 2.3026083293954343 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 840/1000 --- Train Loss: 2.3014457464962126 --- Val Loss: 2.3021957005929625 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 850/1000 --- Train Loss: 2.304075244511229 --- Val Loss: 2.3037781562153477 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3018495846570284 --- Val Loss: 2.3015329571721646 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.303628385974151 --- Val Loss: 2.307304411333447 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.3022634303514864 --- Val Loss: 2.3038593535814447 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.304759465672323 --- Val Loss: 2.303192529134952 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 900/1000 --- Train Loss: 2.3023578152869826 --- Val Loss: 2.3070367142491905 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.302921204388645 --- Val Loss: 2.302106862617549 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 920/1000 --- Train Loss: 2.3023658654081536 --- Val Loss: 2.307491869865628 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.315016844290872 --- Val Loss: 2.307261915744826 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3031424266514953 --- Val Loss: 2.3049798029811006 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.30458476250352 --- Val Loss: 2.3071274540196027 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.302561271493817 --- Val Loss: 2.303498686556102 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: 2.3017618141102565 --- Val Loss: 2.3018399248862838 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 980/1000 --- Train Loss: 2.3026738632475823 --- Val Loss: 2.305429970508966 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3021619846078427 --- Val Loss: 2.301234475159673 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09722222222222222\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.3018414658780753 --- Val Loss: 2.3008386390957294 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.3016328337403533 --- Val Loss: 2.2993607858768317 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 20/500 --- Train Loss: 1.918390677696297 --- Val Loss: 1.9250259287756177 --- Train Acc: 0.21 --- Val Acc: 0.18\n",
      "Epoch 30/500 --- Train Loss: 0.09408487146971208 --- Val Loss: 0.05197493121398385 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 40/500 --- Train Loss: 0.11753486772548076 --- Val Loss: 0.01631708100285634 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 50/500 --- Train Loss: 3.3187678110366052 --- Val Loss: 3.0550951332580207 --- Train Acc: 0.71 --- Val Acc: 0.73\n",
      "Epoch 60/500 --- Train Loss: 0.8593462182689392 --- Val Loss: 0.5579982625014274 --- Train Acc: 0.77 --- Val Acc: 0.78\n",
      "Epoch 70/500 --- Train Loss: 2.023470032140475 --- Val Loss: 1.799898487397491 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 80/500 --- Train Loss: 2.3137532588441574 --- Val Loss: 2.2995739829503408 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3016931680349897 --- Val Loss: 2.2970542047231133 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 100/500 --- Train Loss: 2.301622126236371 --- Val Loss: 2.2997232803467482 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.301662223962331 --- Val Loss: 2.298225869684668 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.30168322955918 --- Val Loss: 2.2986551918412608 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 130/500 --- Train Loss: 2.3017084429070747 --- Val Loss: 2.2993452323033043 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.3016234073018116 --- Val Loss: 2.297455760932887 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "\n",
      "Best Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 32}, Best Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomSearch(NeuralNetwork, param_grid, n_iter=20)\n",
    "best_params, best_accuracy = random_search.search(X, y)\n",
    "print(f\"\\nBest Params: {best_params}, Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc417dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
