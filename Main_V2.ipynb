{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, l1=0.0, l2=0.0):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # Initialise (weights) velocity terms for momentum optimization\n",
    "        self.velocity_biases = np.zeros_like(self.biases) # Initialise (biases) velocity terms for momentum optimization\n",
    "        self.l1 = l1 # L1 regularization coefficient (default 0.0).\n",
    "        self.l2 = l2 # L2 regularization coefficient (default 0.0).\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        '''\n",
    "        # Regularization terms\n",
    "        l1_reg = self.l1 * np.sign(self.weights)\n",
    "        l2_reg = self.l2 * self.weights\n",
    "\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient) + l1_reg + l2_reg\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        if optimizer == 'GD':\n",
    "            # Update weights and biases\n",
    "            self.weights += learning_rate * weights_gradient\n",
    "            self.biases += learning_rate * biases_gradient\n",
    "        elif optimizer == 'Momentum':\n",
    "            # Momentum update for weights and biases\n",
    "            self.velocity_weights = momentum * self.velocity_weights + learning_rate * weights_gradient\n",
    "            self.velocity_biases = momentum * self.velocity_biases + learning_rate * biases_gradient\n",
    "\n",
    "            # Update weights and biases using velocity\n",
    "            self.weights += self.velocity_weights\n",
    "            self.biases += self.velocity_biases\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for printing/plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for printing/plotting\n",
    "        self.accuracy_history = [] #placeholder to store the (train) accuracy for printing/plotting\n",
    "        self.val_accuracy_history = [] #placeholder to store the accuracy calculated on the validation set for printing/plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def compute_accuracy(self, predictions, labels):\n",
    "        '''\n",
    "        Computes the accuracy of predictions by comparing them with the true labels. \n",
    "        Accuracy is computed as the proportion of correct predictions to the total number of predictions.\n",
    "        '''\n",
    "        return np.mean(predictions == labels)\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate, optimizer='GD', momentum=0.9):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate, optimizer, momentum)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, optimizer='GD', momentum=0.9, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - optimizer: Specifies the optimization technique to use. Can be 'GD' for standard Gradient Descent or 'Momentum' for Gradient Descent with Momentum.\n",
    "\n",
    "        - momentum: A hyperparameter representing the momentum coefficient, typically between 0 (no momentum) and 1.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate, optimizer, momentum) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            train_predictions = self.predict(X_train)\n",
    "            train_accuracy = self.compute_accuracy(train_predictions, np.argmax(y_train, axis=1))\n",
    "            self.accuracy_history.append(train_accuracy)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            val_predictions = self.predict(X_val)\n",
    "            val_accuracy = self.compute_accuracy(val_predictions, np.argmax(y_val, axis=1))\n",
    "            self.val_accuracy_history.append(val_accuracy)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss} --- Train Acc: {train_accuracy:.2f} --- Val Acc: {val_accuracy:.2f}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.plot(self.accuracy_history, label='Train Accuracy')\n",
    "        plt.plot(self.val_accuracy_history, label='Val Accuracy')\n",
    "        plt.title(\"Accuracy over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000 --- Train Loss: 2.3007758390826316 --- Val Loss: 2.299180273812341 --- Train Acc: 0.13 --- Val Acc: 0.17\n",
      "Epoch 10/1000 --- Train Loss: 0.26381584425734317 --- Val Loss: 0.1178368630563335 --- Train Acc: 0.97 --- Val Acc: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000 --- Train Loss: 0.14137023106003666 --- Val Loss: 0.04513091293617588 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 30/1000 --- Train Loss: 0.10256882047660491 --- Val Loss: 0.026237429259199075 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 40/1000 --- Train Loss: 0.08726034179530309 --- Val Loss: 0.015825617591109765 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/1000 --- Train Loss: 0.07883093485844742 --- Val Loss: 0.01134010398866157 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 0.07729471698610718 --- Val Loss: 0.008410381036550866 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/1000 --- Train Loss: 0.05518450221680176 --- Val Loss: 0.007436675914350399 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/1000 --- Train Loss: 0.038774707232665274 --- Val Loss: 0.005904527398150457 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/1000 --- Train Loss: 0.04222009728153323 --- Val Loss: 0.004645370400430829 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/1000 --- Train Loss: 0.04211953600494133 --- Val Loss: 0.004023491511823872 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/1000 --- Train Loss: 0.03786658168255727 --- Val Loss: 0.0034034026580955002 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/1000 --- Train Loss: 0.03245975206436677 --- Val Loss: 0.0031862786638988095 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/1000 --- Train Loss: 0.03330620344738519 --- Val Loss: 0.002930760737004089 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/1000 --- Train Loss: 0.03073559013936681 --- Val Loss: 0.0030348189003000727 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/1000 --- Train Loss: 0.02844632665872231 --- Val Loss: 0.0029486513441028812 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/1000 --- Train Loss: 0.02907305954607735 --- Val Loss: 0.00208659015679875 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/1000 --- Train Loss: 0.03499305586141545 --- Val Loss: 0.0021796564668093705 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/1000 --- Train Loss: 0.017919345323845738 --- Val Loss: 0.0023507660730158826 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/1000 --- Train Loss: 0.025297883336391023 --- Val Loss: 0.0015873590727073204 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/1000 --- Train Loss: 0.032679304311553976 --- Val Loss: 0.0013827499125425126 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/1000 --- Train Loss: 0.026267247523643546 --- Val Loss: 0.0015797784821963154 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/1000 --- Train Loss: 0.027855723336138512 --- Val Loss: 0.0013464195911235646 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/1000 --- Train Loss: 0.01944973378690261 --- Val Loss: 0.0010663359989105663 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.02112809254598014 --- Val Loss: 0.0011117796502263602 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.022563059344413158 --- Val Loss: 0.0013717536746365297 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.017605276741115197 --- Val Loss: 0.001098853717024027 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.017937487608694732 --- Val Loss: 0.0009930242465571988 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.021596907732203612 --- Val Loss: 0.0008443279957794099 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.026323089016245634 --- Val Loss: 0.0009518569714996305 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.026771481946366842 --- Val Loss: 0.000970554037332896 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.022366587892703273 --- Val Loss: 0.0008242539799306683 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.016225468440717775 --- Val Loss: 0.0009571149822858614 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.018566609707011707 --- Val Loss: 0.0008036693515458697 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.020585477020698295 --- Val Loss: 0.000769198077751347 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.016336083677246118 --- Val Loss: 0.0007943643164486887 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.013924986495011248 --- Val Loss: 0.0008159669356691807 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.012563475523265614 --- Val Loss: 0.0007299834650034582 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.020839570258921858 --- Val Loss: 0.0006424347972345715 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.011380972517848741 --- Val Loss: 0.0006717403378812136 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.0250498237464414 --- Val Loss: 0.0005693251097136639 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.01396583866192812 --- Val Loss: 0.0005085957482917928 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.020052503215015595 --- Val Loss: 0.0004806621765631895 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.010381241991046553 --- Val Loss: 0.0005236613550057312 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.011812616109369185 --- Val Loss: 0.00044013867750750704 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.01062472747001573 --- Val Loss: 0.00041087150176253223 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.021016803481430823 --- Val Loss: 0.00045603779083849827 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.014347718614904349 --- Val Loss: 0.0004630809247478677 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.007938599767069478 --- Val Loss: 0.00037241294233459797 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.011480949592091274 --- Val Loss: 0.0004282112494737991 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.01164120753211361 --- Val Loss: 0.0004231920513925912 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.01812591310059508 --- Val Loss: 0.00040346057988528515 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.008292020344852914 --- Val Loss: 0.00037528330867461106 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.010877346151016936 --- Val Loss: 0.0003935408695825853 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.009426722689910525 --- Val Loss: 0.00034669948482500746 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.016480593879140533 --- Val Loss: 0.0003998119301659863 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.011193702838996775 --- Val Loss: 0.0003963860516942647 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.010055633285832408 --- Val Loss: 0.00044102388457894603 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.012731125989956715 --- Val Loss: 0.00037106384635375735 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.010573906247984573 --- Val Loss: 0.0003593945244064362 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.008008152836509054 --- Val Loss: 0.00031225513527228984 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.010903993735782627 --- Val Loss: 0.0003069310360052494 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.012103558525636415 --- Val Loss: 0.0002384617267573712 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.013455834385102854 --- Val Loss: 0.0002806714813494963 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.012856370179358142 --- Val Loss: 0.0002610092691026752 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.007795844293738848 --- Val Loss: 0.00030088355665193814 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.012118010287138022 --- Val Loss: 0.00028576077767828714 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.01923235729285391 --- Val Loss: 0.00023003985297028265 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.011959873392473315 --- Val Loss: 0.00026949938830717437 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.010438219348983398 --- Val Loss: 0.000263759420987647 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.010161064385077492 --- Val Loss: 0.00025949041672228686 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.006789055368329128 --- Val Loss: 0.00023260769795499408 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.01182572715013401 --- Val Loss: 0.00023172361497075867 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.01970508368187601 --- Val Loss: 0.0002242796440277472 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.01102638190962354 --- Val Loss: 0.00021895500547113107 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.007819201575069545 --- Val Loss: 0.00021561428217842737 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.014702122882447047 --- Val Loss: 0.00022686935556922552 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.007662987592756194 --- Val Loss: 0.00018920333959768953 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.008304954778785172 --- Val Loss: 0.0002438522170139956 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.014345771604946904 --- Val Loss: 0.00024290540474539385 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.013768932230739437 --- Val Loss: 0.00022350810485134142 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.013988779716191044 --- Val Loss: 0.00023394293890450656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.009606052530255902 --- Val Loss: 0.00023810689930008863 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.008356225853371946 --- Val Loss: 0.0002619060771682914 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.009696270968511275 --- Val Loss: 0.00019332161593871498 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.00805920260558174 --- Val Loss: 0.00019814794780365994 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.005559672208144087 --- Val Loss: 0.00020308777525676776 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.005087448405418154 --- Val Loss: 0.0001869332506334937 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.008991723418617988 --- Val Loss: 0.00018326639477838982 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.009877588890877776 --- Val Loss: 0.00020751172532931336 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.007386662350953984 --- Val Loss: 0.00019122831083712334 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.005116308195763292 --- Val Loss: 0.00017158329852709028 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.0064367934303419935 --- Val Loss: 0.00014470468132316545 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.004588598016899794 --- Val Loss: 0.00013804319497985273 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.011060129471072334 --- Val Loss: 0.00011831693627992106 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.011976802176592404 --- Val Loss: 0.00013342624666819034 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.007178082412616436 --- Val Loss: 0.00013571464775434584 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.008723430578175897 --- Val Loss: 0.0001850342338261925 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.0055989352962573765 --- Val Loss: 0.00016609278922791708 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.009055685445115067 --- Val Loss: 0.00014030891986574064 --- Train Acc: 1.00 --- Val Acc: 1.00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABONklEQVR4nO3deXhM1/8H8PdMlskmm2xCSIh9CSKCWqqisaW1L6VJ0GoVpZaWaq3V+KmiamtVQivEUjStrREUrUotsdSaoCEkob5kQZaZ8/sjcmWaIOFmbpb363nmaebOmTufOVL37Zxz71UJIQSIiIiIygm10gUQERERyYnhhoiIiMoVhhsiIiIqVxhuiIiIqFxhuCEiIqJyheGGiIiIyhWGGyIiIipXGG6IiIioXGG4ISIionKF4YaIqAJyd3dHjx49lC6DqEQw3BApYNmyZVCpVPD19VW6FCoh7u7uUKlUhT66dOmidHlE5Zqx0gUQVUTh4eFwd3dHTEwM4uLi4OnpqXRJVAKaNm2KCRMmFNju6uqqQDVEFQfDDZGBXblyBX/88Qe2bNmCd955B+Hh4Zg+fbrSZRUqIyMDlpaWSpdRKuXk5ECn08HU1PSJbapWrYohQ4YYsCoiAjgtRWRw4eHhsLOzQ/fu3dG3b1+Eh4cX2u7u3bv44IMP4O7uDo1Gg2rVqiEwMBC3b9+W2jx8+BAzZsxAnTp1YGZmhipVqqB3796Ij48HAOzfvx8qlQr79+/X2/fVq1ehUqmwevVqaVtwcDCsrKwQHx+Pbt26oVKlShg8eDAA4ODBg+jXrx+qV68OjUYDNzc3fPDBB3jw4EGBus+fP4/+/fvD0dER5ubmqFu3LqZOnQoA2LdvH1QqFbZu3VrgfevWrYNKpcLhw4ef2n+XL19Gv379YG9vDwsLC7Rq1Qrbt2+XXk9OToaxsTFmzpxZ4L0XLlyASqXCkiVL9Pp53LhxcHNzg0ajgaenJ/7v//4POp2uQH/Nnz8fixYtQq1ataDRaHD27Nmn1loUef1++fJl+Pv7w9LSEq6urpg1axaEEHptMzIyMGHCBKnWunXrYv78+QXaAcDatWvRsmVLWFhYwM7ODu3bt8evv/5aoN2hQ4fQsmVLmJmZoWbNmvj+++/1Xs/OzsbMmTNRu3ZtmJmZoXLlymjbti2ioqJe+LsTlRSO3BAZWHh4OHr37g1TU1MMGjQIy5cvx19//QUfHx+pTXp6Otq1a4dz585h2LBhaN68OW7fvo3IyEhcv34dDg4O0Gq16NGjB6KjozFw4ECMHTsWaWlpiIqKwpkzZ1CrVq1i15aTkwN/f3+0bdsW8+fPh4WFBQBg06ZNuH//PkaOHInKlSsjJiYGX3/9Na5fv45NmzZJ7z916hTatWsHExMTjBgxAu7u7oiPj8fPP/+MOXPm4OWXX4abmxvCw8PRq1evAv1Sq1YttG7d+on1JScno02bNrh//z7ef/99VK5cGWvWrMFrr72GzZs3o1evXnB2dkaHDh2wcePGAiNiGzZsgJGREfr16wcAuH//Pjp06IDExES88847qF69Ov744w9MmTIFN2/exKJFi/TeHxYWhocPH2LEiBHQaDSwt7d/an9mZ2frhdE8lpaWMDc3l55rtVp06dIFrVq1wrx587Br1y5Mnz4dOTk5mDVrFgBACIHXXnsN+/btw/Dhw9G0aVPs3r0bkyZNQmJiIhYuXCjtb+bMmZgxYwbatGmDWbNmwdTUFEeOHMHevXvx6quvSu3i4uLQt29fDB8+HEFBQQgNDUVwcDC8vb3RsGFDAMCMGTMQEhKCt956Cy1btkRqaiqOHj2K48ePo3Pnzk/9/kSKEURkMEePHhUARFRUlBBCCJ1OJ6pVqybGjh2r127atGkCgNiyZUuBfeh0OiGEEKGhoQKAWLBgwRPb7Nu3TwAQ+/bt03v9ypUrAoAICwuTtgUFBQkAYvLkyQX2d//+/QLbQkJChEqlEv/884+0rX379qJSpUp62/LXI4QQU6ZMERqNRty9e1falpKSIoyNjcX06dMLfE5+48aNEwDEwYMHpW1paWnCw8NDuLu7C61WK4QQ4ptvvhEAxOnTp/Xe36BBA/HKK69Iz2fPni0sLS3FxYsX9dpNnjxZGBkZiYSEBCHE4/6ytrYWKSkpT60xT40aNQSAQh8hISFSu7x+HzNmjLRNp9OJ7t27C1NTU3Hr1i0hhBDbtm0TAMRnn32m9zl9+/YVKpVKxMXFCSGEuHTpklCr1aJXr15Sf+Tf73/rO3DggLQtJSVFaDQaMWHCBGmbl5eX6N69e5G+M1FpwWkpIgMKDw+Hs7MzOnbsCABQqVQYMGAAIiIioNVqpXY//vgjvLy8Coxu5L0nr42DgwPGjBnzxDbPY+TIkQW25R9lyMjIwO3bt9GmTRsIIXDixAkAwK1bt3DgwAEMGzYM1atXf2I9gYGByMzMxObNm6VtGzZsQE5OzjPXp+zYsQMtW7ZE27ZtpW1WVlYYMWIErl69Kk0T9e7dG8bGxtiwYYPU7syZMzh79iwGDBggbdu0aRPatWsHOzs73L59W3r4+flBq9XiwIEDep/fp08fODo6PrXG/Hx9fREVFVXgMWjQoAJtR48eLf2sUqkwevRoZGVlYc+ePdJ3NzIywvvvv6/3vgkTJkAIgZ07dwIAtm3bBp1Oh2nTpkGt1v8r/r+/Fw0aNEC7du2k546Ojqhbty4uX74sbbO1tcXff/+NS5cuFfl7EymN4YbIQLRaLSIiItCxY0dcuXIFcXFxiIuLg6+vL5KTkxEdHS21jY+PR6NGjZ66v/j4eNStWxfGxvLNLhsbG6NatWoFtickJCA4OBj29vawsrKCo6MjOnToAAC4d+8eAEgHxGfVXa9ePfj4+OitNQoPD0erVq2eedbYP//8g7p16xbYXr9+fel1AHBwcECnTp2wceNGqc2GDRtgbGyM3r17S9suXbqEXbt2wdHRUe/h5+cHAEhJSdH7HA8Pj6fW918ODg7w8/Mr8KhRo4ZeO7VajZo1a+ptq1OnDoDc9T55383V1RWVKlV66nePj4+HWq1GgwYNnlnff0MoANjZ2eF///uf9HzWrFm4e/cu6tSpg8aNG2PSpEk4derUM/dNpCSuuSEykL179+LmzZuIiIhAREREgdfDw8P11kPI4UkjOPlHifLTaDQF/rWv1WrRuXNn3LlzBx999BHq1asHS0tLJCYmIjg4WG/hbVEFBgZi7NixuH79OjIzM/Hnn3/qLfKVw8CBAzF06FDExsaiadOm2LhxIzp16gQHBwepjU6nQ+fOnfHhhx8Wuo+8gJEn/whWeWBkZFTodpFvgXL79u0RHx+Pn376Cb/++iu+++47LFy4ECtWrMBbb71lqFKJioXhhshAwsPD4eTkhKVLlxZ4bcuWLdi6dStWrFgBc3Nz1KpVC2fOnHnq/mrVqoUjR44gOzsbJiYmhbaxs7MDkHtGUH55/8ovitOnT+PixYtYs2YNAgMDpe3/PVsmb+ThWXUDucFj/PjxWL9+PR48eAATExO96aInqVGjBi5cuFBg+/nz56XX8/Ts2RPvvPOONDV18eJFTJkyRe99tWrVQnp6ujRSoxSdTofLly/rhamLFy8CyL0YIJD73fbs2YO0tDS90Zv/fvdatWpBp9Ph7NmzaNq0qSz12dvbY+jQoRg6dCjS09PRvn17zJgxg+GGSi1OSxEZwIMHD7Blyxb06NEDffv2LfAYPXo00tLSEBkZCSB3bcfJkycLPWU671/Vffr0we3btwsd8chrU6NGDRgZGRVYO7Js2bIi1573r/v8/5oXQuCrr77Sa+fo6Ij27dsjNDQUCQkJhdaTx8HBAV27dsXatWsRHh6OLl266I2oPEm3bt0QExOjd7p4RkYGvv32W7i7u+tNxdja2sLf3x8bN25EREQETE1N0bNnT7399e/fH4cPH8bu3bsLfNbdu3eRk5PzzJrkkv/PUQiBJUuWwMTEBJ06dQKQ+921Wm2BP++FCxdCpVKha9euAHJDnVqtxqxZswqMqv33z6Eo/v33X73nVlZW8PT0RGZmZrH3RWQoHLkhMoDIyEikpaXhtddeK/T1Vq1awdHREeHh4RgwYAAmTZqEzZs3o1+/fhg2bBi8vb1x584dREZGYsWKFfDy8kJgYCC+//57jB8/HjExMWjXrh0yMjKwZ88evPfee3j99ddhY2ODfv364euvv4ZKpUKtWrXwyy+/FFhL8jT16tVDrVq1MHHiRCQmJsLa2ho//vij3rqMPIsXL0bbtm3RvHlzjBgxAh4eHrh69Sq2b9+O2NhYvbaBgYHo27cvAGD27NlFqmXy5MlYv349unbtivfffx/29vZYs2YNrly5gh9//LHAlNqAAQMwZMgQLFu2DP7+/rC1tdV7fdKkSYiMjESPHj2kU6AzMjJw+vRpbN68GVevXi1S6HqSxMRErF27tsB2KysrvaBlZmaGXbt2ISgoCL6+vti5cye2b9+Ojz/+WFrAHBAQgI4dO2Lq1Km4evUqvLy88Ouvv+Knn37CuHHjpFP/PT09MXXqVMyePRvt2rVD7969odFo8Ndff8HV1RUhISHF+g4NGjTAyy+/DG9vb9jb2+Po0aPYvHmz3gJoolJHqdO0iCqSgIAAYWZmJjIyMp7YJjg4WJiYmIjbt28LIYT4999/xejRo0XVqlWFqampqFatmggKCpJeFyL3FO2pU6cKDw8PYWJiIlxcXETfvn1FfHy81ObWrVuiT58+wsLCQtjZ2Yl33nlHnDlzptBTwS0tLQut7ezZs8LPz09YWVkJBwcH8fbbb4uTJ08W2IcQQpw5c0b06tVL2NraCjMzM1G3bl3x6aefFthnZmamsLOzEzY2NuLBgwdF6UYhhBDx8fGib9++0v5btmwpfvnll0LbpqamCnNzcwFArF27ttA2aWlpYsqUKcLT01OYmpoKBwcH0aZNGzF//nyRlZUlhHh8KvgXX3xR5Dqfdip4jRo1pHZ5/R4fHy9effVVYWFhIZydncX06dMLnMqdlpYmPvjgA+Hq6ipMTExE7dq1xRdffKF3inee0NBQ0axZM6HRaISdnZ3o0KGDdAmCvPoKO8W7Q4cOokOHDtLzzz77TLRs2VLY2toKc3NzUa9ePTFnzhypb4hKI5UQzzFOSUT0gnJycuDq6oqAgACsWrVK6XIUExwcjM2bNyM9PV3pUojKDa65ISJFbNu2Dbdu3dJbpExEJAeuuSEigzpy5AhOnTqF2bNno1mzZtL1coiI5MKRGyIyqOXLl2PkyJFwcnIqcJNGIiI5cM0NERERlSscuSEiIqJyheGGiIiIypUKt6BYp9Phxo0bqFSp0gvdOZmIiIgMRwiBtLQ0uLq6Frhg539VuHBz48YNuLm5KV0GERERPYdr166hWrVqT21T4cJN3g3nrl27Bmtra4WrISIioqJITU2Fm5ub3o1jn6TChZu8qShra2uGGyIiojKmKEtKuKCYiIiIyhWGGyIiIipXGG6IiIioXGG4ISIionKF4YaIiIjKFYYbIiIiKlcYboiIiKhcYbghIiKicoXhhoiIiMoVhhsiIiIqVxQNNwcOHEBAQABcXV2hUqmwbdu2Z75n//79aN68OTQaDTw9PbF69eoSr5OIiIjKDkXDTUZGBry8vLB06dIitb9y5Qq6d++Ojh07IjY2FuPGjcNbb72F3bt3l3ClREREVFYoeuPMrl27omvXrkVuv2LFCnh4eODLL78EANSvXx+HDh3CwoUL4e/vX1Jllqz0FCDn4TObPcjWIkcnYKRSwVitwr8ZmQYojoiIqPhMNOZwcKmu2OeXqbuCHz58GH5+fnrb/P39MW7cuCe+JzMzE5mZj4NAampqSZVXfEe+BXZOKlJT8/88ryJ/NURERLI4b1wfDp/8qdjnl6lwk5SUBGdnZ71tzs7OSE1NxYMHD2Bu/t8IAISEhGDmzJmGKrFojv8AnFgLXMv9g88UxhB49i3ciYiIygKtWtl4UabCzfOYMmUKxo8fLz1PTU2Fm5ubcgXduQxEjtbb1CJzBdJgobfNxEgFFVRwstbA3MQI7g6WeLdDTaQ+yIGJkRo+HnbQGBsZsnIiIqIiaajw55epcOPi4oLk5GS9bcnJybC2ti501AYANBoNNBqNIcp7sgf/A4QATC2BdQP1XorTuSINFni/U234N3RGQ1cbhYokIiIqH8pUuGndujV27Nihty0qKgqtW7dWqKIiiBwDHP++wObZ2YNRTXUb69AVByZ1RPXKFoW8mYiIiIpL0XCTnp6OuLg46fmVK1cQGxsLe3t7VK9eHVOmTEFiYiK+/z43HLz77rtYsmQJPvzwQwwbNgx79+7Fxo0bsX37dqW+wtOd31FosNmibYuNJq+jS0MXLG1fk8GGiIhIRoqGm6NHj6Jjx47S87y1MUFBQVi9ejVu3ryJhIQE6XUPDw9s374dH3zwAb766itUq1YN3333Xek8DfzWRWBjIADgqs4ZkbrWeN94G77K6YV/mozD7lfrwtW28Kk0IiIien4qIYRQughDSk1NhY2NDe7duwdra+uS+ZCs+8Dnj0/W9such6vCBb7qc5j0zltoWsOhZD6XiIionCrO8btMrbkpM+5dk368LawRJ6qhkpkxvpkyEVYadjkREVFJ4o0zS8K969KPY7NHwdRYjYMfdmSwISIiMgCGm5KQegMAEG/TCr/rGuONltVha2GqcFFEREQVA8NNSUi7CQBIyLEDANR0tFSyGiIiogqF4aYkZKYBAOJTc7u3TS0uICYiIjIUhpuS8Ogu3/eFCbzcbOHpZKVwQURERBUHw01JeBRuMoUpejerqnAxREREFQvDTQnIyrwPAHgIE/RoUuUZrYmIiEhODDclICfzQe5/1RpUtlL4pp1EREQVDMNNCdBl5YYblbGZwpUQERFVPAw3JSAv3MCE4YaIiMjQGG5KgHi0oFhtwhtjEhERGRrDTUnIfhRuTBluiIiIDI3hpgSotLnhxsiU01JERESGxnBTAlSPpqWMTS0UroSIiKjiYbgpAWptFgDARMNpKSIiIkNjuCkBxrpHIzca3jCTiIjI0BhuSoCRLhsAYKzhBfyIiIgMjeGmBKiEFgCgMTFVuBIiIqKKh+FGbkLACLnhxtTEROFiiIiIKh6GG7kJnfSjiSlHboiIiAyN4UZuOq30o5kpR26IiIgMjeFGbroc6UeNhiM3REREhsZwIzeRf+SGZ0sREREZGsON3PKP3HBaioiIyOAYbuSWb80Nww0REZHhMdzI7VG40QkVzDUMN0RERIbGcCO3R9NSOVDDzNhI4WKIiIgqHoYbuT1aUKyDGhoTdi8REZGh8egrN2nkxghGapXCxRAREVU8DDcy02ofj9yYqNm9REREhsajr8y0OVkActfcGBlx5IaIiMjQGG5klpOTOy2lhRFHboiIiBTAo6/MtNq8cKPmmhsiIiIFMNzITJudDSB3QbExww0REZHBMdzITPdo5EYHFdQMN0RERAbHcCOzx9NSvIAfERGREhhuZPZ45IZdS0REpAQegWWmzcldc6NTceSGiIhICQw3Mns8csNwQ0REpASGG5nlrbnhyA0REZEyGG5kJvJd54aIiIgMj0dgmeWN3AiO3BARESmC4UZmXHNDRESkLIYbmYlHZ0sJFbuWiIhICTwCy0yn0+b+V2WscCVEREQVE8ONzHTSmht2LRERkRJ4BJaZTsdTwYmIiJTEcCOzvFPBwXBDRESkCIYbmem0eWtu2LVERERK4BFYZkLkhhuO3BARESmD4UZmQqvL/YEjN0RERIrgEVhmOh3DDRERkZJ4BJbZ42kpdi0REZESeASWmZBGblTKFkJERFRBMdzITXBaioiISEk8AstMPAo3vEIxERGRMngElplKiEc/sWuJiIiUwCOwzIQ0LaVsHURERBUVw43cHo3ccFqKiIhIGTwCy40LiomIiBTFI7Dc8sINu5aIiEgRPALLjWdLERERKYpHYLkJXsSPiIhISQw3cuOaGyIiIkXxCCw3rrkhIiJSlOJH4KVLl8Ld3R1mZmbw9fVFTEzMU9svWrQIdevWhbm5Odzc3PDBBx/g4cOHBqq2CPLCjVrxriUiIqqQFD0Cb9iwAePHj8f06dNx/PhxeHl5wd/fHykpKYW2X7duHSZPnozp06fj3LlzWLVqFTZs2ICPP/7YwJU/zaMrFHNaioiISBGKHoEXLFiAt99+G0OHDkWDBg2wYsUKWFhYIDQ0tND2f/zxB1566SW88cYbcHd3x6uvvopBgwY9c7THoB7dFVzwEsVERESKUCzcZGVl4dixY/Dz83tcjFoNPz8/HD58uND3tGnTBseOHZPCzOXLl7Fjxw5069btiZ+TmZmJ1NRUvUfJyg03Ko7cEBERKcJYqQ++ffs2tFotnJ2d9bY7Ozvj/Pnzhb7njTfewO3bt9G2bVsIIZCTk4N33333qdNSISEhmDlzpqy1PxVvv0BERKSoMnUE3r9/Pz7//HMsW7YMx48fx5YtW7B9+3bMnj37ie+ZMmUK7t27Jz2uXbtWskWKvJEbTksREREpQbGRGwcHBxgZGSE5OVlve3JyMlxcXAp9z6effoo333wTb731FgCgcePGyMjIwIgRIzB16lSoCzlDSaPRQKPRyP8FnkDF69wQEREpSrEjsKmpKby9vREdHS1t0+l0iI6ORuvWrQt9z/379wsEGCMjIwCAeDQdpDiGGyIiIkUpNnIDAOPHj0dQUBBatGiBli1bYtGiRcjIyMDQoUMBAIGBgahatSpCQkIAAAEBAViwYAGaNWsGX19fxMXF4dNPP0VAQIAUcpTHe0sREREpSdFwM2DAANy6dQvTpk1DUlISmjZtil27dkmLjBMSEvRGaj755BOoVCp88sknSExMhKOjIwICAjBnzhylvkJBj0aQeLYUERGRMlSi1MznGEZqaipsbGxw7949WFtby77/o18NQov/7cAf7qPRJrgUhS4iIqIyrDjHbw4vyE3wCsVERERK4hFYblxQTEREpCgegWXHkRsiIiIl8Qgss7zr3Kh4V3AiIiJF8AgsM+kifrxxJhERkSIYbmTHaSkiIiIl8QgsN2laiiM3RERESmC4kdnje0uVlismExERVSwMNzJTgaeCExERKYlHYLnlXcSP01JERESKYLiRXd69pTgtRUREpASGG5lJ17lRceSGiIhICQw3MuOCYiIiImUx3Mgub1qKXUtERKQEHoFlJo3c8PYLREREiuARWGYqaeSGa26IiIiUwHAjM47cEBERKYtHYJnlXcSPa26IiIiUwSOw7LigmIiISEk8AstM9egKxSpOSxERESmCR2CZPb63FK9zQ0REpASGG7kJTksREREpiUdgmanBs6WIiIiUxCOwzLjmhoiISFk8AsuMp4ITEREpi0dgmUlXKObIDRERkSJ4BJZZ3hWKOXJDRESkDB6BZcZ7SxERESmL4UZ2edNSvM4NERGREhhuZMYFxURERMriEVhmap4KTkREpCgegWWWt+aGt18gIiJSBsONzKRpKTUXFBMRESmB4UZmj8+WYtcSEREpgUdgmT2elmLXEhERKYFHYJmpH13ET81wQ0REpAgegUsKl9wQEREpguFGZtK0FNMNERGRIhhuZMYbZxIRESmLR2CZ8WwpIiIiZfEILDPeOJOIiEhZDDclhSM3REREiuARWGZq5J0KzpEbIiIiJTDcyC5vQTHDDRERkRIYbmSWF2kEu5aIiEgRPALLTC3dOJNdS0REpAQegWWWN3LDNTdERETKYLiRm+AViomIiJTEcCMzFRcUExERKYrhRmZq6SJ+RgpXQkREVDEx3Mju0bQU19wQEREpguFGZnmRhrdfICIiUgbDjcx4KjgREZGyeASWGUduiIiIlMVwI7PHdwVn1xIRESmh2Edgd3d3zJo1CwkJCSVRTzmQF244ckNERKSEYoebcePGYcuWLahZsyY6d+6MiIgIZGZmlkRtZZJ0KjjX3BARESniucJNbGwsYmJiUL9+fYwZMwZVqlTB6NGjcfz48ZKosUyRpqU440dERKSI5z4CN2/eHIsXL8aNGzcwffp0fPfdd/Dx8UHTpk0RGhoKId2GoGKRJqOYbYiIiBRh/LxvzM7OxtatWxEWFoaoqCi0atUKw4cPx/Xr1/Hxxx9jz549WLdunZy1lnpCCKhVuaFOzQXFREREiih2uDl+/DjCwsKwfv16qNVqBAYGYuHChahXr57UplevXvDx8ZG10LJApxPIu+kCz5YiIiJSRrHDjY+PDzp37ozly5ejZ8+eMDExKdDGw8MDAwcOlKXAskQInfQzz5YiIiJSRrHDzeXLl1GjRo2ntrG0tERYWNhzF1VW5V9nxJEbIiIiZRT7CJySkoIjR44U2H7kyBEcPXpUlqLKKqF7PHIDI4YbIiIiJRT7CDxq1Chcu3atwPbExESMGjVKlqLKKl3+aSlwWoqIiEgJxQ43Z8+eRfPmzQtsb9asGc6ePStLUWUWp6WIiIgUV+wjsEajQXJycoHtN2/ehLHxc59ZXi7kX3PDCxQTEREpo9iH4FdffRVTpkzBvXv3pG13797Fxx9/jM6dOxe7gKVLl8Ld3R1mZmbw9fVFTEzMU9vfvXsXo0aNQpUqVaDRaFCnTh3s2LGj2J9bEnQ6rfSzSmX0lJZERERUUoo91DJ//ny0b98eNWrUQLNmzQAAsbGxcHZ2xg8//FCsfW3YsAHjx4/HihUr4Ovri0WLFsHf3x8XLlyAk5NTgfZZWVno3LkznJycsHnzZlStWhX//PMPbG1ti/s1SoT+2VJcc0NERKQElXiO+yRkZGQgPDwcJ0+ehLm5OZo0aYJBgwYVes2bp/H19YWPjw+WLFkCANDpdHBzc8OYMWMwefLkAu1XrFiBL774AufPny/2Z+VJTU2FjY0N7t27B2tr6+fax5Okpf4PlRa4AwAefngdZhaVZN0/ERFRRVWc4/dzLZKxtLTEiBEjnqu4PFlZWTh27BimTJkibVOr1fDz88Phw4cLfU9kZCRat26NUaNG4aeffoKjoyPeeOMNfPTRRzAyKnwaKDMzU++u5ampqS9U99PorbnhgmIiIiJFPPcK4LNnzyIhIQFZWVl621977bUivf/27dvQarVwdnbW2+7s7Izz588X+p7Lly9j7969GDx4MHbs2IG4uDi89957yM7OxvTp0wt9T0hICGbOnFmkml6U0OWbluKKYiIiIkU81xWKe/XqhdOnT0OlUkmjFXlrTLRa7dPe/kJ0Oh2cnJzw7bffwsjICN7e3khMTMQXX3zxxHAzZcoUjB8/XnqempoKNze3kimQt18gIiJSXLGHF8aOHQsPDw+kpKTAwsICf//9Nw4cOIAWLVpg//79Rd6Pg4MDjIyMCpxWnpycDBcXl0LfU6VKFdSpU0dvCqp+/fpISkoqMIKUR6PRwNraWu9RUjgtRUREpLxiH4EPHz6MWbNmwcHBAWq1Gmq1Gm3btkVISAjef//9Iu/H1NQU3t7eiI6OlrbpdDpER0ejdevWhb7npZdeQlxcHHT5bnNw8eJFVKlSBaampsX9KrLTOxWc01JERESKKPYRWKvVolKl3LOAHBwccOPGDQBAjRo1cOHChWLta/z48Vi5ciXWrFmDc+fOYeTIkcjIyMDQoUMBAIGBgXoLjkeOHIk7d+5g7NixuHjxIrZv347PP/+81Nz2gTfOJCIiUl6x19w0atQIJ0+ehIeHB3x9fTFv3jyYmpri22+/Rc2aNYu1rwEDBuDWrVuYNm0akpKS0LRpU+zatUtaZJyQkAB1vhEQNzc37N69Gx988AGaNGmCqlWrYuzYsfjoo4+K+zVKhN5Z9VxzQ0REpIhiX+dm9+7dyMjIQO/evREXF4cePXrg4sWLqFy5MjZs2IBXXnmlpGqVRUle5+ZWUgIcVzTOfTLj3tMbExERUZGV6HVu/P39pZ89PT1x/vx53LlzB3Z2djxD6FFM1AlV8ef7iIiISBbFOgZnZ2fD2NgYZ86c0dtub2/PYIPH17kp9iWfiYiISDbFCjcmJiaoXr16iV7LpiwT0D36L4MeERGRUoo9ezJ16lR8/PHHuHPnTknUU6blnaLOcENERKScYq+5WbJkCeLi4uDq6ooaNWrA0tJS7/Xjx4/LVlyZIxhuiIiIlFbscNOzZ88SKKN8yDvxjOGGiIhIOcUON0+6hxNxQTEREVFpwDOW5fRoWkrHbiUiIlJMsUdu1Gr1U0/7rshnUhXzeohERERUAoodbrZu3ar3PDs7GydOnMCaNWswc+ZM2Qori8SjCSnBa/4QEREpptjh5vXXXy+wrW/fvmjYsCE2bNiA4cOHy1JYWSQe3RWcC4qJiIiUI9vikFatWiE6Olqu3ZVJedNSOoYbIiIixcgSbh48eIDFixejatWqcuyu7JLW3DDcEBERKaXY01L/vUGmEAJpaWmwsLDA2rVrZS2urBG8iB8REZHiih1uFi5cqBdu1Go1HB0d4evrCzs7O1mLK2t4nRsiIiLlFTvcBAcHl0AZ5cPjG2fyOjdERERKKfZROCwsDJs2bSqwfdOmTVizZo0sRZVZgiM3RERESit2uAkJCYGDg0OB7U5OTvj8889lKaqs4l3BiYiIlFfscJOQkAAPD48C22vUqIGEhARZiiqzeONMIiIixRU73Dg5OeHUqVMFtp88eRKVK1eWpaiyi+GGiIhIacUON4MGDcL777+Pffv2QavVQqvVYu/evRg7diwGDhxYEjWWGXlXKOZ1boiIiJRT7LOlZs+ejatXr6JTp04wNs59u06nQ2BgYIVfc5N3DT+O3BARESmn2OHG1NQUGzZswGeffYbY2FiYm5ujcePGqFGjRknUV6bw9gtERETKK3a4yVO7dm3Url1bzlrKvkdXKAbvCk5ERKSYYq+56dOnD/7v//6vwPZ58+ahX79+shRVVumk2y8QERGRUoodbg4cOIBu3boV2N61a1ccOHBAlqLKKhVvnElERKS4Yoeb9PR0mJqaFthuYmKC1NRUWYoqqx6vueHtF4iIiJRS7KNw48aNsWHDhgLbIyIi0KBBA1mKKqvy7gpOREREyin2guJPP/0UvXv3Rnx8PF555RUAQHR0NNatW4fNmzfLXmBZInj7BSIiIsUVO9wEBARg27Zt+Pzzz7F582aYm5vDy8sLe/fuhb29fUnUWIbwCsVERERKe65Twbt3747u3bsDAFJTU7F+/XpMnDgRx44dg1arfca7y6+8NTeCp4ITEREp5rlXvh44cABBQUFwdXXFl19+iVdeeQV//vmnnLWVPbq8NTcMN0REREop1shNUlISVq9ejVWrViE1NRX9+/dHZmYmtm3bVuEXEwOAWeplAICKV7ohIiJSTJFHbgICAlC3bl2cOnUKixYtwo0bN/D111+XZG1ljmfMpwAAN12iwpUQERFVXEUeudm5cyfef/99jBw5krddICIiolKryCM3hw4dQlpaGry9veHr64slS5bg9u3bJVkbERERUbEVOdy0atUKK1euxM2bN/HOO+8gIiICrq6u0Ol0iIqKQlpaWknWSURERFQkxT5bytLSEsOGDcOhQ4dw+vRpTJgwAXPnzoWTkxNee+21kqiRiIiIqMhe6CZIdevWxbx583D9+nWsX79erpqIiIiInpssd3g0MjJCz549ERkZKcfuiIiIiJ4bb19NRERE5QrDDREREZUrDDdERERUrjDcEBERUbnCcENERETlCsMNERERlSsMNzK6a+4GANhXeZDClRAREVVcDDcyylJpAADX7FspXAkREVHFxXAjI51OBwAwNy3yzdaJiIhIZgw3MhKPwo0Fww0REZFiGG5kpBMCAMMNERGRkhhuZCQehRszhhsiIiLFMNzI6VG4MTJitxIRESmFR2FZ5YYbFVQK10FERFRxMdzISPUo3EDFcENERKQUhhtZPQo3HLkhIiJSDMONnKSBG4YbIiIipTDcyIiRhoiISHkMNzKS1tyoGXOIiIiUwnAjq7yzpditRERESuFRWEaPz5ZStg4iIqKKjOGmRDDdEBERKYXhRkZ5IzcqFbuViIhIKTwKy4oX8SMiIlIaw42MVCJv5IbhhoiISCkMNzJSFfITERERGVapCDdLly6Fu7s7zMzM4Ovri5iYmCK9LyIiAiqVCj179izZAouMIzdERERKUzzcbNiwAePHj8f06dNx/PhxeHl5wd/fHykpKU9939WrVzFx4kS0a9fOQJU+m4rhhoiISHGKh5sFCxbg7bffxtChQ9GgQQOsWLECFhYWCA0NfeJ7tFotBg8ejJkzZ6JmzZoGrPbp8iKNYLghIiJSjKLhJisrC8eOHYOfn5+0Ta1Ww8/PD4cPH37i+2bNmgUnJycMHz78mZ+RmZmJ1NRUvUfJ4RWKiYiIlKboUfj27dvQarVwdnbW2+7s7IykpKRC33Po0CGsWrUKK1euLNJnhISEwMbGRnq4ubm9cN1PIk1LMdsQEREppkwdhtPS0vDmm29i5cqVcHBwKNJ7pkyZgnv37kmPa9eulXCVAM+WIiIiUo6xkh/u4OAAIyMjJCcn621PTk6Gi4tLgfbx8fG4evUqAgICpG06nQ4AYGxsjAsXLqBWrVp679FoNNBoNCVQfWG4oJiIiEhpio7cmJqawtvbG9HR0dI2nU6H6OhotG7dukD7evXq4fTp04iNjZUer732Gjp27IjY2NgSnXIqirxIw3BDRESkHEVHbgBg/PjxCAoKQosWLdCyZUssWrQIGRkZGDp0KAAgMDAQVatWRUhICMzMzNCoUSO999va2gJAge1KyLtCMaeliIiIlKN4uBkwYABu3bqFadOmISkpCU2bNsWuXbukRcYJCQlQq8vK0qC8BcUMN0REREpRCSENN1QIqampsLGxwb1792BtbS3rvu/NqAYbpOHvXnvQ0MtH1n0TERFVZMU5fpeVIZEyIe9U8AqVFomIiEoZhhsZqTgtRUREpDiGG1nlXaGY4YaIiEgpDDcykiINL1FMRESkGB6FZcS7ghMRESmP4UZWDDdERERKY7iRkaqQn4iIiMiwGG5kxGkpIiIi5THcyCgv3IDhhoiISDEMNyWAIzdERETKYbgpAbzODRERkXIYbmTEaSkiIiLlMdzIiLdfICIiUh7DjYxU0n8ZboiIiJTCcCMrngpORESkNIYbGXHNDRERkfIYbmTEi/gREREpj+FGRrwrOBERkfJ4FJaRmiM3REREimO4KREMN0REREphuJGLEI9/5sgNERGRYhhu5JIv3HBaioiISDkMN7LJF244LUVERKQYhhu55B+54e0XiIiIFMNwI5t8a27YrURERIrhUbgEcOSGiIhIOQw3csl/thQREREphuFGNjxbioiIqDRguJGL3qng7FYiIiKl8CgsG47cEBERlQYMN3LhRfyIiIhKBYYb2TDcEBERlQYMN3LhvaWIiIhKBYYb2fD2C0RERKUBw41c8o/c8CJ+REREimG4kU3+kRt2KxERkVJ4FJYLb5xJRERUKjDcyEQIrrkhIiIqDRhuZKIXbtTsViIiIqXwKCwXjtwQERGVCgw3MtG7Jzivc0NERKQYhhuZCKGTfma2ISIiUg7DjUz0FxSzW4mIiJTCo7BMBC/iR0REVCow3MiE01JERESlA8ONTPJGbnSC50oREREpieFGLo/CjQCg4tANERGRYhhuZCKkcMORGyIiIiUx3Mgmd82NgIprboiIiBTEcCMTocs3LcWxGyIiIsUw3MhESP/lyA0REZGSGG5kInR5p4Iz2RARESmJ4UYmAvnPllK2FiIiooqM4UYm+mdLMd0QEREpheFGLrp84YbZhoiISDEMN7LJf7YUERERKYXhRiZ6dwXn0A0REZFiGG5kwisUExERlQ4MNzLJuys419wQEREpi+FGZrxxJhERkbKMlS6gvHi85obBhogqDq1Wi+zsbKXLoHLC1NQUavWLj7sw3MhFPD5bioiovBNCICkpCXfv3lW6FCpH1Go1PDw8YGpq+kL7YbiRSf41N0RE5V1esHFycoKFhQWn4+mF6XQ63LhxAzdv3kT16tVf6HeK4UYu+c6WIiIqz7RarRRsKleurHQ5VI44Ojrixo0byMnJgYmJyXPvhwuKZZL/3lJEROVZ3hobCwsLhSuh8iZvOkqr1b7Qfhhu5KLjgmIiqlg4FUVyk+t3qlSEm6VLl8Ld3R1mZmbw9fVFTEzME9uuXLkS7dq1g52dHezs7ODn5/fU9oYiwDU3REQVjbu7OxYtWqR0GfQfioebDRs2YPz48Zg+fTqOHz8OLy8v+Pv7IyUlpdD2+/fvx6BBg7Bv3z4cPnwYbm5uePXVV5GYmGjgyvXlnQnOcENEVPqoVKqnPmbMmPFc+/3rr78wYsQIWWpcv349jIyMMGrUKFn2V5GpRP6bIinA19cXPj4+WLJkCYDc1dJubm4YM2YMJk+e/Mz3a7Va2NnZYcmSJQgMDHxm+9TUVNjY2ODevXuwtrZ+4frzJF86BufwV3BL2MBxZoJs+yUiKm0ePnyIK1euwMPDA2ZmZkqXUyRJSUnSzxs2bMC0adNw4cIFaZuVlRWsrKwA5J7mrtVqYWxs2HNu/Pz84OPjg2+++QY3btxQtG+zsrJe+HTs5/G0363iHL8VHbnJysrCsWPH4OfnJ21Tq9Xw8/PD4cOHi7SP+/fvIzs7G/b29iVVZtHwVHAiolLLxcVFetjY2EClUknPz58/j0qVKmHnzp3w9vaGRqPBoUOHEB8fj9dffx3Ozs6wsrKCj48P9uzZo7ff/05LqVQqfPfdd+jVqxcsLCxQu3ZtREZGPrO+K1eu4I8//sDkyZNRp04dbNmypUCb0NBQNGzYEBqNBlWqVMHo0aOl1+7evYt33nkHzs7OMDMzQ6NGjfDLL78AAGbMmIGmTZvq7WvRokVwd3eXngcHB6Nnz56YM2cOXF1dUbduXQDADz/8gBYtWqBSpUpwcXHBG2+8UWBm5e+//0aPHj1gbW2NSpUqoV27doiPj8eBAwdgYmKiFywBYNy4cWjXrt0z++RFKBpubt++Da1WC2dnZ73tzs7OBTrjST766CO4urrqBaT8MjMzkZqaqvcoCbxCMRFVZEII3M/KMfhDzsmHyZMnY+7cuTh37hyaNGmC9PR0dOvWDdHR0Thx4gS6dOmCgIAAJCQ8fXR+5syZ6N+/P06dOoVu3bph8ODBuHPnzlPfExYWhu7du8PGxgZDhgzBqlWr9F5fvnw5Ro0ahREjRuD06dOIjIyEp6cngNwZj65du+L333/H2rVrcfbsWcydOxdGRkbF+v7R0dG4cOECoqKipGCUnZ2N2bNn4+TJk9i2bRuuXr2K4OBg6T2JiYlo3749NBoN9u7di2PHjmHYsGHIyclB+/btUbNmTfzwww9S++zsbISHh2PYsGHFqq24yvR1bubOnYuIiAjs37//icN3ISEhmDlzpsFq4qngRFQRPcjWosG03Qb/3LOz/GFhKs+hbNasWejcubP03N7eHl5eXtLz2bNnY+vWrYiMjNQbNfmv4OBgDBo0CADw+eefY/HixYiJiUGXLl0Kba/T6bB69Wp8/fXXAICBAwdiwoQJ0vQMAHz22WeYMGECxo4dK73Px8cHALBnzx7ExMTg3LlzqFOnDgCgZs2axf7+lpaW+O677/Smo/KHkJo1a2Lx4sXw8fFBeno6rKyssHTpUtjY2CAiIkK6Lk1eDQAwfPhwhIWFYdKkSQCAn3/+GQ8fPkT//v2LXV9xKDpy4+DgACMjIyQnJ+ttT05OhouLy1PfO3/+fMydOxe//vormjRp8sR2U6ZMwb1796THtWvXZKn9vxReukRERC+oRYsWes/T09MxceJE1K9fH7a2trCyssK5c+eeOXKT/5hkaWkJa2vrJ54kAwBRUVHIyMhAt27dAOQeGzt37ozQ0FAAQEpKCm7cuIFOnToV+v7Y2FhUq1ZNL1Q8j8aNGxdYZ3Ps2DEEBASgevXqqFSpEjp06AAAUh/ExsaiXbt2T7zgXnBwMOLi4vDnn38CAFavXo3+/fvD0tLyhWp9FkVHbkxNTeHt7Y3o6Gj07NkTQG6CjY6OfmoqnjdvHubMmYPdu3cX+GX8L41GA41GI2fZheOaGyKqwMxNjHB2lr8inyuX/x5wJ06ciKioKMyfPx+enp4wNzdH3759kZWV9dT9/PdAr1KpoNPpnth+1apVuHPnDszNzaVtOp0Op06dwsyZM/W2F+ZZr6vV6gL/AC/sZqf//f4ZGRnw9/eHv78/wsPD4ejoiISEBPj7+0t98KzPdnJyQkBAAMLCwuDh4YGdO3di//79T32PHBSflho/fjyCgoLQokULtGzZEosWLUJGRgaGDh0KAAgMDETVqlUREhICAPi///s/TJs2DevWrYO7u7u0Nif/SnclPP69YbghoopHpVLJNj1UWvz+++8IDg5Gr169AOSO5Fy9elXWz/j333/x008/ISIiAg0bNpS2a7VatG3bFr/++iu6dOkCd3d3REdHo2PHjgX20aRJE1y/fh0XL14sdPTG0dERSUlJEEJIF8mLjY19Zm3nz5/Hv//+i7lz58LNzQ0AcPTo0QKfvWbNGmRnZz9x9Oatt97CoEGDUK1aNdSqVQsvvfTSMz/7RSl+nZsBAwZg/vz5mDZtGpo2bYrY2Fjs2rVLWmSckJCAmzdvSu2XL1+OrKws9O3bF1WqVJEe8+fPV+orAHg8LcXJKSKi8qF27drYsmULYmNjcfLkSbzxxhtPHYF5Hj/88AMqV66M/v37o1GjRtLDy8sL3bp1kxYWz5gxA19++SUWL16MS5cu4fjx49IanQ4dOqB9+/bo06cPoqKicOXKFezcuRO7du0CALz88su4desW5s2bh/j4eCxduhQ7d+58Zm3Vq1eHqakpvv76a1y+fBmRkZGYPXu2XpvRo0cjNTUVAwcOxNGjR3Hp0iX88MMPeqfZ+/v7w9raGp999pk0cFHSFA83QG7n/PPPP8jMzMSRI0fg6+srvbZ//36sXr1aen716lUIIQo8nvcCTLLJm5bi5ciJiMqFBQsWwM7ODm3atEFAQAD8/f3RvHlzWT8jNDQUvXr1KvS2A3369EFkZCRu376NoKAgLFq0CMuWLUPDhg3Ro0cPXLp0SWr7448/wsfHB4MGDUKDBg3w4YcfSvdnql+/PpYtW4alS5fCy8sLMTExmDhx4jNrc3R0xOrVq7Fp0yY0aNAAc+fOLTCQULlyZezduxfp6eno0KEDvL29sXLlSr1RHLVajeDgYGi12iJdj04Oil/Ez9BK6iJ+188cRLXNPZAIR1SdESfbfomISpuyeBE/Utbw4cNx69atZ17zR66L+JWvCVIFVbCMSERE9Ez37t3D6dOnsW7duiJdzFAuDDdykdbccFqKiIgIAF5//XXExMTg3Xff1buGUEljuJGJCgw3RERE+RnitO/CMNzIJMfUFr9qvZFqZA83pYshIiKqwBhuZJJlVwsjsifA3tQUfZUuhoiIqAIrFaeClwd564k5KUVERKQshhuZiEdrbniZGyIiImUx3MiEt18gIiIqHRhuZCJNSzHbEBERKYrhRibStJTCdRARUcl5+eWXMW7cOKXLoGdguJEJR26IiEqvgIAAdOnSpdDXDh48CJVKhVOnTsn2eQ8ePIC9vT0cHByQmZkp236paBhuZKbi2A0RUakzfPhwREVF4fr16wVeCwsLQ4sWLdCkSRPZPu/HH39Ew4YNUa9ePWzbtk22/T4PIQRycnIUrcHQGG5kwpEbIqLSq0ePHtJdrvNLT0/Hpk2bMHz4cPz7778YNGgQqlatCgsLCzRu3Bjr169/rs9btWoVhgwZgiFDhmDVqlUFXv/777/Ro0cPWFtbo1KlSmjXrh3i4+Ol10NDQ9GwYUNoNBpUqVIFo0ePBgBcvXoVKpUKsbGxUtu7d+9CpVJJVwPev38/VCoVdu7cCW9vb2g0Ghw6dAjx8fF4/fXX4ezsDCsrK/j4+GDPnj16dWVmZuKjjz6Cm5sbNBoNPD09sWrVKggh4OnpWeCu4LGxsVCpVIiLK103jGa4kQnX3BBRhSYEkJVh+EcRb1psbGyMwMBArF69Wu9Gx5s2bYJWq8WgQYPw8OFDeHt7Y/v27Thz5gxGjBiBN998EzExMcXqivj4eBw+fBj9+/dH//79cfDgQfzzzz/S64mJiWjfvj00Gg327t2LY8eOYdiwYdLoyvLlyzFq1CiMGDECp0+fRmRkJDw9PYtVAwBMnjwZc+fOxblz59CkSROkp6ejW7duiI6OxokTJ9ClSxcEBAQgISFBek9gYCDWr1+PxYsX49y5c/jmm29gZWUFlUqFYcOGISwsTO8zwsLC0L59++eqryTxCsUyeTxyw3hDRBVQ9n3gc1fDf+7HNwBTyyI1HTZsGL744gv89ttvePnllwHkHpz79OkDGxsb2NjYYOLEiVL7MWPGYPfu3di4cSNatmxZ5JJCQ0PRtWtX2NnZAQD8/f0RFhaGGTNmAACWLl0KGxsbREREwMTEBABQp04d6f2fffYZJkyYgLFjx0rbfHx8ivz5eWbNmqV3s0p7e3t4eXlJz2fPno2tW7ciMjISo0ePxsWLF7Fx40ZERUXBz88PAFCzZk2pfXBwMKZNm4aYmBi0bNkS2dnZWLduXYHRnNKAIzcyKdq/HYiISCn16tVDmzZtEBoaCgCIi4vDwYMHMXz4cACAVqvF7Nmz0bhxY9jb28PKygq7d+/WG9l4Fq1WizVr1mDIkCHStiFDhmD16tXQ6XQAcqdy2rVrJwWb/FJSUnDjxg106tTpRb4qAKBFixZ6z9PT0zFx4kTUr18ftra2sLKywrlz56TvFxsbCyMjI3To0KHQ/bm6uqJ79+5S//3888/IzMxEv379XrhWuXHkRiZ5w5wcuCGiCsnEIncURYnPLYbhw4djzJgxWLp0KcLCwlCrVi3pYP7FF1/gq6++wqJFi9C4cWNYWlpi3LhxyMrKKvL+d+/ejcTERAwYMEBvu1arRXR0NDp37gxzc/Mnvv9prwGAWp07JpF/ai07O7vQtpaW+iNaEydORFRUFObPnw9PT0+Ym5ujb9++0vd71mcDwFtvvYU333wTCxcuRFhYGAYMGAALi+L9GRgCR25kkvdrxnBDRBWSSpU7PWToRzH/0u3fvz/UajXWrVuH77//HsOGDZOWE/z+++94/fXXMWTIEHh5eaFmzZq4ePFisfa/atUqDBw4ELGxsXqPgQMHSguLmzRpgoMHDxYaSipVqgR3d3dER0cXun9HR0cAwM2bN6Vt+RcXP83vv/+O4OBg9OrVC40bN4aLiwuuXr0qvd64cWPodDr89ttvT9xHt27dYGlpieXLl2PXrl0YNmxYkT7b0BhuZPL4xplMN0REpZWVlRUGDBiAKVOm4ObNmwgODpZeq127NqKiovDHH3/g3LlzeOedd5CcnFzkfd+6dQs///wzgoKC0KhRI71HYGAgtm3bhjt37mD06NFITU3FwIEDcfToUVy6dAk//PADLly4AACYMWMGvvzySyxevBiXLl3C8ePH8fXXXwPIHV1p1aqVtFD4t99+wyeffFKk+mrXro0tW7YgNjYWJ0+exBtvvCFNlQGAu7s7goKCMGzYMGzbtg1XrlzB/v37sXHjRqmNkZERgoODMWXKFNSuXRutW7cucv8YEsONTNQqQGOshsaYXUpEVJoNHz4c//vf/+Dv7w9X18eLoD/55BM0b94c/v7+ePnll+Hi4oKePXsWeb/ff/89LC0tC10v06lTJ5ibm2Pt2rWoXLky9u7di/T0dHTo0AHe3t5YuXKltAYnKCgIixYtwrJly9CwYUP06NEDly5dkvYVGhqKnJwceHt7Y9y4cfjss8+KVN+CBQtgZ2eHNm3aICAgAP7+/mjevLlem+XLl6Nv37547733UK9ePbz99tvIyMjQazN8+HBkZWVh6NChRe4bQ1MJUcTz6MqJ1NRU2NjY4N69e7C2tla6HCKiMufhw4e4cuUKPDw8YGZmpnQ5ZGAHDx5Ep06dcO3aNTg7O8u676f9bhXn+M0FxURERPRMmZmZuHXrFmbMmIF+/frJHmzkxDkUIiIieqb169ejRo0auHv3LubNm6d0OU/FcENERETPFBwcDK1Wi2PHjqFq1apKl/NUDDdERERUrjDcEBERUbnCcENERM+lgp1sSwYg1+8Uww0RERVL3vVY7t+/r3AlVN7k3QrCyMjohfbDU8GJiKhYjIyMYGtri5SUFACAhYWFdAsDouel0+lw69YtWFhYwNj4xeIJww0RERWbi4sLAEgBh0gOarUa1atXf+GwzHBDRETFplKpUKVKFTg5OT3xrtRExWVqaird+fxFMNwQEdFzMzIyeuH1EURy44JiIiIiKlcYboiIiKhcYbghIiKicqXCrbnJu0BQamqqwpUQERFRUeUdt4tyob8KF27S0tIAAG5ubgpXQkRERMWVlpYGGxubp7ZRiQp2/WydTocbN26gUqVKsl90KjU1FW5ubrh27Rqsra1l3Tc9xn42DPaz4bCvDYP9bBgl1c9CCKSlpcHV1fWZp4tXuJEbtVqNatWqlehnWFtb838cA2A/Gwb72XDY14bBfjaMkujnZ43Y5OGCYiIiIipXGG6IiIioXGG4kZFGo8H06dOh0WiULqVcYz8bBvvZcNjXhsF+NozS0M8VbkExERERlW8cuSEiIqJyheGGiIiIyhWGGyIiIipXGG6IiIioXGG4kcnSpUvh7u4OMzMz+Pr6IiYmRumSypSQkBD4+PigUqVKcHJyQs+ePXHhwgW9Ng8fPsSoUaNQuXJlWFlZoU+fPkhOTtZrk5CQgO7du8PCwgJOTk6YNGkScnJyDPlVypS5c+dCpVJh3Lhx0jb2szwSExMxZMgQVK5cGebm5mjcuDGOHj0qvS6EwLRp01ClShWYm5vDz88Ply5d0tvHnTt3MHjwYFhbW8PW1hbDhw9Henq6ob9KqabVavHpp5/Cw8MD5ubmqFWrFmbPnq13/yH2dfEdOHAAAQEBcHV1hUqlwrZt2/Rel6tPT506hXbt2sHMzAxubm6YN2+ePF9A0AuLiIgQpqamIjQ0VPz999/i7bffFra2tiI5OVnp0soMf39/ERYWJs6cOSNiY2NFt27dRPXq1UV6errU5t133xVubm4iOjpaHD16VLRq1Uq0adNGej0nJ0c0atRI+Pn5iRMnTogdO3YIBwcHMWXKFCW+UqkXExMj3N3dRZMmTcTYsWOl7eznF3fnzh1Ro0YNERwcLI4cOSIuX74sdu/eLeLi4qQ2c+fOFTY2NmLbtm3i5MmT4rXXXhMeHh7iwYMHUpsuXboILy8v8eeff4qDBw8KT09PMWjQICW+Uqk1Z84cUblyZfHLL7+IK1euiE2bNgkrKyvx1VdfSW3Y18W3Y8cOMXXqVLFlyxYBQGzdulXvdTn69N69e8LZ2VkMHjxYnDlzRqxfv16Ym5uLb7755oXrZ7iRQcuWLcWoUaOk51qtVri6uoqQkBAFqyrbUlJSBADx22+/CSGEuHv3rjAxMRGbNm2S2pw7d04AEIcPHxZC5P7PqFarRVJSktRm+fLlwtraWmRmZhr2C5RyaWlponbt2iIqKkp06NBBCjfsZ3l89NFHom3btk98XafTCRcXF/HFF19I2+7evSs0Go1Yv369EEKIs2fPCgDir7/+ktrs3LlTqFQqkZiYWHLFlzHdu3cXw4YN09vWu3dvMXjwYCEE+1oO/w03cvXpsmXLhJ2dnd7fGx999JGoW7fuC9fMaakXlJWVhWPHjsHPz0/aplar4efnh8OHDytYWdl27949AIC9vT0A4NixY8jOztbr53r16qF69epSPx8+fBiNGzeGs7Oz1Mbf3x+pqan4+++/DVh96Tdq1Ch0795drz8B9rNcIiMj0aJFC/Tr1w9OTk5o1qwZVq5cKb1+5coVJCUl6fWzjY0NfH199frZ1tYWLVq0kNr4+flBrVbjyJEjhvsypVybNm0QHR2NixcvAgBOnjyJQ4cOoWvXrgDY1yVBrj49fPgw2rdvD1NTU6mNv78/Lly4gP/9738vVGOFu3Gm3G7fvg2tVqv3Fz0AODs74/z58wpVVbbpdDqMGzcOL730Eho1agQASEpKgqmpKWxtbfXaOjs7IykpSWpT2J9D3muUKyIiAsePH8dff/1V4DX2szwuX76M5cuXY/z48fj444/x119/4f3334epqSmCgoKkfiqsH/P3s5OTk97rxsbGsLe3Zz/nM3nyZKSmpqJevXowMjKCVqvFnDlzMHjwYABgX5cAufo0KSkJHh4eBfaR95qdnd1z18hwQ6XOqFGjcObMGRw6dEjpUsqda9euYezYsYiKioKZmZnS5ZRbOp0OLVq0wOeffw4AaNasGc6cOYMVK1YgKChI4erKl40bNyI8PBzr1q1Dw4YNERsbi3HjxsHV1ZV9XYFxWuoFOTg4wMjIqMDZJMnJyXBxcVGoqrJr9OjR+OWXX7Bv3z5Uq1ZN2u7i4oKsrCzcvXtXr33+fnZxcSn0zyHvNcqddkpJSUHz5s1hbGwMY2Nj/Pbbb1i8eDGMjY3h7OzMfpZBlSpV0KBBA71t9evXR0JCAoDH/fS0vzdcXFyQkpKi93pOTg7u3LnDfs5n0qRJmDx5MgYOHIjGjRvjzTffxAcffICQkBAA7OuSIFefluTfJQw3L8jU1BTe3t6Ijo6Wtul0OkRHR6N169YKVla2CCEwevRobN26FXv37i0wVOnt7Q0TExO9fr5w4QISEhKkfm7dujVOnz6t9z9UVFQUrK2tCxxoKqpOnTrh9OnTiI2NlR4tWrTA4MGDpZ/Zzy/upZdeKnApg4sXL6JGjRoAAA8PD7i4uOj1c2pqKo4cOaLXz3fv3sWxY8ekNnv37oVOp4Ovr68BvkXZcP/+fajV+ocyIyMj6HQ6AOzrkiBXn7Zu3RoHDhxAdna21CYqKgp169Z9oSkpADwVXA4RERFCo9GI1atXi7Nnz4oRI0YIW1tbvbNJ6OlGjhwpbGxsxP79+8XNmzelx/3796U27777rqhevbrYu3evOHr0qGjdurVo3bq19HreKcqvvvqqiI2NFbt27RKOjo48RfkZ8p8tJQT7WQ4xMTHC2NhYzJkzR1y6dEmEh4cLCwsLsXbtWqnN3Llzha2trfjpp5/EqVOnxOuvv17oqbTNmjUTR44cEYcOHRK1a9eu0KcnFyYoKEhUrVpVOhV8y5YtwsHBQXz44YdSG/Z18aWlpYkTJ06IEydOCABiwYIF4sSJE+Kff/4RQsjTp3fv3hXOzs7izTffFGfOnBERERHCwsKCp4KXJl9//bWoXr26MDU1FS1bthR//vmn0iWVKQAKfYSFhUltHjx4IN577z1hZ2cnLCwsRK9evcTNmzf19nP16lXRtWtXYW5uLhwcHMSECRNEdna2gb9N2fLfcMN+lsfPP/8sGjVqJDQajahXr5749ttv9V7X6XTi008/Fc7OzkKj0YhOnTqJCxcu6LX5999/xaBBg4SVlZWwtrYWQ4cOFWlpaYb8GqVeamqqGDt2rKhevbowMzMTNWvWFFOnTtU7vZh9XXz79u0r9O/koKAgIYR8fXry5EnRtm1bodFoRNWqVcXcuXNlqV8lRL7LOBIRERGVcVxzQ0REROUKww0RERGVKww3REREVK4w3BAREVG5wnBDRERE5QrDDREREZUrDDdERERUrjDcEFGFp1KpsG3bNqXLICKZMNwQkaKCg4OhUqkKPLp06aJ0aURURhkrXQARUZcuXRAWFqa3TaPRKFQNEZV1HLkhIsVpNBq4uLjoPfLuCqxSqbB8+XJ07doV5ubmqFmzJjZv3qz3/tOnT+OVV16Bubk5KleujBEjRiA9PV2vTWhoKBo2bAiNRoMqVapg9OjReq/fvn0bvXr1goWFBWrXro3IyMiS/dJEVGIYboio1Pv000/Rp08fnDx5EoMHD8bAgQNx7tw5AEBGRgb8/f1hZ2eHv/76C5s2bcKePXv0wsvy5csxatQojBgxAqdPn0ZkZCQ8PT31PmPmzJno378/Tp06hW7dumHw4MG4c+eOQb8nEclElttvEhE9p6CgIGFkZCQsLS31HnPmzBFC5N4x/t1339V7j6+vrxg5cqQQQohvv/1W2NnZifT0dOn17du3C7VaLZKSkoQQQri6uoqpU6c+sQYA4pNPPpGep6enCwBi586dsn1PIjIcrrkhIsV17NgRy5cv19tmb28v/dy6dWu911q3bo3Y2FgAwLlz5+Dl5QVLS0vp9Zdeegk6nQ4XLlyASqXCjRs30KlTp6fW0KRJE+lnS0tLWFtbIyUl5Xm/EhEpiOGGiBRnaWlZYJpILubm5kVqZ2JiovdcpVJBp9OVRElEVMK45oaISr0///yzwPP69esDAOrXr4+TJ08iIyNDev3333+HWq1G3bp1UalSJbi7uyM6OtqgNRORcjhyQ0SKy8zMRFJSkt42Y2NjODg4AAA2bdqEFi1aoG3btggPD0dMTAxWrVoFABg8eDCmT5+OoKAgzJgxA7du3cKYMWPw5ptvwtnZGQAwY8YMvPvuu3ByckLXrl2RlpaG33//HWPGjDHsFyUig2C4ISLF7dq1C1WqVNHbVrduXZw/fx5A7plMEREReO+991ClShWsX78eDRo0AABYWFhg9+7dGDt2LHx8fGBhYYE+ffpgwYIF0r6CgoLw8OFDLFy4EBMnToSDgwP69u1ruC9IRAalEkIIpYsgInoSlUqFrVu3omfPnkqXQkRlBNfcEBERUbnCcENERETlCtfcEFGpxplzIioujtwQERFRucJwQ0REROUKww0RERGVKww3REREVK4w3BAREVG5wnBDRERE5QrDDREREZUrDDdERERUrjDcEBERUbny/zu2w80JeKX4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 32))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "#network.add_layer(Layer(128, 64, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "#network.add_layer(Layer(64, 32, l2=0.01)) \n",
    "#network.add_layer(ReLU())\n",
    "network.add_layer(Layer(32, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=1000, learning_rate=0.1, optimizer='GD', momentum=0.5, batch_size=32)\n",
    "\n",
    "network.plot_loss()\n",
    "network.plot_accuracy()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 --- Train Loss: 0.6925719825299245 --- Val Loss: 0.6925872945550915 --- Train Acc: 0.61 --- Val Acc: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10000 --- Train Loss: 0.6918405577688314 --- Val Loss: 0.6919795981875108 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 20/10000 --- Train Loss: 0.6911345889983583 --- Val Loss: 0.691394713307281 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 30/10000 --- Train Loss: 0.6904473480428034 --- Val Loss: 0.6908269483577104 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 40/10000 --- Train Loss: 0.6897752379465361 --- Val Loss: 0.6902732617596505 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 50/10000 --- Train Loss: 0.6891191826564763 --- Val Loss: 0.6897342628585372 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 60/10000 --- Train Loss: 0.6884811788833135 --- Val Loss: 0.6892114760366793 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 70/10000 --- Train Loss: 0.6878565494975831 --- Val Loss: 0.6887009436526554 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 80/10000 --- Train Loss: 0.6872508487676493 --- Val Loss: 0.6882070307663626 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 90/10000 --- Train Loss: 0.6866619404624736 --- Val Loss: 0.6877278462971069 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 100/10000 --- Train Loss: 0.6860842617623207 --- Val Loss: 0.6872587201925054 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 110/10000 --- Train Loss: 0.6855185329212395 --- Val Loss: 0.686800080248225 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 120/10000 --- Train Loss: 0.6849618327319839 --- Val Loss: 0.6863494456612012 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 130/10000 --- Train Loss: 0.6844185715616834 --- Val Loss: 0.685910111418823 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 140/10000 --- Train Loss: 0.6838819559204082 --- Val Loss: 0.6854765610424591 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 150/10000 --- Train Loss: 0.6833566938488674 --- Val Loss: 0.6850522777990552 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 160/10000 --- Train Loss: 0.6828363037744427 --- Val Loss: 0.6846319007807615 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 170/10000 --- Train Loss: 0.6823247421018678 --- Val Loss: 0.6842183108380425 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 180/10000 --- Train Loss: 0.681819646459516 --- Val Loss: 0.6838094138657569 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 190/10000 --- Train Loss: 0.6813155334709822 --- Val Loss: 0.6834005894451524 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 200/10000 --- Train Loss: 0.6808135875963445 --- Val Loss: 0.6829923681644943 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 210/10000 --- Train Loss: 0.680309050177985 --- Val Loss: 0.682580738635302 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 220/10000 --- Train Loss: 0.6798085417054179 --- Val Loss: 0.6821703885266529 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 230/10000 --- Train Loss: 0.6793001442639056 --- Val Loss: 0.681751699935535 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 240/10000 --- Train Loss: 0.6787873117239921 --- Val Loss: 0.6813266428386388 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 250/10000 --- Train Loss: 0.6782646904717178 --- Val Loss: 0.6808906942902744 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 260/10000 --- Train Loss: 0.6777322986453609 --- Val Loss: 0.6804431419469533 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 270/10000 --- Train Loss: 0.6771858028282187 --- Val Loss: 0.6799799811457956 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 280/10000 --- Train Loss: 0.6766233795229792 --- Val Loss: 0.6794986447367595 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 290/10000 --- Train Loss: 0.6760394984944921 --- Val Loss: 0.6789940030623423 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 300/10000 --- Train Loss: 0.6754266569208558 --- Val Loss: 0.6784594275810154 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 310/10000 --- Train Loss: 0.6747858577107677 --- Val Loss: 0.6778944035809165 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 320/10000 --- Train Loss: 0.6741126331128405 --- Val Loss: 0.6772941359643864 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 330/10000 --- Train Loss: 0.6733976582478222 --- Val Loss: 0.6766499632683389 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 340/10000 --- Train Loss: 0.6726369254185676 --- Val Loss: 0.6759567514589725 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 350/10000 --- Train Loss: 0.6718224136363388 --- Val Loss: 0.67520681326965 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 360/10000 --- Train Loss: 0.6709430054260733 --- Val Loss: 0.6743897045269196 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 370/10000 --- Train Loss: 0.669995507875241 --- Val Loss: 0.6734998865055597 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 380/10000 --- Train Loss: 0.6689634332232323 --- Val Loss: 0.6725224944870644 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 390/10000 --- Train Loss: 0.6678420745347955 --- Val Loss: 0.6714504843893375 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 400/10000 --- Train Loss: 0.6666171642714089 --- Val Loss: 0.6702696293279599 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 410/10000 --- Train Loss: 0.6652723148625456 --- Val Loss: 0.668963241479804 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 420/10000 --- Train Loss: 0.6637938006786867 --- Val Loss: 0.6675173620678825 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 430/10000 --- Train Loss: 0.6621655172758721 --- Val Loss: 0.6659141494016876 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 440/10000 --- Train Loss: 0.6603674358687827 --- Val Loss: 0.6641322196640778 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 450/10000 --- Train Loss: 0.6583777330889184 --- Val Loss: 0.662148916091931 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 460/10000 --- Train Loss: 0.6561748894592323 --- Val Loss: 0.6599436356852586 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 470/10000 --- Train Loss: 0.6537368893623985 --- Val Loss: 0.6574905958989639 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 480/10000 --- Train Loss: 0.6510305549056059 --- Val Loss: 0.6547556510199732 --- Train Acc: 0.62 --- Val Acc: 0.59\n",
      "Epoch 490/10000 --- Train Loss: 0.6480296235377708 --- Val Loss: 0.6517122133076164 --- Train Acc: 0.62 --- Val Acc: 0.59\n",
      "Epoch 500/10000 --- Train Loss: 0.6447051720395511 --- Val Loss: 0.6483269236698052 --- Train Acc: 0.62 --- Val Acc: 0.60\n",
      "Epoch 510/10000 --- Train Loss: 0.6410222578890421 --- Val Loss: 0.6445665317180956 --- Train Acc: 0.62 --- Val Acc: 0.60\n",
      "Epoch 520/10000 --- Train Loss: 0.6369438581131108 --- Val Loss: 0.6403894862508009 --- Train Acc: 0.62 --- Val Acc: 0.62\n",
      "Epoch 530/10000 --- Train Loss: 0.6324450462490498 --- Val Loss: 0.6357718350950416 --- Train Acc: 0.63 --- Val Acc: 0.64\n",
      "Epoch 540/10000 --- Train Loss: 0.6274865742783795 --- Val Loss: 0.630672355388156 --- Train Acc: 0.65 --- Val Acc: 0.66\n",
      "Epoch 550/10000 --- Train Loss: 0.6220318702855832 --- Val Loss: 0.6250548888448594 --- Train Acc: 0.65 --- Val Acc: 0.66\n",
      "Epoch 560/10000 --- Train Loss: 0.6160426647959847 --- Val Loss: 0.6188771917020939 --- Train Acc: 0.68 --- Val Acc: 0.68\n",
      "Epoch 570/10000 --- Train Loss: 0.6095033782629296 --- Val Loss: 0.6121255581362912 --- Train Acc: 0.70 --- Val Acc: 0.70\n",
      "Epoch 580/10000 --- Train Loss: 0.6023908188726548 --- Val Loss: 0.604777566114101 --- Train Acc: 0.73 --- Val Acc: 0.71\n",
      "Epoch 590/10000 --- Train Loss: 0.5947089862606934 --- Val Loss: 0.5968389865567643 --- Train Acc: 0.77 --- Val Acc: 0.77\n",
      "Epoch 600/10000 --- Train Loss: 0.5864381875428144 --- Val Loss: 0.5882955365113646 --- Train Acc: 0.80 --- Val Acc: 0.80\n",
      "Epoch 610/10000 --- Train Loss: 0.5775844137645773 --- Val Loss: 0.5791565578184774 --- Train Acc: 0.83 --- Val Acc: 0.85\n",
      "Epoch 620/10000 --- Train Loss: 0.5681906323540661 --- Val Loss: 0.5694683630360214 --- Train Acc: 0.86 --- Val Acc: 0.86\n",
      "Epoch 630/10000 --- Train Loss: 0.5582768817399348 --- Val Loss: 0.5592594369665679 --- Train Acc: 0.87 --- Val Acc: 0.87\n",
      "Epoch 640/10000 --- Train Loss: 0.5479251760470838 --- Val Loss: 0.5486244014891942 --- Train Acc: 0.88 --- Val Acc: 0.88\n",
      "Epoch 650/10000 --- Train Loss: 0.5371625506194466 --- Val Loss: 0.5375882936476546 --- Train Acc: 0.89 --- Val Acc: 0.88\n",
      "Epoch 660/10000 --- Train Loss: 0.5260761403189446 --- Val Loss: 0.5262480331659974 --- Train Acc: 0.91 --- Val Acc: 0.90\n",
      "Epoch 670/10000 --- Train Loss: 0.5147442044179333 --- Val Loss: 0.5146883834863184 --- Train Acc: 0.91 --- Val Acc: 0.90\n",
      "Epoch 680/10000 --- Train Loss: 0.503225167130443 --- Val Loss: 0.5029704786680282 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 690/10000 --- Train Loss: 0.4916345671899603 --- Val Loss: 0.49121381364385913 --- Train Acc: 0.94 --- Val Acc: 0.91\n",
      "Epoch 700/10000 --- Train Loss: 0.48002711041511464 --- Val Loss: 0.4794744783586098 --- Train Acc: 0.93 --- Val Acc: 0.91\n",
      "Epoch 710/10000 --- Train Loss: 0.4684791321987904 --- Val Loss: 0.4678234912459199 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 720/10000 --- Train Loss: 0.457044833450813 --- Val Loss: 0.4563166276766307 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 730/10000 --- Train Loss: 0.4457844666511408 --- Val Loss: 0.4450074405243437 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 740/10000 --- Train Loss: 0.4347498693961099 --- Val Loss: 0.4339448328485549 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 750/10000 --- Train Loss: 0.4239609969479789 --- Val Loss: 0.4231448389418731 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 760/10000 --- Train Loss: 0.4134463204187532 --- Val Loss: 0.4126298314605294 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 770/10000 --- Train Loss: 0.403231164550633 --- Val Loss: 0.40242153917101225 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 780/10000 --- Train Loss: 0.393348543911302 --- Val Loss: 0.3925514937380451 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 790/10000 --- Train Loss: 0.38380644582937035 --- Val Loss: 0.38301537607996966 --- Train Acc: 0.95 --- Val Acc: 0.92\n",
      "Epoch 800/10000 --- Train Loss: 0.37459392798638746 --- Val Loss: 0.37381233863634294 --- Train Acc: 0.96 --- Val Acc: 0.93\n",
      "Epoch 810/10000 --- Train Loss: 0.36572203116647384 --- Val Loss: 0.36494102705772763 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 820/10000 --- Train Loss: 0.35718560096447377 --- Val Loss: 0.3563943137316715 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 830/10000 --- Train Loss: 0.34898993862181527 --- Val Loss: 0.34817704158635365 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 840/10000 --- Train Loss: 0.3411136035868407 --- Val Loss: 0.34026951805232103 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 850/10000 --- Train Loss: 0.33355368952241576 --- Val Loss: 0.33266812637066123 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 860/10000 --- Train Loss: 0.32630835174100714 --- Val Loss: 0.32536410255197706 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 870/10000 --- Train Loss: 0.31936124790495984 --- Val Loss: 0.3183486122283647 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 880/10000 --- Train Loss: 0.31270230041639696 --- Val Loss: 0.3116122328145296 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 890/10000 --- Train Loss: 0.3063179627092336 --- Val Loss: 0.3051386086937269 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 900/10000 --- Train Loss: 0.3002071796884588 --- Val Loss: 0.29893108337688995 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 910/10000 --- Train Loss: 0.2943576986694632 --- Val Loss: 0.2929821392232846 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 920/10000 --- Train Loss: 0.2887517985352063 --- Val Loss: 0.28727012273741165 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 930/10000 --- Train Loss: 0.28337977493743144 --- Val Loss: 0.2817859957403659 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 940/10000 --- Train Loss: 0.27823027979490805 --- Val Loss: 0.27652063823866674 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 950/10000 --- Train Loss: 0.2732908364291452 --- Val Loss: 0.2714646357481657 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 960/10000 --- Train Loss: 0.2685536078591735 --- Val Loss: 0.26660141373252544 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 970/10000 --- Train Loss: 0.26400075748963336 --- Val Loss: 0.2619196374597259 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 980/10000 --- Train Loss: 0.2596348476414718 --- Val Loss: 0.2574355074918565 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 990/10000 --- Train Loss: 0.25544544184007356 --- Val Loss: 0.2531425766170866 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1000/10000 --- Train Loss: 0.25142638834025954 --- Val Loss: 0.24904921947971734 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1010/10000 --- Train Loss: 0.24756748292408787 --- Val Loss: 0.245125386961761 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1020/10000 --- Train Loss: 0.2438615442380205 --- Val Loss: 0.24137142644336285 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1030/10000 --- Train Loss: 0.24029288206398933 --- Val Loss: 0.23776068087064867 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1040/10000 --- Train Loss: 0.2368560412552431 --- Val Loss: 0.23428951825840763 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1050/10000 --- Train Loss: 0.23354776546389766 --- Val Loss: 0.23094880909169754 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1060/10000 --- Train Loss: 0.23035560385298426 --- Val Loss: 0.22772523342258721 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1070/10000 --- Train Loss: 0.22727662011854935 --- Val Loss: 0.2246163160978093 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1080/10000 --- Train Loss: 0.22429974183446028 --- Val Loss: 0.22161076039459926 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1090/10000 --- Train Loss: 0.22141551539115376 --- Val Loss: 0.21869782492651246 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 1100/10000 --- Train Loss: 0.21863074975003535 --- Val Loss: 0.21588262670613687 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1110/10000 --- Train Loss: 0.21593505107925207 --- Val Loss: 0.2131586464018154 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1120/10000 --- Train Loss: 0.21332342053815803 --- Val Loss: 0.2105179056902677 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1130/10000 --- Train Loss: 0.21079600064755166 --- Val Loss: 0.20796068108157634 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1140/10000 --- Train Loss: 0.20834714015907918 --- Val Loss: 0.20548165892581552 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1150/10000 --- Train Loss: 0.20597296136045798 --- Val Loss: 0.20307807252926188 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1160/10000 --- Train Loss: 0.20366892943229756 --- Val Loss: 0.20074289843631196 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1170/10000 --- Train Loss: 0.2014269620009699 --- Val Loss: 0.19847026221655129 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1180/10000 --- Train Loss: 0.19925070456114727 --- Val Loss: 0.1962640876876625 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1190/10000 --- Train Loss: 0.1971297904343059 --- Val Loss: 0.19411624751895004 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1200/10000 --- Train Loss: 0.1950699037200291 --- Val Loss: 0.1920298444707948 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 1210/10000 --- Train Loss: 0.19306683838433217 --- Val Loss: 0.1900023463281186 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1220/10000 --- Train Loss: 0.19111363382335464 --- Val Loss: 0.18802829204829943 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1230/10000 --- Train Loss: 0.18921147574892525 --- Val Loss: 0.18610575701161464 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1240/10000 --- Train Loss: 0.18735180166608204 --- Val Loss: 0.18422668183992905 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1250/10000 --- Train Loss: 0.18553658041041177 --- Val Loss: 0.18238804267562392 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1260/10000 --- Train Loss: 0.1837666674436588 --- Val Loss: 0.1805926107477555 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1270/10000 --- Train Loss: 0.18203458150015986 --- Val Loss: 0.1788343886903247 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1280/10000 --- Train Loss: 0.18034112902350166 --- Val Loss: 0.17711359959727513 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1290/10000 --- Train Loss: 0.17868481982551132 --- Val Loss: 0.1754290566297314 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1300/10000 --- Train Loss: 0.17706626658566507 --- Val Loss: 0.17378333683956101 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1310/10000 --- Train Loss: 0.1754863153851759 --- Val Loss: 0.1721783395978008 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1320/10000 --- Train Loss: 0.17394215360439386 --- Val Loss: 0.17060768430534118 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1330/10000 --- Train Loss: 0.17243229183142458 --- Val Loss: 0.1690694659935865 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1340/10000 --- Train Loss: 0.17095196118794045 --- Val Loss: 0.16756020627703275 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1350/10000 --- Train Loss: 0.1695111440911174 --- Val Loss: 0.16609122150081515 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1360/10000 --- Train Loss: 0.16809788518835755 --- Val Loss: 0.1646514174396347 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1370/10000 --- Train Loss: 0.16671526943614332 --- Val Loss: 0.16324345456942874 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1380/10000 --- Train Loss: 0.16536120797332152 --- Val Loss: 0.16186507421081736 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1390/10000 --- Train Loss: 0.1640341668282621 --- Val Loss: 0.16051518877438883 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1400/10000 --- Train Loss: 0.1627320232253721 --- Val Loss: 0.15919065293516838 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1410/10000 --- Train Loss: 0.1614536398094992 --- Val Loss: 0.1578896892247718 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1420/10000 --- Train Loss: 0.160198778447297 --- Val Loss: 0.156615599850314 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1430/10000 --- Train Loss: 0.15897001745247455 --- Val Loss: 0.15536043559411594 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1440/10000 --- Train Loss: 0.1577657333852067 --- Val Loss: 0.1541330501862161 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1450/10000 --- Train Loss: 0.15658595999189792 --- Val Loss: 0.15292834473996922 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1460/10000 --- Train Loss: 0.15542906187665187 --- Val Loss: 0.1517506610105368 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1470/10000 --- Train Loss: 0.1542962721369249 --- Val Loss: 0.15059783349415523 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1480/10000 --- Train Loss: 0.1531817746910161 --- Val Loss: 0.14946613613602616 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1490/10000 --- Train Loss: 0.1520881556629 --- Val Loss: 0.14835592402996264 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1500/10000 --- Train Loss: 0.1510159557712471 --- Val Loss: 0.1472650328396091 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1510/10000 --- Train Loss: 0.14996290542388205 --- Val Loss: 0.14619882064356207 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1520/10000 --- Train Loss: 0.14892743875085396 --- Val Loss: 0.14514872174863336 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1530/10000 --- Train Loss: 0.1479090068751605 --- Val Loss: 0.14411444171859564 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1540/10000 --- Train Loss: 0.14690597803419309 --- Val Loss: 0.14309899975514043 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1550/10000 --- Train Loss: 0.14592159388308837 --- Val Loss: 0.14210373822923758 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1560/10000 --- Train Loss: 0.14495146521243019 --- Val Loss: 0.14112114702266554 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1570/10000 --- Train Loss: 0.1439980081916683 --- Val Loss: 0.14015361628363737 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1580/10000 --- Train Loss: 0.14306208597453846 --- Val Loss: 0.13920638522313222 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1590/10000 --- Train Loss: 0.14214303741961218 --- Val Loss: 0.13827524233408667 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1600/10000 --- Train Loss: 0.141238130992301 --- Val Loss: 0.1373578405982903 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1610/10000 --- Train Loss: 0.14034742979740195 --- Val Loss: 0.13645278776068026 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1620/10000 --- Train Loss: 0.1394727793904735 --- Val Loss: 0.1355621819907193 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1630/10000 --- Train Loss: 0.13861339755996138 --- Val Loss: 0.1346920539794612 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1640/10000 --- Train Loss: 0.13776670984094533 --- Val Loss: 0.13383412050914978 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 1650/10000 --- Train Loss: 0.13693407872490262 --- Val Loss: 0.1329918707994894 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1660/10000 --- Train Loss: 0.13611112979620094 --- Val Loss: 0.13215922619434867 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1670/10000 --- Train Loss: 0.1353010887091597 --- Val Loss: 0.13133839628916505 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1680/10000 --- Train Loss: 0.1345037087438321 --- Val Loss: 0.130528985009348 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1690/10000 --- Train Loss: 0.13371623182852987 --- Val Loss: 0.12972930764782853 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1700/10000 --- Train Loss: 0.13294114585451564 --- Val Loss: 0.1289446838613241 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1710/10000 --- Train Loss: 0.13217606153994618 --- Val Loss: 0.12817143629696742 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1720/10000 --- Train Loss: 0.13142089976983853 --- Val Loss: 0.12740639047707947 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1730/10000 --- Train Loss: 0.1306753304544064 --- Val Loss: 0.12665107607367854 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1740/10000 --- Train Loss: 0.1299416995199241 --- Val Loss: 0.12590928251086536 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1750/10000 --- Train Loss: 0.1292188515288707 --- Val Loss: 0.12517897685643758 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1760/10000 --- Train Loss: 0.12850675439346254 --- Val Loss: 0.12446141537033112 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1770/10000 --- Train Loss: 0.12780559500792893 --- Val Loss: 0.1237559370521494 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1780/10000 --- Train Loss: 0.1271126556015732 --- Val Loss: 0.12306100785099319 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1790/10000 --- Train Loss: 0.1264277961946605 --- Val Loss: 0.1223723271533393 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1800/10000 --- Train Loss: 0.12575138753025167 --- Val Loss: 0.12169151014015593 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1810/10000 --- Train Loss: 0.12508234117187747 --- Val Loss: 0.1210172783361202 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1820/10000 --- Train Loss: 0.12442055536416063 --- Val Loss: 0.12035023099124552 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1830/10000 --- Train Loss: 0.12376560220805313 --- Val Loss: 0.11968811843694127 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1840/10000 --- Train Loss: 0.1231206150759256 --- Val Loss: 0.11903744755334812 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1850/10000 --- Train Loss: 0.12248374517593345 --- Val Loss: 0.11839699949851566 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1860/10000 --- Train Loss: 0.1218552078684773 --- Val Loss: 0.11776456875056239 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1870/10000 --- Train Loss: 0.12123426507122155 --- Val Loss: 0.11713892237440232 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1880/10000 --- Train Loss: 0.12062085861018672 --- Val Loss: 0.1165240632958919 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1890/10000 --- Train Loss: 0.12001491768655188 --- Val Loss: 0.1159146859230048 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1900/10000 --- Train Loss: 0.11941900492523844 --- Val Loss: 0.1153161940061179 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1910/10000 --- Train Loss: 0.11883138853149579 --- Val Loss: 0.11472396330488135 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1920/10000 --- Train Loss: 0.11825098243706196 --- Val Loss: 0.11414068372915813 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1930/10000 --- Train Loss: 0.11767906969224491 --- Val Loss: 0.11356832680781233 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1940/10000 --- Train Loss: 0.11711573025777859 --- Val Loss: 0.11300102867528287 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1950/10000 --- Train Loss: 0.1165608352109492 --- Val Loss: 0.11244463686629767 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1960/10000 --- Train Loss: 0.11601202609535516 --- Val Loss: 0.1118916061861789 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1970/10000 --- Train Loss: 0.11547129194541379 --- Val Loss: 0.1113482412525488 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1980/10000 --- Train Loss: 0.11493710438726466 --- Val Loss: 0.11081078517811523 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 1990/10000 --- Train Loss: 0.1144096920149948 --- Val Loss: 0.11027905389055741 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2000/10000 --- Train Loss: 0.11388866955522021 --- Val Loss: 0.10975623731429264 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2010/10000 --- Train Loss: 0.11337393050362572 --- Val Loss: 0.10923993697453752 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2020/10000 --- Train Loss: 0.11286510088629137 --- Val Loss: 0.10872897413922898 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2030/10000 --- Train Loss: 0.11236389263156668 --- Val Loss: 0.1082258994401593 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2040/10000 --- Train Loss: 0.11186885762254137 --- Val Loss: 0.10772963469998532 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2050/10000 --- Train Loss: 0.11138094616421444 --- Val Loss: 0.10723818685129588 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2060/10000 --- Train Loss: 0.11089992981693328 --- Val Loss: 0.10675213480470488 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2070/10000 --- Train Loss: 0.11042570741611617 --- Val Loss: 0.10627557833971153 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2080/10000 --- Train Loss: 0.10995640890037095 --- Val Loss: 0.10580264047454946 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2090/10000 --- Train Loss: 0.10949365858824242 --- Val Loss: 0.10533491426591748 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2100/10000 --- Train Loss: 0.10903593355891186 --- Val Loss: 0.10487188679271864 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2110/10000 --- Train Loss: 0.10858403593841125 --- Val Loss: 0.10441464696394022 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2120/10000 --- Train Loss: 0.10813612488855644 --- Val Loss: 0.10396057709853733 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2130/10000 --- Train Loss: 0.10769549663605825 --- Val Loss: 0.10351491695573965 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2140/10000 --- Train Loss: 0.10726093357108345 --- Val Loss: 0.10307427780350577 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2150/10000 --- Train Loss: 0.10683301185140352 --- Val Loss: 0.10263871036949422 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2160/10000 --- Train Loss: 0.10640814010394711 --- Val Loss: 0.10220942701148425 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2170/10000 --- Train Loss: 0.10598899362519272 --- Val Loss: 0.1017816152906054 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2180/10000 --- Train Loss: 0.10557524976173636 --- Val Loss: 0.10135895993853195 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2190/10000 --- Train Loss: 0.10516559104716429 --- Val Loss: 0.1009446090890067 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2200/10000 --- Train Loss: 0.1047624216130777 --- Val Loss: 0.10053434278805286 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2210/10000 --- Train Loss: 0.10436308022153656 --- Val Loss: 0.10012641315179927 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2220/10000 --- Train Loss: 0.10396824433435548 --- Val Loss: 0.09972255496908897 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2230/10000 --- Train Loss: 0.10357896773670497 --- Val Loss: 0.09932694520641694 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2240/10000 --- Train Loss: 0.10319513517828308 --- Val Loss: 0.09893460330052266 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2250/10000 --- Train Loss: 0.10281528199488034 --- Val Loss: 0.09854407628236658 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2260/10000 --- Train Loss: 0.10244052499153802 --- Val Loss: 0.09815912306424364 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2270/10000 --- Train Loss: 0.10206969782962619 --- Val Loss: 0.09778012709302257 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2280/10000 --- Train Loss: 0.10170419857026695 --- Val Loss: 0.09740430644661793 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2290/10000 --- Train Loss: 0.10134331452298012 --- Val Loss: 0.0970336758445863 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2300/10000 --- Train Loss: 0.10098503818080116 --- Val Loss: 0.09666752800606032 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2310/10000 --- Train Loss: 0.10062999609890826 --- Val Loss: 0.0963014871424536 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2320/10000 --- Train Loss: 0.10027932516393107 --- Val Loss: 0.09593979258667384 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2330/10000 --- Train Loss: 0.0999324216050013 --- Val Loss: 0.09558306912628634 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2340/10000 --- Train Loss: 0.0995900759996644 --- Val Loss: 0.0952322962287692 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 2350/10000 --- Train Loss: 0.09924949627691566 --- Val Loss: 0.09488239728755242 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2360/10000 --- Train Loss: 0.09891263603025122 --- Val Loss: 0.09453737755005738 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2370/10000 --- Train Loss: 0.09857953229490528 --- Val Loss: 0.0941960390808321 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2380/10000 --- Train Loss: 0.09824933574382932 --- Val Loss: 0.09385805421222468 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2390/10000 --- Train Loss: 0.09792179720517265 --- Val Loss: 0.09352147087935538 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2400/10000 --- Train Loss: 0.09759818931087426 --- Val Loss: 0.09318778191281299 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2410/10000 --- Train Loss: 0.09727873602404395 --- Val Loss: 0.09285676095384496 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2420/10000 --- Train Loss: 0.09696296940574714 --- Val Loss: 0.0925328549474766 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2430/10000 --- Train Loss: 0.09664854798810378 --- Val Loss: 0.09220548094557438 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2440/10000 --- Train Loss: 0.0963382027577668 --- Val Loss: 0.09188432659373287 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2450/10000 --- Train Loss: 0.09602970441573964 --- Val Loss: 0.09156353886649685 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2460/10000 --- Train Loss: 0.0957254493726084 --- Val Loss: 0.0912487476503701 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2470/10000 --- Train Loss: 0.0954240631021873 --- Val Loss: 0.09093652221710431 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2480/10000 --- Train Loss: 0.09512487812046323 --- Val Loss: 0.09062696188392882 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2490/10000 --- Train Loss: 0.0948298973625903 --- Val Loss: 0.09032109496161576 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2500/10000 --- Train Loss: 0.09453609640133266 --- Val Loss: 0.09001695942162705 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2510/10000 --- Train Loss: 0.09424496051590418 --- Val Loss: 0.08971417979821694 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 2520/10000 --- Train Loss: 0.09395555703895793 --- Val Loss: 0.08941384725445309 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2530/10000 --- Train Loss: 0.09367050011425396 --- Val Loss: 0.08911765195515767 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2540/10000 --- Train Loss: 0.09338684304737781 --- Val Loss: 0.08882232282427989 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2550/10000 --- Train Loss: 0.09310607958593602 --- Val Loss: 0.08853189562241019 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2560/10000 --- Train Loss: 0.09282632625460703 --- Val Loss: 0.08824158091753018 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2570/10000 --- Train Loss: 0.09255000055354698 --- Val Loss: 0.08795348665722891 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2580/10000 --- Train Loss: 0.0922755375091534 --- Val Loss: 0.08766672014883108 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2590/10000 --- Train Loss: 0.09200390868660956 --- Val Loss: 0.08738238214698277 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2600/10000 --- Train Loss: 0.09173482879375922 --- Val Loss: 0.08710085168328459 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2610/10000 --- Train Loss: 0.0914686037813502 --- Val Loss: 0.08682490933368124 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2620/10000 --- Train Loss: 0.09120465521804559 --- Val Loss: 0.08655113391618119 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2630/10000 --- Train Loss: 0.09094262607077512 --- Val Loss: 0.08627788527645561 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2640/10000 --- Train Loss: 0.09068334132338118 --- Val Loss: 0.08600580894476506 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2650/10000 --- Train Loss: 0.09042541024949845 --- Val Loss: 0.08573652383453713 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2660/10000 --- Train Loss: 0.09017013397192002 --- Val Loss: 0.08546958915064941 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2670/10000 --- Train Loss: 0.08991675281770062 --- Val Loss: 0.08520708401073361 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2680/10000 --- Train Loss: 0.08966652562468187 --- Val Loss: 0.08494697636051446 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2690/10000 --- Train Loss: 0.08941854651302464 --- Val Loss: 0.0846878465400864 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2700/10000 --- Train Loss: 0.08917338523837928 --- Val Loss: 0.0844313308861459 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2710/10000 --- Train Loss: 0.08892958833402276 --- Val Loss: 0.08417561331342818 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2720/10000 --- Train Loss: 0.08868661210111269 --- Val Loss: 0.08392069878458719 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2730/10000 --- Train Loss: 0.08844664738453578 --- Val Loss: 0.08366829831621188 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2740/10000 --- Train Loss: 0.08820830927034294 --- Val Loss: 0.08341714348493431 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2750/10000 --- Train Loss: 0.08797199809501173 --- Val Loss: 0.08316976969296162 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2760/10000 --- Train Loss: 0.0877367728720755 --- Val Loss: 0.08292395708931151 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2770/10000 --- Train Loss: 0.08750428482676591 --- Val Loss: 0.08267901922315735 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2780/10000 --- Train Loss: 0.08727440380351677 --- Val Loss: 0.08243675827312187 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2790/10000 --- Train Loss: 0.08704650999991313 --- Val Loss: 0.08219748699811243 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2800/10000 --- Train Loss: 0.08681871006169113 --- Val Loss: 0.08195612142646819 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2810/10000 --- Train Loss: 0.0865929019495387 --- Val Loss: 0.08171765319614072 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2820/10000 --- Train Loss: 0.08636957321425666 --- Val Loss: 0.08148227907841832 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2830/10000 --- Train Loss: 0.0861473935953764 --- Val Loss: 0.08124782796061852 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2840/10000 --- Train Loss: 0.08592712676179196 --- Val Loss: 0.08101408187767578 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2850/10000 --- Train Loss: 0.08570980378554112 --- Val Loss: 0.08078548677922859 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2860/10000 --- Train Loss: 0.08549330397244229 --- Val Loss: 0.08055659903570365 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2870/10000 --- Train Loss: 0.08527853868822151 --- Val Loss: 0.08032869431756984 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2880/10000 --- Train Loss: 0.08506559002236368 --- Val Loss: 0.08010145861384073 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2890/10000 --- Train Loss: 0.08485460291275984 --- Val Loss: 0.07987695588068551 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2900/10000 --- Train Loss: 0.08464472498456958 --- Val Loss: 0.07965460226001429 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2910/10000 --- Train Loss: 0.08443630746022787 --- Val Loss: 0.0794339141016876 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2920/10000 --- Train Loss: 0.08422981958012311 --- Val Loss: 0.07921403644716192 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2930/10000 --- Train Loss: 0.08402501739924564 --- Val Loss: 0.07899690716995593 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2940/10000 --- Train Loss: 0.0838219009503868 --- Val Loss: 0.07878094006337653 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2950/10000 --- Train Loss: 0.08361923168360828 --- Val Loss: 0.07856418506043404 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2960/10000 --- Train Loss: 0.08341951360315912 --- Val Loss: 0.0783513696541444 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2970/10000 --- Train Loss: 0.08322122769638657 --- Val Loss: 0.0781397784562505 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2980/10000 --- Train Loss: 0.08302441603948349 --- Val Loss: 0.07792891151288783 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 2990/10000 --- Train Loss: 0.08282879438042358 --- Val Loss: 0.07771963512468198 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3000/10000 --- Train Loss: 0.08263615076998043 --- Val Loss: 0.07751457180047511 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3010/10000 --- Train Loss: 0.08244341143305095 --- Val Loss: 0.07730941660903128 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3020/10000 --- Train Loss: 0.0822518132672237 --- Val Loss: 0.07710528990620658 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3030/10000 --- Train Loss: 0.08206188870126352 --- Val Loss: 0.07690153774198036 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3040/10000 --- Train Loss: 0.0818736429461083 --- Val Loss: 0.07670019399825717 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3050/10000 --- Train Loss: 0.08168706106706221 --- Val Loss: 0.07650146176016277 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3060/10000 --- Train Loss: 0.08150180867399233 --- Val Loss: 0.07630269623080292 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3070/10000 --- Train Loss: 0.08131678372758176 --- Val Loss: 0.07610446282088253 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3080/10000 --- Train Loss: 0.081133779195468 --- Val Loss: 0.07590851257369474 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3090/10000 --- Train Loss: 0.08095134217601807 --- Val Loss: 0.0757131666917184 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3100/10000 --- Train Loss: 0.08077014302661921 --- Val Loss: 0.07551890622176861 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3110/10000 --- Train Loss: 0.08059139352165984 --- Val Loss: 0.0753279706603778 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3120/10000 --- Train Loss: 0.08041332913873926 --- Val Loss: 0.075136143341642 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3130/10000 --- Train Loss: 0.08023716344403113 --- Val Loss: 0.0749464942065891 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3140/10000 --- Train Loss: 0.0800623023105596 --- Val Loss: 0.07476002464156628 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3150/10000 --- Train Loss: 0.07988872447193981 --- Val Loss: 0.07457434837240393 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3160/10000 --- Train Loss: 0.07971515889332022 --- Val Loss: 0.07438798017233933 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3170/10000 --- Train Loss: 0.07954295366954205 --- Val Loss: 0.0742034419175685 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3180/10000 --- Train Loss: 0.07937224443838045 --- Val Loss: 0.07401988903249622 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3190/10000 --- Train Loss: 0.07920281433677034 --- Val Loss: 0.07383766432210638 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3200/10000 --- Train Loss: 0.07903413088016169 --- Val Loss: 0.07365646165974565 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3210/10000 --- Train Loss: 0.07886767538822292 --- Val Loss: 0.07347709707039607 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3220/10000 --- Train Loss: 0.07870225140278718 --- Val Loss: 0.07329768847942637 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3230/10000 --- Train Loss: 0.07853813782661523 --- Val Loss: 0.0731198258558919 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3240/10000 --- Train Loss: 0.07837503052743094 --- Val Loss: 0.07294306774699444 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3250/10000 --- Train Loss: 0.07821302590180255 --- Val Loss: 0.07276920258882347 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3260/10000 --- Train Loss: 0.07805183373524542 --- Val Loss: 0.07259497443341746 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3270/10000 --- Train Loss: 0.07789191612346906 --- Val Loss: 0.07242160841682044 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3280/10000 --- Train Loss: 0.0777334537036483 --- Val Loss: 0.07225159688419662 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3290/10000 --- Train Loss: 0.07757636269102512 --- Val Loss: 0.07208206807036488 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3300/10000 --- Train Loss: 0.07741919542660133 --- Val Loss: 0.07191037625809556 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3310/10000 --- Train Loss: 0.07726453137397601 --- Val Loss: 0.07174402065551296 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3320/10000 --- Train Loss: 0.07710995276807536 --- Val Loss: 0.0715751281970031 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3330/10000 --- Train Loss: 0.07695654595041874 --- Val Loss: 0.07140888548274085 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3340/10000 --- Train Loss: 0.07680368232367221 --- Val Loss: 0.07124312814061222 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3350/10000 --- Train Loss: 0.07665201663054731 --- Val Loss: 0.07107936139534084 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3360/10000 --- Train Loss: 0.07650295982371358 --- Val Loss: 0.07091938597668217 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3370/10000 --- Train Loss: 0.07635410238400331 --- Val Loss: 0.07075911379963265 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3380/10000 --- Train Loss: 0.07620640571159262 --- Val Loss: 0.07059909718366504 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3390/10000 --- Train Loss: 0.0760597528458264 --- Val Loss: 0.07044100953125251 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3400/10000 --- Train Loss: 0.07591381307953735 --- Val Loss: 0.07028272880206057 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3410/10000 --- Train Loss: 0.07576716744299823 --- Val Loss: 0.07011949214965606 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3420/10000 --- Train Loss: 0.07562404575638197 --- Val Loss: 0.06996516490939667 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3430/10000 --- Train Loss: 0.0754801663796294 --- Val Loss: 0.0698083689836943 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3440/10000 --- Train Loss: 0.07533766557626302 --- Val Loss: 0.06965286226522395 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3450/10000 --- Train Loss: 0.07519612500671437 --- Val Loss: 0.06949932821908751 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3460/10000 --- Train Loss: 0.07505523279702936 --- Val Loss: 0.06934746485096145 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3470/10000 --- Train Loss: 0.07491526141502375 --- Val Loss: 0.06919657507355101 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3480/10000 --- Train Loss: 0.07477602259508347 --- Val Loss: 0.0690450652672401 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3490/10000 --- Train Loss: 0.07463786632088344 --- Val Loss: 0.06889663160538806 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3500/10000 --- Train Loss: 0.07450109876662268 --- Val Loss: 0.06874916545623405 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3510/10000 --- Train Loss: 0.0743651457107096 --- Val Loss: 0.06860231620527561 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3520/10000 --- Train Loss: 0.07422934944065641 --- Val Loss: 0.06845438249735013 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3530/10000 --- Train Loss: 0.07409467144415093 --- Val Loss: 0.06830668151306396 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3540/10000 --- Train Loss: 0.07396085355583945 --- Val Loss: 0.06815948387731585 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3550/10000 --- Train Loss: 0.07382854624320573 --- Val Loss: 0.06801625493072444 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3560/10000 --- Train Loss: 0.07369616851060001 --- Val Loss: 0.06787206692701449 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3570/10000 --- Train Loss: 0.07356498749157937 --- Val Loss: 0.06772956586323219 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3580/10000 --- Train Loss: 0.07343512627501649 --- Val Loss: 0.06758698144559797 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3590/10000 --- Train Loss: 0.07330563432274212 --- Val Loss: 0.06744848792714232 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3600/10000 --- Train Loss: 0.07317698373221856 --- Val Loss: 0.06730903959429181 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3610/10000 --- Train Loss: 0.07304855790629293 --- Val Loss: 0.06717066411753576 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3620/10000 --- Train Loss: 0.07292112799077463 --- Val Loss: 0.06703069438678129 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3630/10000 --- Train Loss: 0.07279439149288326 --- Val Loss: 0.0668916391908019 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3640/10000 --- Train Loss: 0.07266808361681003 --- Val Loss: 0.06675502356031657 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3650/10000 --- Train Loss: 0.07254283501991377 --- Val Loss: 0.06661798774178011 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3660/10000 --- Train Loss: 0.07241796764303533 --- Val Loss: 0.06648231448154715 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3670/10000 --- Train Loss: 0.07229508224565365 --- Val Loss: 0.0663483151643159 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3680/10000 --- Train Loss: 0.07217246826729172 --- Val Loss: 0.06621527433863225 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3690/10000 --- Train Loss: 0.0720506197413189 --- Val Loss: 0.06608328995010124 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3700/10000 --- Train Loss: 0.07192936128747404 --- Val Loss: 0.06595210374132582 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3710/10000 --- Train Loss: 0.07180938441894057 --- Val Loss: 0.06582192677359067 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3720/10000 --- Train Loss: 0.07168952073800096 --- Val Loss: 0.06569284552580149 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3730/10000 --- Train Loss: 0.07157011091610267 --- Val Loss: 0.06556280396461103 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3740/10000 --- Train Loss: 0.07145142750728747 --- Val Loss: 0.06543360751579083 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3750/10000 --- Train Loss: 0.0713334177509928 --- Val Loss: 0.06530602855590709 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3760/10000 --- Train Loss: 0.07121647115201966 --- Val Loss: 0.0651794585128422 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3770/10000 --- Train Loss: 0.07110079116805561 --- Val Loss: 0.06505327999149929 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3780/10000 --- Train Loss: 0.07098487776586392 --- Val Loss: 0.06492856953985664 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3790/10000 --- Train Loss: 0.07086998891741209 --- Val Loss: 0.06480482903887032 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3800/10000 --- Train Loss: 0.07075595124781092 --- Val Loss: 0.06468171408762013 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3810/10000 --- Train Loss: 0.07064158316918763 --- Val Loss: 0.0645575104609827 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3820/10000 --- Train Loss: 0.07052821177365504 --- Val Loss: 0.06443594129022047 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3830/10000 --- Train Loss: 0.07041598371183554 --- Val Loss: 0.06431485631481092 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3840/10000 --- Train Loss: 0.07030369745425667 --- Val Loss: 0.06419177178853964 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3850/10000 --- Train Loss: 0.0701924504207058 --- Val Loss: 0.06407096865017047 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3860/10000 --- Train Loss: 0.07008229309108358 --- Val Loss: 0.06395309096933573 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3870/10000 --- Train Loss: 0.06997243191486342 --- Val Loss: 0.0638359698475313 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3880/10000 --- Train Loss: 0.06986272307756553 --- Val Loss: 0.06371651007268522 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3890/10000 --- Train Loss: 0.06975445526524744 --- Val Loss: 0.0636003573195052 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3900/10000 --- Train Loss: 0.06964660663412355 --- Val Loss: 0.06348535971709363 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3910/10000 --- Train Loss: 0.06953904489864181 --- Val Loss: 0.06336936499305593 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3920/10000 --- Train Loss: 0.069431929725868 --- Val Loss: 0.06325348055226751 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3930/10000 --- Train Loss: 0.06932553302649282 --- Val Loss: 0.06313985423595346 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3940/10000 --- Train Loss: 0.0692195955280398 --- Val Loss: 0.06302526348557404 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3950/10000 --- Train Loss: 0.06911420070248753 --- Val Loss: 0.06291192677463903 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3960/10000 --- Train Loss: 0.06900898434790376 --- Val Loss: 0.06279641385042643 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3970/10000 --- Train Loss: 0.06890506385610273 --- Val Loss: 0.06268461814338219 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3980/10000 --- Train Loss: 0.06880185047484633 --- Val Loss: 0.06257435616557505 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 3990/10000 --- Train Loss: 0.06869908460789925 --- Val Loss: 0.06246440965219362 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4000/10000 --- Train Loss: 0.06859608815614522 --- Val Loss: 0.062353111733749335 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4010/10000 --- Train Loss: 0.06849474117476996 --- Val Loss: 0.06224436914666966 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4020/10000 --- Train Loss: 0.06839368343782509 --- Val Loss: 0.062136482885525236 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4030/10000 --- Train Loss: 0.06829342902684693 --- Val Loss: 0.06202892133214964 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4040/10000 --- Train Loss: 0.0681931572827974 --- Val Loss: 0.06192081697940318 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4050/10000 --- Train Loss: 0.06809384402737985 --- Val Loss: 0.06181575217446422 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4060/10000 --- Train Loss: 0.06799476399786597 --- Val Loss: 0.06171027650985091 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4070/10000 --- Train Loss: 0.06789647301776253 --- Val Loss: 0.06160550443523202 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4080/10000 --- Train Loss: 0.06779787559376037 --- Val Loss: 0.06149825470005155 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4090/10000 --- Train Loss: 0.06770003886055333 --- Val Loss: 0.061391627123457415 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4100/10000 --- Train Loss: 0.0676031797935439 --- Val Loss: 0.06128815204841695 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4110/10000 --- Train Loss: 0.06750680761317379 --- Val Loss: 0.06118411366303329 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4120/10000 --- Train Loss: 0.06741075320897734 --- Val Loss: 0.06108180084252115 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4130/10000 --- Train Loss: 0.06731477009043245 --- Val Loss: 0.06097972039329507 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4140/10000 --- Train Loss: 0.06721947969244714 --- Val Loss: 0.060876536851384624 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4150/10000 --- Train Loss: 0.06712495454711519 --- Val Loss: 0.06077488757962343 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4160/10000 --- Train Loss: 0.06703119003187744 --- Val Loss: 0.06067562503121495 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4170/10000 --- Train Loss: 0.06693748308199696 --- Val Loss: 0.060574624947824736 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4180/10000 --- Train Loss: 0.06684479661056417 --- Val Loss: 0.06047584383969358 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4190/10000 --- Train Loss: 0.06675142374181026 --- Val Loss: 0.060375446838424024 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4200/10000 --- Train Loss: 0.06665950602071333 --- Val Loss: 0.06027615475563608 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4210/10000 --- Train Loss: 0.06656783177468002 --- Val Loss: 0.06017838072281652 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4220/10000 --- Train Loss: 0.06647596277595245 --- Val Loss: 0.060079682670861674 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4230/10000 --- Train Loss: 0.06638568949162385 --- Val Loss: 0.059982915432328705 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4240/10000 --- Train Loss: 0.06629487003646929 --- Val Loss: 0.05988607829843056 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4250/10000 --- Train Loss: 0.06620471051809415 --- Val Loss: 0.05978976711458074 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4260/10000 --- Train Loss: 0.06611495181981868 --- Val Loss: 0.05969438769474616 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4270/10000 --- Train Loss: 0.06602555353347594 --- Val Loss: 0.059597963694301245 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4280/10000 --- Train Loss: 0.06593675196805407 --- Val Loss: 0.05950258903944544 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4290/10000 --- Train Loss: 0.06584795511791433 --- Val Loss: 0.059408923364831064 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4300/10000 --- Train Loss: 0.06576005994495589 --- Val Loss: 0.05931381633225111 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4310/10000 --- Train Loss: 0.06567318748874389 --- Val Loss: 0.05922209272475922 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4320/10000 --- Train Loss: 0.06558626557663128 --- Val Loss: 0.05912892934824763 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4330/10000 --- Train Loss: 0.06549930875161045 --- Val Loss: 0.05903502346776214 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4340/10000 --- Train Loss: 0.0654131203274347 --- Val Loss: 0.0589424990532181 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4350/10000 --- Train Loss: 0.06532718034683159 --- Val Loss: 0.05885159038075596 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4360/10000 --- Train Loss: 0.06524160435370302 --- Val Loss: 0.05876215517049584 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4370/10000 --- Train Loss: 0.06515680877506748 --- Val Loss: 0.0586707261956783 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4380/10000 --- Train Loss: 0.06507138814503226 --- Val Loss: 0.0585788655762177 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4390/10000 --- Train Loss: 0.06498747739604635 --- Val Loss: 0.05849039454423145 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4400/10000 --- Train Loss: 0.06490336693052906 --- Val Loss: 0.05839998816676132 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4410/10000 --- Train Loss: 0.06481976078402749 --- Val Loss: 0.058310136975718165 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4420/10000 --- Train Loss: 0.06473660467872559 --- Val Loss: 0.0582213537892733 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4430/10000 --- Train Loss: 0.06465391085404786 --- Val Loss: 0.05813346011704571 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4440/10000 --- Train Loss: 0.06457109841147625 --- Val Loss: 0.05804570300435189 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4450/10000 --- Train Loss: 0.06448950329193186 --- Val Loss: 0.05795877688680489 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4460/10000 --- Train Loss: 0.06440724180136005 --- Val Loss: 0.057869612567351444 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4470/10000 --- Train Loss: 0.06432549834060372 --- Val Loss: 0.05778166981852026 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4480/10000 --- Train Loss: 0.06424446620810133 --- Val Loss: 0.05769378318602604 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4490/10000 --- Train Loss: 0.06416388908293835 --- Val Loss: 0.05760755912903707 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4500/10000 --- Train Loss: 0.06408321209784196 --- Val Loss: 0.05752067985217009 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4510/10000 --- Train Loss: 0.06400300722392607 --- Val Loss: 0.05743343493831086 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4520/10000 --- Train Loss: 0.0639238835469463 --- Val Loss: 0.057347975621425365 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4530/10000 --- Train Loss: 0.06384527347202544 --- Val Loss: 0.05726351868230132 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4540/10000 --- Train Loss: 0.06376678730964384 --- Val Loss: 0.05717848329674821 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4550/10000 --- Train Loss: 0.06368896253706077 --- Val Loss: 0.05709580376143496 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4560/10000 --- Train Loss: 0.06361075450808709 --- Val Loss: 0.0570119269702992 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4570/10000 --- Train Loss: 0.06353345225972819 --- Val Loss: 0.05692911735064827 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4580/10000 --- Train Loss: 0.06345637231347608 --- Val Loss: 0.05684717533641247 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4590/10000 --- Train Loss: 0.06337975644058393 --- Val Loss: 0.05676727116910387 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4600/10000 --- Train Loss: 0.06330309870645336 --- Val Loss: 0.05668546801921979 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4610/10000 --- Train Loss: 0.06322746744222227 --- Val Loss: 0.0566045103942116 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4620/10000 --- Train Loss: 0.06315085217233518 --- Val Loss: 0.056521705197695185 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4630/10000 --- Train Loss: 0.06307555867751581 --- Val Loss: 0.0564413209826271 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4640/10000 --- Train Loss: 0.06300049169855584 --- Val Loss: 0.056363438022316595 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4650/10000 --- Train Loss: 0.06292587692450605 --- Val Loss: 0.056283765849465435 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4660/10000 --- Train Loss: 0.06285167337362745 --- Val Loss: 0.05620538859926644 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4670/10000 --- Train Loss: 0.06277748012675269 --- Val Loss: 0.05612693847096457 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4680/10000 --- Train Loss: 0.06270372341715327 --- Val Loss: 0.05605010682833134 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4690/10000 --- Train Loss: 0.06263012354460251 --- Val Loss: 0.05597040812014298 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4700/10000 --- Train Loss: 0.06255724510726156 --- Val Loss: 0.055893450299167556 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4710/10000 --- Train Loss: 0.06248447734929511 --- Val Loss: 0.05581668482968465 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4720/10000 --- Train Loss: 0.06241132911374853 --- Val Loss: 0.05573997478667379 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4730/10000 --- Train Loss: 0.06233875792072966 --- Val Loss: 0.05566257986247675 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4740/10000 --- Train Loss: 0.062266701901909964 --- Val Loss: 0.05558685882145173 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4750/10000 --- Train Loss: 0.0621952852958321 --- Val Loss: 0.0555122120850537 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4760/10000 --- Train Loss: 0.062123254618005164 --- Val Loss: 0.05543529494202083 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4770/10000 --- Train Loss: 0.06205222367766433 --- Val Loss: 0.05536101123680727 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4780/10000 --- Train Loss: 0.06198122996903173 --- Val Loss: 0.05528612360565898 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4790/10000 --- Train Loss: 0.06191155004107902 --- Val Loss: 0.055212912498222744 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4800/10000 --- Train Loss: 0.061841524990900155 --- Val Loss: 0.05513918967537446 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4810/10000 --- Train Loss: 0.06177085652467212 --- Val Loss: 0.055063659420680325 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4820/10000 --- Train Loss: 0.06170092308926966 --- Val Loss: 0.05499070810041067 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4830/10000 --- Train Loss: 0.061631472919250335 --- Val Loss: 0.05491636385660482 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4840/10000 --- Train Loss: 0.0615621622992065 --- Val Loss: 0.054843535998141124 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4850/10000 --- Train Loss: 0.061493312073251906 --- Val Loss: 0.054771722240999965 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4860/10000 --- Train Loss: 0.06142482181416854 --- Val Loss: 0.05469962312524533 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4870/10000 --- Train Loss: 0.06135659016078245 --- Val Loss: 0.05462786030841245 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4880/10000 --- Train Loss: 0.0612882400037554 --- Val Loss: 0.05455448682530581 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4890/10000 --- Train Loss: 0.06122023378740525 --- Val Loss: 0.054483082917361716 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4900/10000 --- Train Loss: 0.061152584099758184 --- Val Loss: 0.05441037793478688 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4910/10000 --- Train Loss: 0.06108595929393368 --- Val Loss: 0.0543402088433855 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4920/10000 --- Train Loss: 0.06101949776450032 --- Val Loss: 0.0542710834068466 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4930/10000 --- Train Loss: 0.06095280151033667 --- Val Loss: 0.05420030491276394 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4940/10000 --- Train Loss: 0.06088632075616364 --- Val Loss: 0.0541307520632487 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4950/10000 --- Train Loss: 0.060820065164691406 --- Val Loss: 0.05406169331138107 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4960/10000 --- Train Loss: 0.060753622756048224 --- Val Loss: 0.05399204419074726 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4970/10000 --- Train Loss: 0.06068776252533854 --- Val Loss: 0.053921564976730035 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4980/10000 --- Train Loss: 0.06062189028003073 --- Val Loss: 0.053851732664451146 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 4990/10000 --- Train Loss: 0.06055712844186518 --- Val Loss: 0.05378245480352977 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5000/10000 --- Train Loss: 0.06049302158915932 --- Val Loss: 0.05371531695966633 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5010/10000 --- Train Loss: 0.06042857727714484 --- Val Loss: 0.05364773635378511 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5020/10000 --- Train Loss: 0.06036440131910746 --- Val Loss: 0.05358138250585269 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5030/10000 --- Train Loss: 0.060300917370163 --- Val Loss: 0.053515549592886326 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5040/10000 --- Train Loss: 0.06023658850832791 --- Val Loss: 0.053447562485927934 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5050/10000 --- Train Loss: 0.06017268480231835 --- Val Loss: 0.053380936314140745 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5060/10000 --- Train Loss: 0.06010974383695382 --- Val Loss: 0.05331555110076463 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5070/10000 --- Train Loss: 0.060046634688285985 --- Val Loss: 0.05324938578497719 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5080/10000 --- Train Loss: 0.059984066381894276 --- Val Loss: 0.05318466807850704 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5090/10000 --- Train Loss: 0.059921354760113295 --- Val Loss: 0.053119020137676776 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5100/10000 --- Train Loss: 0.059859130574123065 --- Val Loss: 0.0530548038913373 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5110/10000 --- Train Loss: 0.059797006421643845 --- Val Loss: 0.052987430927665266 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5120/10000 --- Train Loss: 0.05973498748560067 --- Val Loss: 0.052923344341234496 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5130/10000 --- Train Loss: 0.059673273973164964 --- Val Loss: 0.052858826557041064 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5140/10000 --- Train Loss: 0.05961186627394717 --- Val Loss: 0.05279518058666096 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5150/10000 --- Train Loss: 0.05955037979673195 --- Val Loss: 0.05273079489796521 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5160/10000 --- Train Loss: 0.0594893613320622 --- Val Loss: 0.0526680058229982 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5170/10000 --- Train Loss: 0.059428342117576576 --- Val Loss: 0.05260426098577319 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5180/10000 --- Train Loss: 0.05936772219528198 --- Val Loss: 0.0525406676365841 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5190/10000 --- Train Loss: 0.059307491064671355 --- Val Loss: 0.05247763873804376 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5200/10000 --- Train Loss: 0.059247275083720914 --- Val Loss: 0.052415091285884105 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5210/10000 --- Train Loss: 0.05918748624425915 --- Val Loss: 0.05235297073169073 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5220/10000 --- Train Loss: 0.05912832627922134 --- Val Loss: 0.052291471679588024 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5230/10000 --- Train Loss: 0.059069204244202374 --- Val Loss: 0.052230871472248454 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5240/10000 --- Train Loss: 0.05900983145092795 --- Val Loss: 0.052169607740558455 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5250/10000 --- Train Loss: 0.058951158887691466 --- Val Loss: 0.05210851838107956 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5260/10000 --- Train Loss: 0.05889245385279453 --- Val Loss: 0.05204777484947069 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5270/10000 --- Train Loss: 0.05883388633318835 --- Val Loss: 0.05198697515147231 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5280/10000 --- Train Loss: 0.058775761090212826 --- Val Loss: 0.0519281912247561 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5290/10000 --- Train Loss: 0.05871750425332063 --- Val Loss: 0.051868069589611786 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5300/10000 --- Train Loss: 0.05866032259151212 --- Val Loss: 0.051809950335494695 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5310/10000 --- Train Loss: 0.058602866638584834 --- Val Loss: 0.051750808128888194 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5320/10000 --- Train Loss: 0.05854559907000186 --- Val Loss: 0.051690675125704466 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5330/10000 --- Train Loss: 0.0584886215090629 --- Val Loss: 0.05163209003645921 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5340/10000 --- Train Loss: 0.05843215641589893 --- Val Loss: 0.051573614414005556 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5350/10000 --- Train Loss: 0.058375001498085194 --- Val Loss: 0.05151387844886141 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5360/10000 --- Train Loss: 0.058318784090529575 --- Val Loss: 0.05145687828874071 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5370/10000 --- Train Loss: 0.0582624200554659 --- Val Loss: 0.051397964969039575 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5380/10000 --- Train Loss: 0.058206213551118546 --- Val Loss: 0.05134060389196494 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5390/10000 --- Train Loss: 0.058150495286620044 --- Val Loss: 0.05128391363476238 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5400/10000 --- Train Loss: 0.058094656043004865 --- Val Loss: 0.05122554782951064 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5410/10000 --- Train Loss: 0.05803893012018234 --- Val Loss: 0.05116740164789762 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5420/10000 --- Train Loss: 0.05798377061554233 --- Val Loss: 0.05111026471109869 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5430/10000 --- Train Loss: 0.05792835146142124 --- Val Loss: 0.05105349574349137 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5440/10000 --- Train Loss: 0.057872916769421225 --- Val Loss: 0.05099637352475101 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5450/10000 --- Train Loss: 0.0578181098731519 --- Val Loss: 0.05094003970043058 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5460/10000 --- Train Loss: 0.057763390382073766 --- Val Loss: 0.05088426392663764 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5470/10000 --- Train Loss: 0.05770899872543867 --- Val Loss: 0.05082768828141558 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5480/10000 --- Train Loss: 0.057655132702450825 --- Val Loss: 0.050773309469643806 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5490/10000 --- Train Loss: 0.0576007600284142 --- Val Loss: 0.05071714847683027 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5500/10000 --- Train Loss: 0.05754725503866113 --- Val Loss: 0.05066231255385339 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5510/10000 --- Train Loss: 0.057493817501221574 --- Val Loss: 0.05060684994488069 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5520/10000 --- Train Loss: 0.05744067890070492 --- Val Loss: 0.050553027158600494 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5530/10000 --- Train Loss: 0.057387487612652555 --- Val Loss: 0.050499147934275186 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5540/10000 --- Train Loss: 0.05733452939583172 --- Val Loss: 0.05044663177298367 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5550/10000 --- Train Loss: 0.05728151578085664 --- Val Loss: 0.05039267980731658 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5560/10000 --- Train Loss: 0.05722834066282849 --- Val Loss: 0.0503368303193374 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5570/10000 --- Train Loss: 0.05717578640666778 --- Val Loss: 0.05028106316397758 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5580/10000 --- Train Loss: 0.057123545344931075 --- Val Loss: 0.05022871756666688 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5590/10000 --- Train Loss: 0.057070932407705444 --- Val Loss: 0.05017386924099457 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5600/10000 --- Train Loss: 0.05701938604997015 --- Val Loss: 0.05012193883053791 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5610/10000 --- Train Loss: 0.05696794038218197 --- Val Loss: 0.05007033016635506 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5620/10000 --- Train Loss: 0.0569164775541703 --- Val Loss: 0.0500176462253555 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5630/10000 --- Train Loss: 0.05686506631310719 --- Val Loss: 0.04996471803832002 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5640/10000 --- Train Loss: 0.05681418085039326 --- Val Loss: 0.04991258350283109 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5650/10000 --- Train Loss: 0.05676313565725239 --- Val Loss: 0.04985975025661341 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5660/10000 --- Train Loss: 0.05671272626151519 --- Val Loss: 0.049807933994447134 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5670/10000 --- Train Loss: 0.056662023805173066 --- Val Loss: 0.04975601432818709 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5680/10000 --- Train Loss: 0.05661129088698589 --- Val Loss: 0.04970354238697416 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5690/10000 --- Train Loss: 0.056561381009532744 --- Val Loss: 0.0496532718885894 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5700/10000 --- Train Loss: 0.05651148091246748 --- Val Loss: 0.04960378105969579 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5710/10000 --- Train Loss: 0.05646158821983012 --- Val Loss: 0.04955332732976557 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5720/10000 --- Train Loss: 0.05641196033553188 --- Val Loss: 0.04950179973563267 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5730/10000 --- Train Loss: 0.056362525070203624 --- Val Loss: 0.04945112561913486 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5740/10000 --- Train Loss: 0.056312698173615665 --- Val Loss: 0.04940092973749551 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5750/10000 --- Train Loss: 0.056263181989667524 --- Val Loss: 0.049349857808852156 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5760/10000 --- Train Loss: 0.05621377398625408 --- Val Loss: 0.0493002767335461 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5770/10000 --- Train Loss: 0.05616425673997839 --- Val Loss: 0.04924884179326581 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5780/10000 --- Train Loss: 0.05611499698941088 --- Val Loss: 0.04919810491736315 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5790/10000 --- Train Loss: 0.05606614038637079 --- Val Loss: 0.049147395313823714 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5800/10000 --- Train Loss: 0.05601794230951105 --- Val Loss: 0.049098907767642905 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5810/10000 --- Train Loss: 0.05596920131098104 --- Val Loss: 0.049048525803087516 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5820/10000 --- Train Loss: 0.0559207829680644 --- Val Loss: 0.04899831347570428 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5830/10000 --- Train Loss: 0.05587338442741757 --- Val Loss: 0.04895101053043342 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5840/10000 --- Train Loss: 0.055826119349475574 --- Val Loss: 0.048903978371500546 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5850/10000 --- Train Loss: 0.05577816613313681 --- Val Loss: 0.04885570851407605 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5860/10000 --- Train Loss: 0.05573080455079732 --- Val Loss: 0.04880789614026847 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5870/10000 --- Train Loss: 0.05568354058760318 --- Val Loss: 0.04876001399396357 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5880/10000 --- Train Loss: 0.055636262643776115 --- Val Loss: 0.0487130290054262 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5890/10000 --- Train Loss: 0.05559007142704783 --- Val Loss: 0.04866796901579555 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5900/10000 --- Train Loss: 0.055542572260633596 --- Val Loss: 0.0486192756247559 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5910/10000 --- Train Loss: 0.055495783571410724 --- Val Loss: 0.04857297348077907 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5920/10000 --- Train Loss: 0.055448721505182125 --- Val Loss: 0.04852524530860142 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5930/10000 --- Train Loss: 0.05540196075856497 --- Val Loss: 0.04847710879630305 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5940/10000 --- Train Loss: 0.05535573764622887 --- Val Loss: 0.0484303085690803 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5950/10000 --- Train Loss: 0.055309770575324664 --- Val Loss: 0.048385116882425534 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5960/10000 --- Train Loss: 0.05526411752567581 --- Val Loss: 0.04834002126231845 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5970/10000 --- Train Loss: 0.055217988208526574 --- Val Loss: 0.04829274513493326 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5980/10000 --- Train Loss: 0.055171994406071165 --- Val Loss: 0.048246473745055515 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 5990/10000 --- Train Loss: 0.055125961456904196 --- Val Loss: 0.04819906253576908 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6000/10000 --- Train Loss: 0.055080791383918776 --- Val Loss: 0.048153230000578855 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6010/10000 --- Train Loss: 0.05503547718407013 --- Val Loss: 0.04810793084169599 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6020/10000 --- Train Loss: 0.05499066735451825 --- Val Loss: 0.04806303967144395 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6030/10000 --- Train Loss: 0.05494544322248535 --- Val Loss: 0.048018120918380876 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6040/10000 --- Train Loss: 0.054900161283905 --- Val Loss: 0.047972874433393334 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6050/10000 --- Train Loss: 0.054855460332559114 --- Val Loss: 0.04792779637784304 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6060/10000 --- Train Loss: 0.05481075392676348 --- Val Loss: 0.04788407655355636 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6070/10000 --- Train Loss: 0.05476584659910539 --- Val Loss: 0.04783914262929166 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6080/10000 --- Train Loss: 0.05472147735357029 --- Val Loss: 0.0477949492330224 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6090/10000 --- Train Loss: 0.05467746952138599 --- Val Loss: 0.04775128958867253 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6100/10000 --- Train Loss: 0.054633452145669635 --- Val Loss: 0.04770743942389293 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6110/10000 --- Train Loss: 0.0545896474357106 --- Val Loss: 0.04766306499207617 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6120/10000 --- Train Loss: 0.05454547694551472 --- Val Loss: 0.047618821097171235 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6130/10000 --- Train Loss: 0.05450155323793298 --- Val Loss: 0.04757636398372342 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6140/10000 --- Train Loss: 0.05445811196737003 --- Val Loss: 0.04753251423583956 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6150/10000 --- Train Loss: 0.054414025238035416 --- Val Loss: 0.047487422574143144 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6160/10000 --- Train Loss: 0.05437055056762188 --- Val Loss: 0.04744396789096032 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6170/10000 --- Train Loss: 0.054327828028205065 --- Val Loss: 0.04740240369208417 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6180/10000 --- Train Loss: 0.05428486067120217 --- Val Loss: 0.047360025352761966 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6190/10000 --- Train Loss: 0.05424170725941386 --- Val Loss: 0.04731711135481483 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6200/10000 --- Train Loss: 0.05419857451061048 --- Val Loss: 0.047274208270408326 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6210/10000 --- Train Loss: 0.05415609691855714 --- Val Loss: 0.04723292420829296 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6220/10000 --- Train Loss: 0.054113664946428275 --- Val Loss: 0.04719164518827747 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6230/10000 --- Train Loss: 0.054071467334329375 --- Val Loss: 0.0471500550192465 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6240/10000 --- Train Loss: 0.054028780993703646 --- Val Loss: 0.04710766858070744 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6250/10000 --- Train Loss: 0.05398673929775463 --- Val Loss: 0.04706525895837674 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6260/10000 --- Train Loss: 0.05394428009198903 --- Val Loss: 0.04702421112776718 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6270/10000 --- Train Loss: 0.05390226337997819 --- Val Loss: 0.046982570111952726 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 6280/10000 --- Train Loss: 0.05386031309084078 --- Val Loss: 0.04694067588331546 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6290/10000 --- Train Loss: 0.05381937946540597 --- Val Loss: 0.0469032726308953 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6300/10000 --- Train Loss: 0.05377737595606553 --- Val Loss: 0.04686082348743504 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6310/10000 --- Train Loss: 0.053736022028394725 --- Val Loss: 0.0468195176436266 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6320/10000 --- Train Loss: 0.053694365430791965 --- Val Loss: 0.04677778985850841 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6330/10000 --- Train Loss: 0.053652982035467554 --- Val Loss: 0.04673707613975933 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6340/10000 --- Train Loss: 0.05361108651912158 --- Val Loss: 0.04669547229631748 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6350/10000 --- Train Loss: 0.05356957548385605 --- Val Loss: 0.04665406309248775 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6360/10000 --- Train Loss: 0.05352886279153458 --- Val Loss: 0.046614361472431975 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6370/10000 --- Train Loss: 0.05348774553201285 --- Val Loss: 0.04657286015522452 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6380/10000 --- Train Loss: 0.053447010164913045 --- Val Loss: 0.046532999714519206 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6390/10000 --- Train Loss: 0.053406807285276796 --- Val Loss: 0.04649478198358644 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6400/10000 --- Train Loss: 0.05336652898784339 --- Val Loss: 0.04645489138265196 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6410/10000 --- Train Loss: 0.05332636813045618 --- Val Loss: 0.0464147899141654 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6420/10000 --- Train Loss: 0.053286249946385256 --- Val Loss: 0.046376652639877426 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6430/10000 --- Train Loss: 0.05324604539189814 --- Val Loss: 0.04633763853474024 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6440/10000 --- Train Loss: 0.05320614809319563 --- Val Loss: 0.0462990949682013 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6450/10000 --- Train Loss: 0.05316621634156967 --- Val Loss: 0.04625955607873748 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6460/10000 --- Train Loss: 0.05312622541909635 --- Val Loss: 0.046221711230546955 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6470/10000 --- Train Loss: 0.05308673047984639 --- Val Loss: 0.04618284873185982 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6480/10000 --- Train Loss: 0.05304718167046542 --- Val Loss: 0.04614534019245163 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6490/10000 --- Train Loss: 0.05300769690476669 --- Val Loss: 0.046106380079283873 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6500/10000 --- Train Loss: 0.05296800303536303 --- Val Loss: 0.04606678596683752 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6510/10000 --- Train Loss: 0.05292858701349463 --- Val Loss: 0.046029193286921795 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6520/10000 --- Train Loss: 0.05288938501418356 --- Val Loss: 0.045991034420715594 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6530/10000 --- Train Loss: 0.05285033607037957 --- Val Loss: 0.04595259374846962 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6540/10000 --- Train Loss: 0.05281162811680352 --- Val Loss: 0.04591488078153407 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6550/10000 --- Train Loss: 0.05277257359290437 --- Val Loss: 0.045876909843234163 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6560/10000 --- Train Loss: 0.052733970949093684 --- Val Loss: 0.04583986107030597 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6570/10000 --- Train Loss: 0.05269567490112798 --- Val Loss: 0.04580458105809276 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6580/10000 --- Train Loss: 0.05265714211731794 --- Val Loss: 0.04576561891863413 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6590/10000 --- Train Loss: 0.05261868614807657 --- Val Loss: 0.045728650846197094 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6600/10000 --- Train Loss: 0.052580120662352854 --- Val Loss: 0.04569004130800074 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6610/10000 --- Train Loss: 0.05254145937644337 --- Val Loss: 0.04565181733708805 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6620/10000 --- Train Loss: 0.052503138073389535 --- Val Loss: 0.04561513254919992 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6630/10000 --- Train Loss: 0.05246496395883008 --- Val Loss: 0.045577880394347334 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6640/10000 --- Train Loss: 0.0524265828890176 --- Val Loss: 0.04554041431715266 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6650/10000 --- Train Loss: 0.05238828823287435 --- Val Loss: 0.045501754874111004 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6660/10000 --- Train Loss: 0.05235049216106788 --- Val Loss: 0.04546447054634984 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6670/10000 --- Train Loss: 0.05231293871388343 --- Val Loss: 0.045428199800634325 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6680/10000 --- Train Loss: 0.05227554623526325 --- Val Loss: 0.04539115140238 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6690/10000 --- Train Loss: 0.05223809043835867 --- Val Loss: 0.045354829093900324 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6700/10000 --- Train Loss: 0.05220066301455927 --- Val Loss: 0.04531873832735697 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6710/10000 --- Train Loss: 0.05216308685878082 --- Val Loss: 0.04528156090995615 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6720/10000 --- Train Loss: 0.05212570451980908 --- Val Loss: 0.04524561871782646 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6730/10000 --- Train Loss: 0.05208846015975508 --- Val Loss: 0.04520955139891621 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6740/10000 --- Train Loss: 0.05205158306952181 --- Val Loss: 0.045174497080877986 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6750/10000 --- Train Loss: 0.05201439332823094 --- Val Loss: 0.045138901076042355 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6760/10000 --- Train Loss: 0.05197746001930984 --- Val Loss: 0.04510407288773708 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6770/10000 --- Train Loss: 0.051940428994630326 --- Val Loss: 0.045068138705912746 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6780/10000 --- Train Loss: 0.051903553076808606 --- Val Loss: 0.04503222986173581 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6790/10000 --- Train Loss: 0.05186690922184245 --- Val Loss: 0.04499788806418317 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6800/10000 --- Train Loss: 0.05183039603739362 --- Val Loss: 0.044962874012430105 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6810/10000 --- Train Loss: 0.05179385580487072 --- Val Loss: 0.044927925860794735 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6820/10000 --- Train Loss: 0.05175705511311406 --- Val Loss: 0.044891528772507606 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6830/10000 --- Train Loss: 0.051720601286597084 --- Val Loss: 0.04485584750938679 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6840/10000 --- Train Loss: 0.051684574116206985 --- Val Loss: 0.044821647421986015 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6850/10000 --- Train Loss: 0.051648469089903754 --- Val Loss: 0.04478735037836399 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6860/10000 --- Train Loss: 0.051612193637537246 --- Val Loss: 0.04475220777338257 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6870/10000 --- Train Loss: 0.051576461978645065 --- Val Loss: 0.044719454371949616 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6880/10000 --- Train Loss: 0.05154016051938347 --- Val Loss: 0.04468470814214426 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6890/10000 --- Train Loss: 0.051504186411598867 --- Val Loss: 0.044649749359878345 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6900/10000 --- Train Loss: 0.05146834406067077 --- Val Loss: 0.044614377557855134 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6910/10000 --- Train Loss: 0.051432966997323075 --- Val Loss: 0.04458061404473776 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6920/10000 --- Train Loss: 0.05139733592397379 --- Val Loss: 0.04454588035057031 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6930/10000 --- Train Loss: 0.05136134338441386 --- Val Loss: 0.044510718536917425 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6940/10000 --- Train Loss: 0.05132603381341973 --- Val Loss: 0.044476581009609856 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6950/10000 --- Train Loss: 0.051290945838610665 --- Val Loss: 0.044442845408848755 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6960/10000 --- Train Loss: 0.05125582555178706 --- Val Loss: 0.04441138370214492 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 6970/10000 --- Train Loss: 0.05122068655084162 --- Val Loss: 0.044378555092773435 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 6980/10000 --- Train Loss: 0.05118521517671138 --- Val Loss: 0.04434315230921467 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 6990/10000 --- Train Loss: 0.05115013847253401 --- Val Loss: 0.04431180399577208 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7000/10000 --- Train Loss: 0.05111481348450581 --- Val Loss: 0.04427749087125019 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7010/10000 --- Train Loss: 0.05107996259540935 --- Val Loss: 0.04424424786557354 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7020/10000 --- Train Loss: 0.05104487803709616 --- Val Loss: 0.04420931481053168 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7030/10000 --- Train Loss: 0.051010190644538246 --- Val Loss: 0.04417711954812276 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7040/10000 --- Train Loss: 0.05097552950329139 --- Val Loss: 0.044143975489561386 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7050/10000 --- Train Loss: 0.050940909983142366 --- Val Loss: 0.04411063390807709 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7060/10000 --- Train Loss: 0.050906304346444796 --- Val Loss: 0.044076880116899864 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7070/10000 --- Train Loss: 0.05087212547843755 --- Val Loss: 0.04404483696230277 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7080/10000 --- Train Loss: 0.05083775208905382 --- Val Loss: 0.044011196904292275 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7090/10000 --- Train Loss: 0.05080380447877816 --- Val Loss: 0.04397978105709816 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7100/10000 --- Train Loss: 0.050769588720164634 --- Val Loss: 0.04394755889857166 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7110/10000 --- Train Loss: 0.05073504581451004 --- Val Loss: 0.043914582976420494 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7120/10000 --- Train Loss: 0.05070152767047323 --- Val Loss: 0.043882998446723935 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7130/10000 --- Train Loss: 0.05066743506479376 --- Val Loss: 0.04385011521867299 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7140/10000 --- Train Loss: 0.05063388667503471 --- Val Loss: 0.04381871506948312 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7150/10000 --- Train Loss: 0.050599549134481205 --- Val Loss: 0.04378586752135637 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7160/10000 --- Train Loss: 0.05056578135262754 --- Val Loss: 0.043754088706716335 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7170/10000 --- Train Loss: 0.05053190154267743 --- Val Loss: 0.0437216417210054 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7180/10000 --- Train Loss: 0.050498186098567696 --- Val Loss: 0.043689574593165637 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7190/10000 --- Train Loss: 0.050464904557242604 --- Val Loss: 0.043658869159662696 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7200/10000 --- Train Loss: 0.05043160347093576 --- Val Loss: 0.0436272795551658 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7210/10000 --- Train Loss: 0.0503979669875093 --- Val Loss: 0.04359490939959992 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7220/10000 --- Train Loss: 0.05036479785048774 --- Val Loss: 0.04356500251590015 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7230/10000 --- Train Loss: 0.05033157818345734 --- Val Loss: 0.04353461422399588 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7240/10000 --- Train Loss: 0.05029869053192521 --- Val Loss: 0.04350461538614659 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7250/10000 --- Train Loss: 0.0502655183426136 --- Val Loss: 0.04347353269997456 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7260/10000 --- Train Loss: 0.050232407549672245 --- Val Loss: 0.043441672556695336 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7270/10000 --- Train Loss: 0.05019954536171641 --- Val Loss: 0.043411866682570376 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7280/10000 --- Train Loss: 0.05016660192385632 --- Val Loss: 0.04338045563987284 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7290/10000 --- Train Loss: 0.05013419464225866 --- Val Loss: 0.04335075117701605 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7300/10000 --- Train Loss: 0.050101370389136105 --- Val Loss: 0.04332033172541733 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7310/10000 --- Train Loss: 0.050068373448401746 --- Val Loss: 0.0432894245711441 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7320/10000 --- Train Loss: 0.05003574846588402 --- Val Loss: 0.0432594970019159 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7330/10000 --- Train Loss: 0.05000339909392109 --- Val Loss: 0.04322904753247505 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7340/10000 --- Train Loss: 0.04997109551868705 --- Val Loss: 0.04320009482933667 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7350/10000 --- Train Loss: 0.04993880364922146 --- Val Loss: 0.043169698702610865 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7360/10000 --- Train Loss: 0.049906247054675335 --- Val Loss: 0.043138558507967543 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7370/10000 --- Train Loss: 0.04987434086994051 --- Val Loss: 0.04310919403290261 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7380/10000 --- Train Loss: 0.04984217568482811 --- Val Loss: 0.04307921052571153 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7390/10000 --- Train Loss: 0.049810400971382514 --- Val Loss: 0.04305029329257378 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7400/10000 --- Train Loss: 0.049778526087908274 --- Val Loss: 0.04302112283224468 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7410/10000 --- Train Loss: 0.04974650907510602 --- Val Loss: 0.0429907765894767 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7420/10000 --- Train Loss: 0.04971483183869808 --- Val Loss: 0.04296158644268163 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7430/10000 --- Train Loss: 0.04968290487018277 --- Val Loss: 0.042931881405502914 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7440/10000 --- Train Loss: 0.04965107337014036 --- Val Loss: 0.042901723907884805 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7450/10000 --- Train Loss: 0.04961933237651761 --- Val Loss: 0.04287184499183126 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7460/10000 --- Train Loss: 0.04958731739134351 --- Val Loss: 0.042842893111064234 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7470/10000 --- Train Loss: 0.04955555295901134 --- Val Loss: 0.0428117729458145 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7480/10000 --- Train Loss: 0.04952446494974474 --- Val Loss: 0.0427833569852451 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7490/10000 --- Train Loss: 0.049493256979294374 --- Val Loss: 0.0427539362972382 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7500/10000 --- Train Loss: 0.049462082601715786 --- Val Loss: 0.04272608951806206 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7510/10000 --- Train Loss: 0.04943075574465397 --- Val Loss: 0.04269716798080697 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7520/10000 --- Train Loss: 0.049399700402336634 --- Val Loss: 0.042667986855276445 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7530/10000 --- Train Loss: 0.049368366238308994 --- Val Loss: 0.0426374175733127 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7540/10000 --- Train Loss: 0.049337645927756406 --- Val Loss: 0.04260965347717977 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7550/10000 --- Train Loss: 0.04930637128732498 --- Val Loss: 0.042579352109771505 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7560/10000 --- Train Loss: 0.049275313078117576 --- Val Loss: 0.04255131119575031 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7570/10000 --- Train Loss: 0.04924490517395169 --- Val Loss: 0.0425248040897779 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7580/10000 --- Train Loss: 0.04921453568869436 --- Val Loss: 0.04249717539854109 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7590/10000 --- Train Loss: 0.04918411002069859 --- Val Loss: 0.042469596542826095 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7600/10000 --- Train Loss: 0.04915361854804117 --- Val Loss: 0.042443071579489414 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7610/10000 --- Train Loss: 0.049122827840396024 --- Val Loss: 0.04241558849735607 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7620/10000 --- Train Loss: 0.049091840434517524 --- Val Loss: 0.04238642036842826 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7630/10000 --- Train Loss: 0.04906141878817679 --- Val Loss: 0.042359039872334504 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7640/10000 --- Train Loss: 0.0490308729029266 --- Val Loss: 0.04233195038702217 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7650/10000 --- Train Loss: 0.049000716807988676 --- Val Loss: 0.04230468538644111 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7660/10000 --- Train Loss: 0.04896991227981329 --- Val Loss: 0.04227701453527296 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7670/10000 --- Train Loss: 0.04893974319214046 --- Val Loss: 0.04225055240206445 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7680/10000 --- Train Loss: 0.04890953579782081 --- Val Loss: 0.0422238549963216 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7690/10000 --- Train Loss: 0.04887926376922503 --- Val Loss: 0.0421955911627633 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7700/10000 --- Train Loss: 0.04884928995025569 --- Val Loss: 0.042169199733653774 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7710/10000 --- Train Loss: 0.048819271155878766 --- Val Loss: 0.042142704285822945 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7720/10000 --- Train Loss: 0.04878916161819968 --- Val Loss: 0.04211695347544858 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7730/10000 --- Train Loss: 0.04875934926523588 --- Val Loss: 0.042089588983203814 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7740/10000 --- Train Loss: 0.048729568485138146 --- Val Loss: 0.04206247542422475 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7750/10000 --- Train Loss: 0.04869988637128733 --- Val Loss: 0.04203709294632653 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7760/10000 --- Train Loss: 0.04867055091490096 --- Val Loss: 0.042011641347307026 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7770/10000 --- Train Loss: 0.048640939218521806 --- Val Loss: 0.041985704415796715 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7780/10000 --- Train Loss: 0.04861129897983241 --- Val Loss: 0.041958032860556785 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7790/10000 --- Train Loss: 0.04858196389764365 --- Val Loss: 0.04193172686305302 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7800/10000 --- Train Loss: 0.04855289273963156 --- Val Loss: 0.04190663252888345 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7810/10000 --- Train Loss: 0.048523580282547885 --- Val Loss: 0.04188067125326852 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7820/10000 --- Train Loss: 0.048493905663252734 --- Val Loss: 0.04185258133840045 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7830/10000 --- Train Loss: 0.048464493782748544 --- Val Loss: 0.04182588188500818 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7840/10000 --- Train Loss: 0.04843494103911771 --- Val Loss: 0.041798714071288684 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7850/10000 --- Train Loss: 0.04840598669325758 --- Val Loss: 0.04177361304149665 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7860/10000 --- Train Loss: 0.048376872957372766 --- Val Loss: 0.041747445422546145 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7870/10000 --- Train Loss: 0.04834802918706396 --- Val Loss: 0.04172175059319633 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7880/10000 --- Train Loss: 0.04831919902420765 --- Val Loss: 0.041696656866421794 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7890/10000 --- Train Loss: 0.04829055887612168 --- Val Loss: 0.041671718732217686 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7900/10000 --- Train Loss: 0.04826131376875441 --- Val Loss: 0.04164524192173761 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7910/10000 --- Train Loss: 0.04823226531712757 --- Val Loss: 0.04161859465879029 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7920/10000 --- Train Loss: 0.048203581413322726 --- Val Loss: 0.04159354901315767 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7930/10000 --- Train Loss: 0.048174473583536596 --- Val Loss: 0.04156777980660967 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7940/10000 --- Train Loss: 0.04814545557904593 --- Val Loss: 0.04154177323898353 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7950/10000 --- Train Loss: 0.04811741708922906 --- Val Loss: 0.041519324862457616 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7960/10000 --- Train Loss: 0.04808852080210401 --- Val Loss: 0.04149316931110766 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7970/10000 --- Train Loss: 0.04805987562652363 --- Val Loss: 0.041468128718963645 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7980/10000 --- Train Loss: 0.048031905278160855 --- Val Loss: 0.04144485480822943 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 7990/10000 --- Train Loss: 0.04800310439055372 --- Val Loss: 0.04142017464399665 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8000/10000 --- Train Loss: 0.04797442804460844 --- Val Loss: 0.04139389748498882 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8010/10000 --- Train Loss: 0.047946300039115125 --- Val Loss: 0.04136854500444975 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8020/10000 --- Train Loss: 0.047917991167685636 --- Val Loss: 0.04134268307977186 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8030/10000 --- Train Loss: 0.047890251337582825 --- Val Loss: 0.04131860287228889 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8040/10000 --- Train Loss: 0.04786205294637455 --- Val Loss: 0.041294088368075924 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8050/10000 --- Train Loss: 0.047834241705254056 --- Val Loss: 0.041269299825136406 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8060/10000 --- Train Loss: 0.0478058395111845 --- Val Loss: 0.041244711414169945 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8070/10000 --- Train Loss: 0.04777800569892229 --- Val Loss: 0.04122014992558965 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8080/10000 --- Train Loss: 0.04774973194266175 --- Val Loss: 0.04119449442507766 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8090/10000 --- Train Loss: 0.04772208980012634 --- Val Loss: 0.04117059316842436 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8100/10000 --- Train Loss: 0.04769441567840521 --- Val Loss: 0.041145965611551975 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8110/10000 --- Train Loss: 0.04766689719637145 --- Val Loss: 0.04112170960491541 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8120/10000 --- Train Loss: 0.04763893958956021 --- Val Loss: 0.04109565134424468 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8130/10000 --- Train Loss: 0.047611071590731774 --- Val Loss: 0.04107168269613163 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8140/10000 --- Train Loss: 0.047582842874499265 --- Val Loss: 0.04104545895506234 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8150/10000 --- Train Loss: 0.0475552396794465 --- Val Loss: 0.04102104784435144 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8160/10000 --- Train Loss: 0.047528500312699624 --- Val Loss: 0.04099863766932845 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8170/10000 --- Train Loss: 0.047500695437730577 --- Val Loss: 0.04097418110590233 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8180/10000 --- Train Loss: 0.04747289631894667 --- Val Loss: 0.04094899039629962 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8190/10000 --- Train Loss: 0.04744590752116107 --- Val Loss: 0.04092649184627783 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8200/10000 --- Train Loss: 0.04741884003444928 --- Val Loss: 0.040903059706587426 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8210/10000 --- Train Loss: 0.04739142975671657 --- Val Loss: 0.04087961737603074 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8220/10000 --- Train Loss: 0.04736429528497421 --- Val Loss: 0.040856179858657506 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8230/10000 --- Train Loss: 0.04733733666440567 --- Val Loss: 0.04083484362999356 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8240/10000 --- Train Loss: 0.047310918165095764 --- Val Loss: 0.04081282108700266 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8250/10000 --- Train Loss: 0.04728371669259672 --- Val Loss: 0.0407890308408721 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8260/10000 --- Train Loss: 0.04725639017362681 --- Val Loss: 0.04076396446577069 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8270/10000 --- Train Loss: 0.04722958448540353 --- Val Loss: 0.04074082067445575 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8280/10000 --- Train Loss: 0.04720269033586683 --- Val Loss: 0.040718212070344796 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8290/10000 --- Train Loss: 0.04717601771985583 --- Val Loss: 0.04069502064514584 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8300/10000 --- Train Loss: 0.047148400159025806 --- Val Loss: 0.04066852872290477 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8310/10000 --- Train Loss: 0.04712177354966704 --- Val Loss: 0.04064561478695031 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8320/10000 --- Train Loss: 0.047095030663330414 --- Val Loss: 0.0406227909115636 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8330/10000 --- Train Loss: 0.0470679612163719 --- Val Loss: 0.04059794018759336 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8340/10000 --- Train Loss: 0.04704155680810959 --- Val Loss: 0.04057681852689766 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8350/10000 --- Train Loss: 0.04701505265901601 --- Val Loss: 0.04055436787370475 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8360/10000 --- Train Loss: 0.04698883931302767 --- Val Loss: 0.04053261883188144 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8370/10000 --- Train Loss: 0.04696242584031412 --- Val Loss: 0.04050955778816174 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8380/10000 --- Train Loss: 0.04693642065477267 --- Val Loss: 0.04048867056105319 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8390/10000 --- Train Loss: 0.0469100286650093 --- Val Loss: 0.04046605092736329 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8400/10000 --- Train Loss: 0.04688327220971714 --- Val Loss: 0.04044186775316984 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8410/10000 --- Train Loss: 0.046857470963658776 --- Val Loss: 0.04042024960546541 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8420/10000 --- Train Loss: 0.04683125423687849 --- Val Loss: 0.040397390455152414 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 8430/10000 --- Train Loss: 0.046805163793980746 --- Val Loss: 0.04037594886161278 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8440/10000 --- Train Loss: 0.046779393803339536 --- Val Loss: 0.040354168868480186 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8450/10000 --- Train Loss: 0.04675386803957321 --- Val Loss: 0.04033313241827348 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8460/10000 --- Train Loss: 0.04672810532670381 --- Val Loss: 0.04031119538950631 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8470/10000 --- Train Loss: 0.04670248163517131 --- Val Loss: 0.04028913442116255 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8480/10000 --- Train Loss: 0.04667644264893688 --- Val Loss: 0.04026656708755549 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8490/10000 --- Train Loss: 0.04665004733367713 --- Val Loss: 0.040242591253322545 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8500/10000 --- Train Loss: 0.04662417290858269 --- Val Loss: 0.04022028377325786 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8510/10000 --- Train Loss: 0.04659811525805075 --- Val Loss: 0.040197483050723876 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8520/10000 --- Train Loss: 0.046572710075885114 --- Val Loss: 0.04017664508743901 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8530/10000 --- Train Loss: 0.04654728020728261 --- Val Loss: 0.040155601157164975 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8540/10000 --- Train Loss: 0.04652134677634705 --- Val Loss: 0.040133094360288356 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8550/10000 --- Train Loss: 0.04649617732750298 --- Val Loss: 0.04011267643965066 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8560/10000 --- Train Loss: 0.04647062168530249 --- Val Loss: 0.04009171744844459 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8570/10000 --- Train Loss: 0.046445066698205924 --- Val Loss: 0.04006837604271363 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8580/10000 --- Train Loss: 0.046419394329813186 --- Val Loss: 0.04004567448490457 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8590/10000 --- Train Loss: 0.04639416598039709 --- Val Loss: 0.0400248073434621 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8600/10000 --- Train Loss: 0.04636861343865749 --- Val Loss: 0.04000202530336192 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8610/10000 --- Train Loss: 0.0463438355060713 --- Val Loss: 0.039982633340928074 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8620/10000 --- Train Loss: 0.04631841873004045 --- Val Loss: 0.03996061403895646 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8630/10000 --- Train Loss: 0.04629284617887038 --- Val Loss: 0.03993825055891814 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8640/10000 --- Train Loss: 0.04626775160351323 --- Val Loss: 0.03991722475643007 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8650/10000 --- Train Loss: 0.046242206074740555 --- Val Loss: 0.039895392097713284 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8660/10000 --- Train Loss: 0.04621691328468758 --- Val Loss: 0.039873229350778146 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8670/10000 --- Train Loss: 0.04619221027410614 --- Val Loss: 0.03985341239256761 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8680/10000 --- Train Loss: 0.046167217561221705 --- Val Loss: 0.03983144464007994 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8690/10000 --- Train Loss: 0.04614216277639299 --- Val Loss: 0.03981050029447999 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8700/10000 --- Train Loss: 0.046117409514573944 --- Val Loss: 0.03978916131158853 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8710/10000 --- Train Loss: 0.0460922148263698 --- Val Loss: 0.03976848312410841 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8720/10000 --- Train Loss: 0.04606752609934507 --- Val Loss: 0.0397477071563121 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8730/10000 --- Train Loss: 0.04604291586152904 --- Val Loss: 0.03972808152532289 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8740/10000 --- Train Loss: 0.04601773136904559 --- Val Loss: 0.039705994912207235 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8750/10000 --- Train Loss: 0.045993313544404765 --- Val Loss: 0.03968582073045553 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8760/10000 --- Train Loss: 0.04596875423107096 --- Val Loss: 0.039665942340308256 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8770/10000 --- Train Loss: 0.045943950718324994 --- Val Loss: 0.03964561820015831 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8780/10000 --- Train Loss: 0.04591974681982853 --- Val Loss: 0.039627062581919836 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8790/10000 --- Train Loss: 0.04589546581623346 --- Val Loss: 0.03960791634698244 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8800/10000 --- Train Loss: 0.04587041432916468 --- Val Loss: 0.03958626305722881 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8810/10000 --- Train Loss: 0.045846070252530596 --- Val Loss: 0.03956740043487272 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8820/10000 --- Train Loss: 0.045821232446559265 --- Val Loss: 0.03954533191280092 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8830/10000 --- Train Loss: 0.04579685178986335 --- Val Loss: 0.03952525068745949 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8840/10000 --- Train Loss: 0.04577236275876013 --- Val Loss: 0.039504691642628306 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8850/10000 --- Train Loss: 0.045748357108005164 --- Val Loss: 0.03948504985506705 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8860/10000 --- Train Loss: 0.04572411332281508 --- Val Loss: 0.03946488486915344 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8870/10000 --- Train Loss: 0.045699644392948574 --- Val Loss: 0.039444913784937634 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8880/10000 --- Train Loss: 0.045675414132042624 --- Val Loss: 0.03942580270478723 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8890/10000 --- Train Loss: 0.04565134926753175 --- Val Loss: 0.03940615777526836 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8900/10000 --- Train Loss: 0.04562726895655029 --- Val Loss: 0.039385932379750806 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8910/10000 --- Train Loss: 0.04560368280153606 --- Val Loss: 0.039366359893550563 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8920/10000 --- Train Loss: 0.04557977023409222 --- Val Loss: 0.039346732138863534 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8930/10000 --- Train Loss: 0.04555553273611497 --- Val Loss: 0.03932658265304463 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8940/10000 --- Train Loss: 0.04553157779174506 --- Val Loss: 0.0393069665230212 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8950/10000 --- Train Loss: 0.04550819742837302 --- Val Loss: 0.03928852496373009 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8960/10000 --- Train Loss: 0.04548419511378117 --- Val Loss: 0.03926874161333264 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8970/10000 --- Train Loss: 0.04546081923293603 --- Val Loss: 0.03924931979240741 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8980/10000 --- Train Loss: 0.04543691275177593 --- Val Loss: 0.039230401657137856 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 8990/10000 --- Train Loss: 0.045412883862367304 --- Val Loss: 0.039210665198524564 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9000/10000 --- Train Loss: 0.045389114300794564 --- Val Loss: 0.03919183052590093 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9010/10000 --- Train Loss: 0.04536519165903441 --- Val Loss: 0.03917296696198511 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9020/10000 --- Train Loss: 0.045341677846596165 --- Val Loss: 0.03915363265841424 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9030/10000 --- Train Loss: 0.04531819508533376 --- Val Loss: 0.03913440165231522 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9040/10000 --- Train Loss: 0.045294641081900205 --- Val Loss: 0.039114550715251974 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9050/10000 --- Train Loss: 0.04527093084526478 --- Val Loss: 0.03909536638405439 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9060/10000 --- Train Loss: 0.04524721673814393 --- Val Loss: 0.03907616306661232 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9070/10000 --- Train Loss: 0.04522395581540315 --- Val Loss: 0.03905747885324734 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9080/10000 --- Train Loss: 0.04520059859066824 --- Val Loss: 0.03903958433206781 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9090/10000 --- Train Loss: 0.04517669529844341 --- Val Loss: 0.039018805920884976 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9100/10000 --- Train Loss: 0.04515342926434757 --- Val Loss: 0.039000050322976335 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9110/10000 --- Train Loss: 0.045130370796706606 --- Val Loss: 0.038982233624181524 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9120/10000 --- Train Loss: 0.04510762976898653 --- Val Loss: 0.03896430179419323 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9130/10000 --- Train Loss: 0.045084418553659404 --- Val Loss: 0.03894579752364494 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9140/10000 --- Train Loss: 0.045061085196625814 --- Val Loss: 0.03892735116947467 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9150/10000 --- Train Loss: 0.045038291642687286 --- Val Loss: 0.03890987764226787 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9160/10000 --- Train Loss: 0.04501453897223594 --- Val Loss: 0.03889123733667485 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9170/10000 --- Train Loss: 0.044991220766504435 --- Val Loss: 0.038871631056262415 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9180/10000 --- Train Loss: 0.0449680721406052 --- Val Loss: 0.03885219108341843 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9190/10000 --- Train Loss: 0.04494506253864193 --- Val Loss: 0.0388338839371981 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9200/10000 --- Train Loss: 0.04492198054028549 --- Val Loss: 0.03881481407973509 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9210/10000 --- Train Loss: 0.04489906190368226 --- Val Loss: 0.038795794527355466 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9220/10000 --- Train Loss: 0.04487632894644961 --- Val Loss: 0.03877823194248961 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9230/10000 --- Train Loss: 0.04485364066893226 --- Val Loss: 0.03875982534837667 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9240/10000 --- Train Loss: 0.044830671605918415 --- Val Loss: 0.038741236184416425 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9250/10000 --- Train Loss: 0.04480846939018638 --- Val Loss: 0.038723876802634026 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9260/10000 --- Train Loss: 0.044785192513507724 --- Val Loss: 0.038703786687219736 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9270/10000 --- Train Loss: 0.04476245529463974 --- Val Loss: 0.03868614425245404 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9280/10000 --- Train Loss: 0.04473957040717432 --- Val Loss: 0.03866845475936067 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9290/10000 --- Train Loss: 0.04471710317429674 --- Val Loss: 0.03865033205854671 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9300/10000 --- Train Loss: 0.04469498046495051 --- Val Loss: 0.03863334959267659 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9310/10000 --- Train Loss: 0.04467225880078802 --- Val Loss: 0.03861542344609988 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9320/10000 --- Train Loss: 0.04464974225567211 --- Val Loss: 0.038597649009080186 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9330/10000 --- Train Loss: 0.044627098048648874 --- Val Loss: 0.03857914306794155 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9340/10000 --- Train Loss: 0.044604537186465815 --- Val Loss: 0.038561524472855414 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9350/10000 --- Train Loss: 0.04458204161011394 --- Val Loss: 0.03854351652102431 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9360/10000 --- Train Loss: 0.04455933462579886 --- Val Loss: 0.03852441646876361 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9370/10000 --- Train Loss: 0.04453730741049572 --- Val Loss: 0.03850731124260956 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9380/10000 --- Train Loss: 0.044515292476252896 --- Val Loss: 0.03848970163084062 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9390/10000 --- Train Loss: 0.04449314419451816 --- Val Loss: 0.03847289756245417 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9400/10000 --- Train Loss: 0.044471075408540034 --- Val Loss: 0.038456583990071874 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9410/10000 --- Train Loss: 0.04444894703857762 --- Val Loss: 0.03843884148720257 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9420/10000 --- Train Loss: 0.0444268511651362 --- Val Loss: 0.038421781186677675 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9430/10000 --- Train Loss: 0.044404615209466466 --- Val Loss: 0.03840396449157205 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9440/10000 --- Train Loss: 0.04438301606068204 --- Val Loss: 0.03838872282981757 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9450/10000 --- Train Loss: 0.04436046330954458 --- Val Loss: 0.0383711253315328 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9460/10000 --- Train Loss: 0.04433849831182861 --- Val Loss: 0.03835485579504933 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9470/10000 --- Train Loss: 0.0443161469115482 --- Val Loss: 0.038336513590003216 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9480/10000 --- Train Loss: 0.044294071340537304 --- Val Loss: 0.03831950261059874 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9490/10000 --- Train Loss: 0.04427183337951821 --- Val Loss: 0.038300586369418715 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9500/10000 --- Train Loss: 0.04425009811067674 --- Val Loss: 0.03828518354720122 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9510/10000 --- Train Loss: 0.04422808191990333 --- Val Loss: 0.03826826312810212 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9520/10000 --- Train Loss: 0.04420641332348361 --- Val Loss: 0.03825207825754043 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9530/10000 --- Train Loss: 0.04418450743409718 --- Val Loss: 0.038234963155347856 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9540/10000 --- Train Loss: 0.04416356723635385 --- Val Loss: 0.038221317207564734 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9550/10000 --- Train Loss: 0.04414130610126049 --- Val Loss: 0.03820217650351137 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9560/10000 --- Train Loss: 0.04411963971949588 --- Val Loss: 0.038185198344440695 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9570/10000 --- Train Loss: 0.04409728197953085 --- Val Loss: 0.03816702180970394 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9580/10000 --- Train Loss: 0.04407545682835128 --- Val Loss: 0.038149717160654815 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9590/10000 --- Train Loss: 0.04405375060913321 --- Val Loss: 0.038132246346192465 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9600/10000 --- Train Loss: 0.04403251928928668 --- Val Loss: 0.038116379460780704 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9610/10000 --- Train Loss: 0.044010554604820076 --- Val Loss: 0.03809904541696735 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9620/10000 --- Train Loss: 0.043988466095257334 --- Val Loss: 0.03808009892775545 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9630/10000 --- Train Loss: 0.043967924705130654 --- Val Loss: 0.03806707202301107 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9640/10000 --- Train Loss: 0.04394596829718963 --- Val Loss: 0.03804972512289837 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9650/10000 --- Train Loss: 0.04392396216078936 --- Val Loss: 0.03803145210915019 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9660/10000 --- Train Loss: 0.04390218997520354 --- Val Loss: 0.03801341573575365 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9670/10000 --- Train Loss: 0.04388145190559956 --- Val Loss: 0.03799828235801773 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9680/10000 --- Train Loss: 0.043859852563906 --- Val Loss: 0.03798126218646937 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9690/10000 --- Train Loss: 0.043838236177366784 --- Val Loss: 0.03796301991589817 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9700/10000 --- Train Loss: 0.04381682533955321 --- Val Loss: 0.0379455763225507 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9710/10000 --- Train Loss: 0.04379589391470494 --- Val Loss: 0.037930729545535856 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9720/10000 --- Train Loss: 0.04377407208256446 --- Val Loss: 0.037912598754798604 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9730/10000 --- Train Loss: 0.043752748754630706 --- Val Loss: 0.037895531608669396 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9740/10000 --- Train Loss: 0.04373204265777331 --- Val Loss: 0.03788032937140475 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9750/10000 --- Train Loss: 0.04371178297513493 --- Val Loss: 0.03786613657204367 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9760/10000 --- Train Loss: 0.043690853012602456 --- Val Loss: 0.037850373167503705 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9770/10000 --- Train Loss: 0.04366971160522143 --- Val Loss: 0.03783292312023333 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9780/10000 --- Train Loss: 0.04364895426499745 --- Val Loss: 0.037818002896783776 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9790/10000 --- Train Loss: 0.0436279017020088 --- Val Loss: 0.037801634229664094 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9800/10000 --- Train Loss: 0.04360670516594706 --- Val Loss: 0.037783826642404574 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9810/10000 --- Train Loss: 0.04358566822709396 --- Val Loss: 0.037768021043500695 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9820/10000 --- Train Loss: 0.04356507121527063 --- Val Loss: 0.03775258361379401 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9830/10000 --- Train Loss: 0.0435442877656014 --- Val Loss: 0.03773693155032157 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9840/10000 --- Train Loss: 0.04352371284714921 --- Val Loss: 0.037721555521099855 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9850/10000 --- Train Loss: 0.04350218559857111 --- Val Loss: 0.03770231689797188 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9860/10000 --- Train Loss: 0.043481567803301306 --- Val Loss: 0.037687808708264915 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9870/10000 --- Train Loss: 0.04346107102186224 --- Val Loss: 0.037672880279301935 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9880/10000 --- Train Loss: 0.04343990801013893 --- Val Loss: 0.03765535189675613 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9890/10000 --- Train Loss: 0.04341979522278063 --- Val Loss: 0.03764128230987473 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9900/10000 --- Train Loss: 0.04339902044094906 --- Val Loss: 0.03762556523233631 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9910/10000 --- Train Loss: 0.0433781742481414 --- Val Loss: 0.03760968253680731 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9920/10000 --- Train Loss: 0.04335770871478795 --- Val Loss: 0.037594594423190845 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9930/10000 --- Train Loss: 0.04333729342811226 --- Val Loss: 0.03758002926679293 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9940/10000 --- Train Loss: 0.043317184802452446 --- Val Loss: 0.03756595556477388 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9950/10000 --- Train Loss: 0.04329636978551559 --- Val Loss: 0.03754951009627625 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9960/10000 --- Train Loss: 0.04327555186275891 --- Val Loss: 0.03753272700944937 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9970/10000 --- Train Loss: 0.04325483690116161 --- Val Loss: 0.03751628354926994 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9980/10000 --- Train Loss: 0.04323444222311792 --- Val Loss: 0.03750155662988503 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 9990/10000 --- Train Loss: 0.04321369260660278 --- Val Loss: 0.03748564049896458 --- Train Acc: 0.99 --- Val Acc: 0.99\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 2))\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=10000, learning_rate=0.001, batch_size=16)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "* optimization of hyperparameters (random search and grid search function?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearch:\n",
    "    def __init__(self, network, param_grid, n_iter=10):\n",
    "        self.network = NeuralNetwork\n",
    "        self.param_grid = param_grid\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def sample_params(self):\n",
    "        sampled_params = {}\n",
    "        for param, values in self.param_grid.items():\n",
    "            sampled_params[param] = np.random.choice(values)\n",
    "        return sampled_params\n",
    "\n",
    "    def evaluate(self, X_train, y_train, X_val, y_val, params):\n",
    "        network = self.network()\n",
    "\n",
    "        config = params['layer_configs']\n",
    "        # Add the first Dense layer\n",
    "        network.add_layer(Layer(64, config['layer1_nodes'], l1=config['layer1_l1'], l2=config['layer1_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        network.add_layer(Dropout(0.25))\n",
    "\n",
    "        print(config['layer1_nodes'])\n",
    "        # Add the second Dense layer\n",
    "        network.add_layer(Layer(config['layer1_nodes'], config['layer2_nodes'], l1=config['layer2_l1'], l2=config['layer2_l2']))\n",
    "        network.add_layer(ReLU())\n",
    "\n",
    "        # Add the output Softmax layer\n",
    "        network.add_layer(Layer(config['layer2_nodes'], 10))\n",
    "        network.add_layer(Softmax())\n",
    "        \n",
    "        network.train(X_train, y_train, epochs=params['epochs'], learning_rate=params['learning_rate'],\n",
    "                      optimizer=params['optimizer'], momentum=params['momentum'], batch_size=params['batch_size'])\n",
    "        \n",
    "        y_pred = network.predict(X_val)\n",
    "        y_val = np.argmax(y_val, axis=1)\n",
    "        accuracy = np.mean(y_pred == y_val)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def search(self, X, y):\n",
    "        best_params = None\n",
    "        best_accuracy = 0\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            params = self.sample_params()\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            accuracy = self.evaluate(X_train, y_train, X_val, y_val, params)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "\n",
    "            print(f\"Params: {params}, Accuracy: {accuracy}\")\n",
    "\n",
    "        return best_params, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'layer_configs': [\n",
    "        {\n",
    "            'layer1_nodes': layer1_nodes,\n",
    "            'layer1_l1': layer1_l1,\n",
    "            'layer1_l2': layer1_l2,\n",
    "            'layer2_nodes': layer2_nodes,\n",
    "            'layer2_l1': layer2_l1,\n",
    "            'layer2_l2': layer2_l2\n",
    "        }\n",
    "        for layer1_nodes in [32, 64, 128]  # Possible node counts for the first Dense layer\n",
    "        for layer1_l1 in [0.0, 0.01]       # L1 regularization for the first Dense layer\n",
    "        for layer1_l2 in [0.0, 0.01]       # L2 regularization for the first Dense layer\n",
    "        for layer2_nodes in [16, 32, 64]   # Possible node counts for the second Dense layer\n",
    "        for layer2_l1 in [0.0, 0.01]       # L1 regularization for the second Dense layer\n",
    "        for layer2_l2 in [0.0, 0.01]       # L2 regularization for the second Dense layer\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'epochs': [100, 500, 1000],\n",
    "    'optimizer': ['GD', 'Momentum'],\n",
    "    'momentum': [0.5, 0.9],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.300766811573432 --- Val Loss: 2.303600083573397 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/1000 --- Train Loss: 2.2797398128238977 --- Val Loss: 2.249519633641929 --- Train Acc: 0.18 --- Val Acc: 0.18\n",
      "Epoch 20/1000 --- Train Loss: 2.3031069257751953 --- Val Loss: 2.306943833579092 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 30/1000 --- Train Loss: 2.3199980819621273 --- Val Loss: 2.3040400165957395 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/1000 --- Train Loss: 2.3008665075864165 --- Val Loss: 2.3050464906058328 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 50/1000 --- Train Loss: 2.3088638387968934 --- Val Loss: 2.3040194520490758 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: 2.3005227799750747 --- Val Loss: 2.304606529003566 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: 2.29932104654558 --- Val Loss: 2.307214382120952 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 80/1000 --- Train Loss: 2.300614453481691 --- Val Loss: 2.3029581830023242 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/1000 --- Train Loss: 2.301158264795493 --- Val Loss: 2.3079918147600984 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 100/1000 --- Train Loss: 2.300938788970343 --- Val Loss: 2.3048356854590497 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/1000 --- Train Loss: 2.300732595158264 --- Val Loss: 2.3059202472321325 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/1000 --- Train Loss: 2.3010389267366995 --- Val Loss: 2.3045562174797936 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/1000 --- Train Loss: 2.324887271184212 --- Val Loss: 2.3047147369502325 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/1000 --- Train Loss: 2.3009482144605977 --- Val Loss: 2.3035014882550593 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/1000 --- Train Loss: 2.3010768006452196 --- Val Loss: 2.3052587607040027 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: 2.3008374849605358 --- Val Loss: 2.3050251486887867 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/1000 --- Train Loss: 2.3128391126874703 --- Val Loss: 2.30441984962222 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: 2.3006948145940838 --- Val Loss: 2.3056908583505336 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: 2.3009980799696756 --- Val Loss: 2.3028936307931076 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.298528138275878 --- Val Loss: 2.3052446160642077 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/1000 --- Train Loss: 2.301710911655381 --- Val Loss: 2.3049412093682013 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: 2.300854076168656 --- Val Loss: 2.304793137532418 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 230/1000 --- Train Loss: 2.300742519872601 --- Val Loss: 2.3041090233052173 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.30111973766537 --- Val Loss: 2.3063455139965914 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/1000 --- Train Loss: 2.301004539023519 --- Val Loss: 2.304783768838715 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 260/1000 --- Train Loss: 2.3014169842881507 --- Val Loss: 2.3072669513410147 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.3009802934309866 --- Val Loss: 2.305217933553989 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/1000 --- Train Loss: 2.301201319349771 --- Val Loss: 2.304829984468792 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/1000 --- Train Loss: 2.300897115247216 --- Val Loss: 2.305822669669109 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/1000 --- Train Loss: 2.300955945942555 --- Val Loss: 2.3054628930470558 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/1000 --- Train Loss: 2.300906328095281 --- Val Loss: 2.302840483344274 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/1000 --- Train Loss: 2.3009370607704724 --- Val Loss: 2.302603056554765 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/1000 --- Train Loss: 2.3007806131360873 --- Val Loss: 2.3045766068392326 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/1000 --- Train Loss: 2.3014690562526416 --- Val Loss: 2.306582743787303 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/1000 --- Train Loss: 2.300992126781392 --- Val Loss: 2.303206142955677 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/1000 --- Train Loss: 2.300707147461154 --- Val Loss: 2.3053967723967834 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 370/1000 --- Train Loss: 2.300641865308807 --- Val Loss: 2.3041639328683066 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/1000 --- Train Loss: 2.300979084854328 --- Val Loss: 2.3039377220793016 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/1000 --- Train Loss: 2.3009298006838206 --- Val Loss: 2.3055857700728466 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/1000 --- Train Loss: 2.300676880591683 --- Val Loss: 2.3039793708557754 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/1000 --- Train Loss: 2.3007868711433166 --- Val Loss: 2.3042752450010195 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/1000 --- Train Loss: 2.3009503935297952 --- Val Loss: 2.3065167151694985 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 430/1000 --- Train Loss: 2.300843716582905 --- Val Loss: 2.305151801958833 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/1000 --- Train Loss: 2.3011122960689767 --- Val Loss: 2.3055957220916468 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.301195421936828 --- Val Loss: 2.305442315224633 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/1000 --- Train Loss: 2.301112652414361 --- Val Loss: 2.3050065693660136 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/1000 --- Train Loss: 2.301255526621818 --- Val Loss: 2.301394968953006 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/1000 --- Train Loss: 2.300784389928719 --- Val Loss: 2.3055340425247786 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: 2.3013503438196614 --- Val Loss: 2.3071970432015774 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 500/1000 --- Train Loss: 2.3010482542682773 --- Val Loss: 2.305995412199893 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 510/1000 --- Train Loss: 2.300736108688473 --- Val Loss: 2.3054039330588023 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 520/1000 --- Train Loss: 2.3008490435855364 --- Val Loss: 2.302218921834617 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 530/1000 --- Train Loss: 2.3011083712166145 --- Val Loss: 2.3065832020698926 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 540/1000 --- Train Loss: 2.3005779703865596 --- Val Loss: 2.3043224603001464 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 550/1000 --- Train Loss: 2.3006759424247125 --- Val Loss: 2.304730666202051 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 560/1000 --- Train Loss: 2.3009904783311828 --- Val Loss: 2.3067946693329313 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 570/1000 --- Train Loss: 2.3014770749048536 --- Val Loss: 2.305928691988746 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 580/1000 --- Train Loss: 2.3014653045030338 --- Val Loss: 2.3074834331038847 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.300572514868058 --- Val Loss: 2.30334031241083 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 600/1000 --- Train Loss: 2.3127538350857173 --- Val Loss: 2.3049394975139084 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 610/1000 --- Train Loss: 2.300962392286273 --- Val Loss: 2.3061030145306067 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 620/1000 --- Train Loss: 2.300782731716182 --- Val Loss: 2.303668276952074 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 630/1000 --- Train Loss: 2.301069145212391 --- Val Loss: 2.303422162856178 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 640/1000 --- Train Loss: 2.3010836647357134 --- Val Loss: 2.304804546189932 --- Train Acc: 0.11 --- Val Acc: 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Temp\\ipykernel_18472\\118437895.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/100 --- Train Loss: 2.3024106212925717 --- Val Loss: 2.302216737361261 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/100 --- Train Loss: 2.3015663182424118 --- Val Loss: 2.2999466052478312 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/100 --- Train Loss: 2.301369848891938 --- Val Loss: 2.2991035305451186 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 30/100 --- Train Loss: 2.301321286705358 --- Val Loss: 2.2987225510391194 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 40/100 --- Train Loss: 2.3013108008566245 --- Val Loss: 2.2985128926843146 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 50/100 --- Train Loss: 2.301308404879275 --- Val Loss: 2.2984086244872244 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 60/100 --- Train Loss: 2.301307373507223 --- Val Loss: 2.2983558226519483 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/100 --- Train Loss: 2.3013070614357383 --- Val Loss: 2.2983594669727396 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/100 --- Train Loss: 2.301306713717216 --- Val Loss: 2.2983511344193888 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/100 --- Train Loss: 2.3013061501024588 --- Val Loss: 2.2983595959192225 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.08333333333333333\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3018928238612806 --- Val Loss: 2.323159514949902 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.325478265572882 --- Val Loss: 2.309987966932261 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 20/1000 --- Train Loss: 2.320112416332356 --- Val Loss: 2.312743165174037 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 30/1000 --- Train Loss: 2.30970639663684 --- Val Loss: 2.309050779648849 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 40/1000 --- Train Loss: 2.3159786415278765 --- Val Loss: 2.306904611961989 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 50/1000 --- Train Loss: 2.3190635733180796 --- Val Loss: 2.3024549348191714 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 60/1000 --- Train Loss: 2.3177789315220574 --- Val Loss: 2.309198872976596 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 70/1000 --- Train Loss: 2.3068121225263627 --- Val Loss: 2.306809015755503 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.3066538196279582 --- Val Loss: 2.305697934351053 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 90/1000 --- Train Loss: 2.3119133896881063 --- Val Loss: 2.3272860490066702 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 100/1000 --- Train Loss: 2.3092095228056806 --- Val Loss: 2.3127600316561026 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.308219330991792 --- Val Loss: 2.3085092170662276 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.309678689955943 --- Val Loss: 2.30563195493639 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.3204055851658687 --- Val Loss: 2.329868736532551 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/1000 --- Train Loss: 2.3163636909502188 --- Val Loss: 2.312228413503331 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 150/1000 --- Train Loss: 2.3143987790260545 --- Val Loss: 2.323235375936163 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/1000 --- Train Loss: 2.31484786795646 --- Val Loss: 2.3237188570678295 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.3099123393188106 --- Val Loss: 2.2987678887081007 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 180/1000 --- Train Loss: 2.311149121245321 --- Val Loss: 2.3058459914410543 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.3108000330894134 --- Val Loss: 2.305858877157076 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 200/1000 --- Train Loss: 2.309184403218868 --- Val Loss: 2.3013124548819874 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.311864088120272 --- Val Loss: 2.31782886857409 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/1000 --- Train Loss: 2.3128853115203656 --- Val Loss: 2.309873937614476 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.312206689713366 --- Val Loss: 2.318235786243831 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 240/1000 --- Train Loss: 2.3104109958024597 --- Val Loss: 2.3176999837382746 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/1000 --- Train Loss: 2.3274429626633966 --- Val Loss: 2.3206782581614513 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 260/1000 --- Train Loss: 2.311219528540183 --- Val Loss: 2.3005141391359865 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 270/1000 --- Train Loss: 2.3075572448646695 --- Val Loss: 2.3095132841756656 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.314293054547254 --- Val Loss: 2.316279742669379 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/1000 --- Train Loss: 2.310469687049343 --- Val Loss: 2.307537023487921 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3115569952348234 --- Val Loss: 2.3142332457157693 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/1000 --- Train Loss: 2.3138355443511003 --- Val Loss: 2.321393272678163 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: 2.312802315038286 --- Val Loss: 2.3157699372397396 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/1000 --- Train Loss: 2.307791342843622 --- Val Loss: 2.311807164856841 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/1000 --- Train Loss: 2.315329159999571 --- Val Loss: 2.315507040509022 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.3163191908250558 --- Val Loss: 2.336692537534398 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 360/1000 --- Train Loss: 2.319019983206032 --- Val Loss: 2.3063152202923347 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 370/1000 --- Train Loss: 2.3115384834278854 --- Val Loss: 2.3213149659432446 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 380/1000 --- Train Loss: 2.3147726672556246 --- Val Loss: 2.3080737592531437 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.3217181935853155 --- Val Loss: 2.321348436513617 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3160499814657514 --- Val Loss: 2.3205930237772603 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 410/1000 --- Train Loss: 2.315655903799268 --- Val Loss: 2.3256619856906324 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/1000 --- Train Loss: 2.315658483509644 --- Val Loss: 2.314330932500924 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.309515108686023 --- Val Loss: 2.303671570297056 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 440/1000 --- Train Loss: 2.3118501184275195 --- Val Loss: 2.308726243738337 --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 450/1000 --- Train Loss: 2.305885156844148 --- Val Loss: 2.310360785446751 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/1000 --- Train Loss: 2.3083482880929713 --- Val Loss: 2.3078219263051842 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.3093701643126265 --- Val Loss: 2.3074801328555976 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.311632069776053 --- Val Loss: 2.317086552189625 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/1000 --- Train Loss: 2.3143800310038336 --- Val Loss: 2.3143396782645693 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.3016326501532087 --- Val Loss: 2.301418178297649 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/500 --- Train Loss: 0.31657419697355454 --- Val Loss: 0.07186982556892371 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 20/500 --- Train Loss: 0.9453974938187758 --- Val Loss: 0.5927111186934897 --- Train Acc: 0.74 --- Val Acc: 0.75\n",
      "Epoch 30/500 --- Train Loss: 2.2264313567493232 --- Val Loss: 2.212829309917095 --- Train Acc: 0.14 --- Val Acc: 0.16\n",
      "Epoch 40/500 --- Train Loss: 2.2290216343602305 --- Val Loss: 2.1539809056467516 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 50/500 --- Train Loss: 2.2952445852045664 --- Val Loss: 2.2994346546357765 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 60/500 --- Train Loss: 2.3012413048857234 --- Val Loss: 2.3007576353033694 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 70/500 --- Train Loss: 2.29935522354202 --- Val Loss: 2.2986301555516344 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: 2.31344132535743 --- Val Loss: 2.302784614377221 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 90/500 --- Train Loss: 2.3014796211562647 --- Val Loss: 2.3005391092515817 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 100/500 --- Train Loss: 2.3013328431736872 --- Val Loss: 2.30121210305001 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: 2.301288828527852 --- Val Loss: 2.3002527877027577 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: 2.30200557562934 --- Val Loss: 2.3010602092574297 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 130/500 --- Train Loss: 2.3013082466055974 --- Val Loss: 2.2984934799150496 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 140/500 --- Train Loss: 2.3012908102604372 --- Val Loss: 2.2999777970136197 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 150/500 --- Train Loss: 2.30124307549727 --- Val Loss: 2.2996505424005016 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 160/500 --- Train Loss: 2.3012974460526747 --- Val Loss: 2.3009180426753537 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 170/500 --- Train Loss: 2.3013260346051108 --- Val Loss: 2.3006794796256487 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 180/500 --- Train Loss: 2.301254283283899 --- Val Loss: 2.300618095635835 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: 2.3013155344026206 --- Val Loss: 2.299520051201752 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 200/500 --- Train Loss: 2.3132499329921528 --- Val Loss: 2.2982685038221824 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 210/500 --- Train Loss: 2.3013286876825965 --- Val Loss: 2.29939199038239 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 220/500 --- Train Loss: 2.3013394692008378 --- Val Loss: 2.300054744794156 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: 2.301329559435048 --- Val Loss: 2.300659621617173 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 240/500 --- Train Loss: 2.301275549586047 --- Val Loss: 2.299308176845467 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 250/500 --- Train Loss: 2.313273113363269 --- Val Loss: 2.300981456016297 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 260/500 --- Train Loss: 2.3013067492051285 --- Val Loss: 2.3007647539709106 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 270/500 --- Train Loss: 2.3013889435451276 --- Val Loss: 2.2997474401622693 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 280/500 --- Train Loss: 2.3012890018714547 --- Val Loss: 2.2994091253109348 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: 2.301252434965879 --- Val Loss: 2.299254047302315 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 300/500 --- Train Loss: 2.301349991322966 --- Val Loss: 2.299145121524319 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 310/500 --- Train Loss: 2.30148535324974 --- Val Loss: 2.3032331535896686 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 320/500 --- Train Loss: 2.301258912512214 --- Val Loss: 2.2997278653032507 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 330/500 --- Train Loss: 2.301274762852353 --- Val Loss: 2.3011893075251804 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 340/500 --- Train Loss: 2.301358704609705 --- Val Loss: 2.298308360815763 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: 2.3013271021126522 --- Val Loss: 2.299465347529512 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 360/500 --- Train Loss: 2.3013001468649787 --- Val Loss: 2.3007408529431497 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 370/500 --- Train Loss: 2.3012567857973774 --- Val Loss: 2.3002992085305736 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 380/500 --- Train Loss: 2.301281595971586 --- Val Loss: 2.3007937761305706 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 390/500 --- Train Loss: 2.30121835207767 --- Val Loss: 2.299375850240455 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: 2.301287703127459 --- Val Loss: 2.3000197143623375 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 410/500 --- Train Loss: 2.301353234571229 --- Val Loss: 2.2996802683910342 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 420/500 --- Train Loss: 2.301233546884303 --- Val Loss: 2.299996926598933 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 430/500 --- Train Loss: 2.3013886317514096 --- Val Loss: 2.2998878897990775 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 440/500 --- Train Loss: 2.301265609783139 --- Val Loss: 2.3007218862596286 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 450/500 --- Train Loss: 2.301253457637985 --- Val Loss: 2.300504758044334 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 460/500 --- Train Loss: 2.301423729936473 --- Val Loss: 2.3013447916822742 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 470/500 --- Train Loss: 2.3014197750846668 --- Val Loss: 2.2991934078153693 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 480/500 --- Train Loss: 2.3012790950664606 --- Val Loss: 2.2995749265172476 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 490/500 --- Train Loss: 2.301377241469268 --- Val Loss: 2.3020186669050138 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3023165643519623 --- Val Loss: 2.301901029034731 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.2215030509704436 --- Val Loss: 2.2191969029448555 --- Train Acc: 0.21 --- Val Acc: 0.21\n",
      "Epoch 20/1000 --- Train Loss: 0.5529103865686866 --- Val Loss: 0.35951479784304247 --- Train Acc: 0.91 --- Val Acc: 0.90\n",
      "Epoch 30/1000 --- Train Loss: 0.3558122312412738 --- Val Loss: 0.18094261271352924 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 40/1000 --- Train Loss: 0.373069355088886 --- Val Loss: 0.1542616345635649 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 50/1000 --- Train Loss: 0.3524495443420918 --- Val Loss: 0.12601173866394297 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 60/1000 --- Train Loss: 0.4102320100616763 --- Val Loss: 0.15804677665611874 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 70/1000 --- Train Loss: 0.3546035936570288 --- Val Loss: 0.105712740716284 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 80/1000 --- Train Loss: 0.32507345002269483 --- Val Loss: 0.08398587295581246 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 90/1000 --- Train Loss: 0.33027691289117633 --- Val Loss: 0.07730440031550771 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 100/1000 --- Train Loss: 0.3674962222571035 --- Val Loss: 0.0798341902903426 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 110/1000 --- Train Loss: 0.32567522859853143 --- Val Loss: 0.09383167843709786 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 120/1000 --- Train Loss: 0.38506254959012604 --- Val Loss: 0.10298380637802575 --- Train Acc: 0.97 --- Val Acc: 0.95\n",
      "Epoch 130/1000 --- Train Loss: 0.377143009294168 --- Val Loss: 0.09695627640558968 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 140/1000 --- Train Loss: 0.4070236034450674 --- Val Loss: 0.15497279869503452 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 150/1000 --- Train Loss: 0.5361594937916623 --- Val Loss: 0.19351782994497557 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 160/1000 --- Train Loss: 0.3871980262938787 --- Val Loss: 0.13578946828660257 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 170/1000 --- Train Loss: 0.38001230487829113 --- Val Loss: 0.1217179419329326 --- Train Acc: 0.97 --- Val Acc: 0.95\n",
      "Epoch 180/1000 --- Train Loss: 0.3645783445021559 --- Val Loss: 0.11694625798847606 --- Train Acc: 0.97 --- Val Acc: 0.98\n",
      "Epoch 190/1000 --- Train Loss: 0.4542480369797017 --- Val Loss: 0.1327800063666425 --- Train Acc: 0.95 --- Val Acc: 0.96\n",
      "Epoch 200/1000 --- Train Loss: 0.491229138054629 --- Val Loss: 0.13927912638415976 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 210/1000 --- Train Loss: 0.5646886048539781 --- Val Loss: 0.20692601300637634 --- Train Acc: 0.94 --- Val Acc: 0.92\n",
      "Epoch 220/1000 --- Train Loss: 0.4974046554538077 --- Val Loss: 0.18757595915561728 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 230/1000 --- Train Loss: 0.5081539310089256 --- Val Loss: 0.17597950892553432 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 240/1000 --- Train Loss: 0.47331826595518006 --- Val Loss: 0.21965881937656836 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 250/1000 --- Train Loss: 0.5026878492874572 --- Val Loss: 0.2296941981186679 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 260/1000 --- Train Loss: 0.589931448966363 --- Val Loss: 0.31390385824339684 --- Train Acc: 0.91 --- Val Acc: 0.88\n",
      "Epoch 270/1000 --- Train Loss: 0.6254985508430587 --- Val Loss: 0.1997220793052185 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 280/1000 --- Train Loss: 0.6552761423404107 --- Val Loss: 0.26024027414762274 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 290/1000 --- Train Loss: 0.5670536367204057 --- Val Loss: 0.2400405312784644 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 300/1000 --- Train Loss: 0.6187529977273974 --- Val Loss: 0.25362675970000154 --- Train Acc: 0.94 --- Val Acc: 0.93\n",
      "Epoch 310/1000 --- Train Loss: 0.62786822044136 --- Val Loss: 0.2967246192654267 --- Train Acc: 0.91 --- Val Acc: 0.92\n",
      "Epoch 320/1000 --- Train Loss: 0.5971985984906765 --- Val Loss: 0.33504172640695895 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 330/1000 --- Train Loss: 0.8803770415155867 --- Val Loss: 0.5251071808942918 --- Train Acc: 0.82 --- Val Acc: 0.80\n",
      "Epoch 340/1000 --- Train Loss: 0.7993445352282813 --- Val Loss: 0.36493752618440384 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 350/1000 --- Train Loss: 0.715879892256107 --- Val Loss: 0.4155034240023509 --- Train Acc: 0.86 --- Val Acc: 0.88\n",
      "Epoch 360/1000 --- Train Loss: 0.8254165390916053 --- Val Loss: 0.40107437446979394 --- Train Acc: 0.87 --- Val Acc: 0.88\n",
      "Epoch 370/1000 --- Train Loss: 0.9731249893438155 --- Val Loss: 0.6039971865328639 --- Train Acc: 0.80 --- Val Acc: 0.82\n",
      "Epoch 380/1000 --- Train Loss: 0.9301499153632249 --- Val Loss: 0.6053144919961592 --- Train Acc: 0.84 --- Val Acc: 0.84\n",
      "Epoch 390/1000 --- Train Loss: 0.9218155244471692 --- Val Loss: 0.65517045247764 --- Train Acc: 0.78 --- Val Acc: 0.79\n",
      "Epoch 400/1000 --- Train Loss: 0.9391950446326904 --- Val Loss: 0.6133161187749078 --- Train Acc: 0.76 --- Val Acc: 0.77\n",
      "Epoch 410/1000 --- Train Loss: 1.2169088083450634 --- Val Loss: 0.8798490060692673 --- Train Acc: 0.69 --- Val Acc: 0.71\n",
      "Epoch 420/1000 --- Train Loss: 1.45210809896451 --- Val Loss: 0.9837118656179463 --- Train Acc: 0.62 --- Val Acc: 0.66\n",
      "Epoch 430/1000 --- Train Loss: 1.856110022829117 --- Val Loss: 1.8305124487587605 --- Train Acc: 0.47 --- Val Acc: 0.46\n",
      "Epoch 440/1000 --- Train Loss: 1.3349506448257777 --- Val Loss: 1.0097505732626244 --- Train Acc: 0.64 --- Val Acc: 0.64\n",
      "Epoch 450/1000 --- Train Loss: 1.4290488470744174 --- Val Loss: 1.207461474359543 --- Train Acc: 0.56 --- Val Acc: 0.57\n",
      "Epoch 460/1000 --- Train Loss: 1.3325773014762845 --- Val Loss: 1.094439043303065 --- Train Acc: 0.60 --- Val Acc: 0.60\n",
      "Epoch 470/1000 --- Train Loss: 1.577380784013806 --- Val Loss: 1.2564045198200406 --- Train Acc: 0.52 --- Val Acc: 0.55\n",
      "Epoch 480/1000 --- Train Loss: 1.4529088823134098 --- Val Loss: 1.1585486029575778 --- Train Acc: 0.61 --- Val Acc: 0.59\n",
      "Epoch 490/1000 --- Train Loss: 1.809640756617401 --- Val Loss: 1.5909510915245149 --- Train Acc: 0.41 --- Val Acc: 0.41\n",
      "Epoch 500/1000 --- Train Loss: 1.595868219541461 --- Val Loss: 1.382620480214558 --- Train Acc: 0.47 --- Val Acc: 0.48\n",
      "Epoch 510/1000 --- Train Loss: 1.7418884698484896 --- Val Loss: 1.5130948943100835 --- Train Acc: 0.41 --- Val Acc: 0.43\n",
      "Epoch 520/1000 --- Train Loss: 1.6845650401248302 --- Val Loss: 1.4981330933218988 --- Train Acc: 0.41 --- Val Acc: 0.45\n",
      "Epoch 530/1000 --- Train Loss: 1.7246809804906964 --- Val Loss: 1.4384702758220385 --- Train Acc: 0.46 --- Val Acc: 0.47\n",
      "Epoch 540/1000 --- Train Loss: 1.7885743853079574 --- Val Loss: 1.5957761814650453 --- Train Acc: 0.39 --- Val Acc: 0.43\n",
      "Epoch 550/1000 --- Train Loss: 1.7844315682937149 --- Val Loss: 1.7010207621590745 --- Train Acc: 0.37 --- Val Acc: 0.38\n",
      "Epoch 560/1000 --- Train Loss: 1.7173799681287445 --- Val Loss: 1.6905682016804797 --- Train Acc: 0.36 --- Val Acc: 0.39\n",
      "Epoch 570/1000 --- Train Loss: 1.9539576150711668 --- Val Loss: 1.8530172713875996 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 580/1000 --- Train Loss: 2.123377747882572 --- Val Loss: 2.170207553711024 --- Train Acc: 0.18 --- Val Acc: 0.18\n",
      "Epoch 590/1000 --- Train Loss: 2.0742403076544145 --- Val Loss: 2.1097169483885896 --- Train Acc: 0.20 --- Val Acc: 0.21\n",
      "Epoch 600/1000 --- Train Loss: 2.125272847793129 --- Val Loss: 2.1321679899328934 --- Train Acc: 0.19 --- Val Acc: 0.21\n",
      "Epoch 610/1000 --- Train Loss: 2.094362385566904 --- Val Loss: 2.113146845315762 --- Train Acc: 0.21 --- Val Acc: 0.21\n",
      "Epoch 620/1000 --- Train Loss: 2.1985032940983436 --- Val Loss: 2.2224941667495903 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 630/1000 --- Train Loss: 2.033468991315505 --- Val Loss: 1.9509162302731082 --- Train Acc: 0.26 --- Val Acc: 0.27\n",
      "Epoch 640/1000 --- Train Loss: 2.1912110094270134 --- Val Loss: 2.175180634375599 --- Train Acc: 0.16 --- Val Acc: 0.18\n",
      "Epoch 650/1000 --- Train Loss: 2.028826232895809 --- Val Loss: 2.0290487853256995 --- Train Acc: 0.23 --- Val Acc: 0.24\n",
      "Epoch 660/1000 --- Train Loss: 2.063399060633805 --- Val Loss: 1.9765890050583146 --- Train Acc: 0.23 --- Val Acc: 0.26\n",
      "Epoch 670/1000 --- Train Loss: 2.2393785910060235 --- Val Loss: 2.2631003520435744 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 680/1000 --- Train Loss: 2.0881619991674034 --- Val Loss: 2.0858967904380967 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Epoch 690/1000 --- Train Loss: 2.4655445096204485 --- Val Loss: 2.375313874056301 --- Train Acc: 0.16 --- Val Acc: 0.19\n",
      "Epoch 700/1000 --- Train Loss: 2.155489920618325 --- Val Loss: 2.197035914952971 --- Train Acc: 0.17 --- Val Acc: 0.17\n",
      "Epoch 710/1000 --- Train Loss: 2.0688707195767733 --- Val Loss: 2.046609935155227 --- Train Acc: 0.21 --- Val Acc: 0.23\n",
      "Epoch 720/1000 --- Train Loss: 2.1415461636999322 --- Val Loss: 2.16781722331505 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 730/1000 --- Train Loss: 2.2071538663989076 --- Val Loss: 2.2309575154505557 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 740/1000 --- Train Loss: 2.2261173188833125 --- Val Loss: 2.2687924347288297 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 750/1000 --- Train Loss: 2.1601703166308144 --- Val Loss: 2.1502467630848168 --- Train Acc: 0.18 --- Val Acc: 0.18\n",
      "Epoch 760/1000 --- Train Loss: 2.151686361956339 --- Val Loss: 2.17325641663025 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 770/1000 --- Train Loss: 2.167501763389261 --- Val Loss: 2.1895679263702426 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 780/1000 --- Train Loss: 2.2239731098603315 --- Val Loss: 2.2603625190230487 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 790/1000 --- Train Loss: 2.1927408376511544 --- Val Loss: 2.2203266272882956 --- Train Acc: 0.16 --- Val Acc: 0.16\n",
      "Epoch 800/1000 --- Train Loss: 2.2192536820816597 --- Val Loss: 2.1630305494341227 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 810/1000 --- Train Loss: 2.2749635610381276 --- Val Loss: 2.2843656581815734 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 820/1000 --- Train Loss: 2.285080896592853 --- Val Loss: 2.2745834051227685 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 830/1000 --- Train Loss: 2.2728699392414726 --- Val Loss: 2.2913641793788395 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: 2.2621190349858225 --- Val Loss: 2.2836763561720304 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: 2.2247740456818774 --- Val Loss: 2.26847359911136 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 860/1000 --- Train Loss: 2.1978135319285723 --- Val Loss: 2.2272926594927274 --- Train Acc: 0.15 --- Val Acc: 0.16\n",
      "Epoch 870/1000 --- Train Loss: 2.193439427498554 --- Val Loss: 2.237600634824141 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 880/1000 --- Train Loss: 2.3007553474482263 --- Val Loss: 2.290541899238107 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: 2.2351681824106477 --- Val Loss: 2.2880103513849064 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 900/1000 --- Train Loss: 2.2462674137191363 --- Val Loss: 2.2717006825308146 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 910/1000 --- Train Loss: 2.2675519191014852 --- Val Loss: 2.2925124912962813 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.2914790551209183 --- Val Loss: 2.2926769150718234 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.2932794312867864 --- Val Loss: 2.299651513669305 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 940/1000 --- Train Loss: 2.261884467988211 --- Val Loss: 2.2589255165739823 --- Train Acc: 0.12 --- Val Acc: 0.14\n",
      "Epoch 950/1000 --- Train Loss: 2.289558753395573 --- Val Loss: 2.299923706343204 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.2933972664132285 --- Val Loss: 2.2989595278042216 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.2936314355896386 --- Val Loss: 2.298913185874653 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.301751412503566 --- Val Loss: 2.299219225843645 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: 2.2997107168674753 --- Val Loss: 2.2992347516300735 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 0.7739879965207019 --- Val Loss: 0.7514227307620897 --- Train Acc: 0.79 --- Val Acc: 0.77\n",
      "Epoch 10/500 --- Train Loss: 2.33336060549586 --- Val Loss: 2.292949065921693 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 20/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 30/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 40/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 60/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 70/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 80/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 90/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 100/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 110/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/500 --- Train Loss: 2.3018639907714658 --- Val Loss: 2.3034469901039314 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 1.3805890499213533 --- Val Loss: 1.3460612858647736 --- Train Acc: 0.48 --- Val Acc: 0.51\n",
      "Epoch 20/500 --- Train Loss: 1.4290593221180594 --- Val Loss: 1.1618796985881656 --- Train Acc: 0.50 --- Val Acc: 0.53\n",
      "Epoch 30/500 --- Train Loss: 1.8141914010430036 --- Val Loss: 1.6803786918165622 --- Train Acc: 0.31 --- Val Acc: 0.34\n",
      "Epoch 40/500 --- Train Loss: 2.1917067367034573 --- Val Loss: 2.1709030046865214 --- Train Acc: 0.16 --- Val Acc: 0.15\n",
      "Epoch 50/500 --- Train Loss: 2.1965279633882497 --- Val Loss: 2.1677442502848616 --- Train Acc: 0.15 --- Val Acc: 0.14\n",
      "Epoch 60/500 --- Train Loss: 2.3014593575066664 --- Val Loss: 2.3016588488212917 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/500 --- Train Loss: 2.3017890819718754 --- Val Loss: 2.3016510896126965 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/500 --- Train Loss: 2.3017286294907233 --- Val Loss: 2.30212031140239 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 90/500 --- Train Loss: 2.3016689662535375 --- Val Loss: 2.30368866082691 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/500 --- Train Loss: 2.3018333817757695 --- Val Loss: 2.301894223752777 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/500 --- Train Loss: 2.301954774977536 --- Val Loss: 2.3055440398353473 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/500 --- Train Loss: 2.301759473853421 --- Val Loss: 2.3036433000261867 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/500 --- Train Loss: 2.3015186478916836 --- Val Loss: 2.304569537821071 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/500 --- Train Loss: 2.30169238841491 --- Val Loss: 2.3025560372913203 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/500 --- Train Loss: 2.3379028282705776 --- Val Loss: 2.3002613591951344 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 160/500 --- Train Loss: 2.3015705234548127 --- Val Loss: 2.2999946809722966 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/500 --- Train Loss: 2.301438002852126 --- Val Loss: 2.3050869822424493 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/500 --- Train Loss: 2.3019458544526508 --- Val Loss: 2.3033540585602563 --- Train Acc: 0.10 --- Val Acc: 0.08\n",
      "Epoch 190/500 --- Train Loss: 2.3017140434158785 --- Val Loss: 2.3036333427499907 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/500 --- Train Loss: 2.301714968501476 --- Val Loss: 2.300551606299449 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/500 --- Train Loss: 2.3019453350701706 --- Val Loss: 2.3060729979309142 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 220/500 --- Train Loss: 2.3019234424414576 --- Val Loss: 2.3025390823096 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/500 --- Train Loss: 2.302060867856521 --- Val Loss: 2.3026622985864154 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 240/500 --- Train Loss: 2.302073953654829 --- Val Loss: 2.3024224094697527 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 250/500 --- Train Loss: 2.3016839082065763 --- Val Loss: 2.3043058407029746 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/500 --- Train Loss: 2.3014800464837193 --- Val Loss: 2.3051052757745087 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/500 --- Train Loss: 2.301834925391515 --- Val Loss: 2.307428100644557 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/500 --- Train Loss: 2.302057510183354 --- Val Loss: 2.3004847866919316 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 290/500 --- Train Loss: 2.301576453101188 --- Val Loss: 2.302917299503246 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 300/500 --- Train Loss: 2.3015677017349407 --- Val Loss: 2.304886452395461 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/500 --- Train Loss: 2.3016900830824474 --- Val Loss: 2.3014290382993456 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 320/500 --- Train Loss: 2.3019588327710983 --- Val Loss: 2.305466528595064 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/500 --- Train Loss: 2.3019441531288733 --- Val Loss: 2.3050462554018054 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: 2.301490896624571 --- Val Loss: 2.3001493658994785 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: 2.301605206982127 --- Val Loss: 2.3051886635251795 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: 2.301579974981105 --- Val Loss: 2.301969431082256 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: 2.3016321826012027 --- Val Loss: 2.30220539814672 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: 2.3020650927505093 --- Val Loss: 2.304708543366089 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: 2.301722487901891 --- Val Loss: 2.3023713831443398 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: 2.3014525578086924 --- Val Loss: 2.302343809162234 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: 2.30152816509436 --- Val Loss: 2.301858262028637 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: 2.3018930297295275 --- Val Loss: 2.303915359287709 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: 2.301432081975674 --- Val Loss: 2.304107182309778 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: 2.302178808293358 --- Val Loss: 2.300318388890979 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 450/500 --- Train Loss: 2.3013158475413333 --- Val Loss: 2.3018101813851057 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: 2.3019528111620127 --- Val Loss: 2.3068353954662384 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: 2.3016816647042324 --- Val Loss: 2.301691166697031 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: 2.301508703072638 --- Val Loss: 2.3028081361160857 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: 2.3021557947603446 --- Val Loss: 2.3057429544421617 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.5, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "32\n",
      "Epoch 0/100 --- Train Loss: 2.3024281605426995 --- Val Loss: 2.302437986600432 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/100 --- Train Loss: 2.300557101756885 --- Val Loss: 2.3002998323537516 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/100 --- Train Loss: 2.287512967901329 --- Val Loss: 2.287310099283949 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/100 --- Train Loss: 2.1403263802907952 --- Val Loss: 2.144041874137602 --- Train Acc: 0.41 --- Val Acc: 0.38\n",
      "Epoch 40/100 --- Train Loss: 1.4380969166377648 --- Val Loss: 1.3790799907849176 --- Train Acc: 0.62 --- Val Acc: 0.59\n",
      "Epoch 50/100 --- Train Loss: 0.844806674825517 --- Val Loss: 0.7168862426844814 --- Train Acc: 0.81 --- Val Acc: 0.81\n",
      "Epoch 60/100 --- Train Loss: 0.6657287471931461 --- Val Loss: 0.41586547693172693 --- Train Acc: 0.88 --- Val Acc: 0.89\n",
      "Epoch 70/100 --- Train Loss: 0.5266965777011677 --- Val Loss: 0.2744028284330164 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 80/100 --- Train Loss: 0.4423826203936505 --- Val Loss: 0.20964923856319362 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 90/100 --- Train Loss: 0.43354555335686906 --- Val Loss: 0.17174985255106537 --- Train Acc: 0.94 --- Val Acc: 0.95\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.9222222222222223\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3021807931980387 --- Val Loss: 2.302185667488222 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.1972493174779304 --- Val Loss: 2.1777097345975216 --- Train Acc: 0.16 --- Val Acc: 0.18\n",
      "Epoch 20/1000 --- Train Loss: 0.5799178579640883 --- Val Loss: 0.4169147865109239 --- Train Acc: 0.90 --- Val Acc: 0.89\n",
      "Epoch 30/1000 --- Train Loss: 0.4404938208901422 --- Val Loss: 0.2879811091237247 --- Train Acc: 0.92 --- Val Acc: 0.89\n",
      "Epoch 40/1000 --- Train Loss: 0.39367989902000006 --- Val Loss: 0.16223637686168485 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 50/1000 --- Train Loss: 0.39421749011617796 --- Val Loss: 0.1649266348470845 --- Train Acc: 0.94 --- Val Acc: 0.94\n",
      "Epoch 60/1000 --- Train Loss: 0.31812657977221015 --- Val Loss: 0.11824254028203847 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 70/1000 --- Train Loss: 0.29665506121032637 --- Val Loss: 0.10772039758175418 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 80/1000 --- Train Loss: 0.45319018347096546 --- Val Loss: 0.1904158896172431 --- Train Acc: 0.97 --- Val Acc: 0.95\n",
      "Epoch 90/1000 --- Train Loss: 0.34719607391992124 --- Val Loss: 0.17681272805910095 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 100/1000 --- Train Loss: 0.317815130033308 --- Val Loss: 0.10215147573967213 --- Train Acc: 0.98 --- Val Acc: 0.95\n",
      "Epoch 110/1000 --- Train Loss: 0.41734856726099834 --- Val Loss: 0.1310367962041764 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 120/1000 --- Train Loss: 0.3700291174250176 --- Val Loss: 0.09497600137975098 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 130/1000 --- Train Loss: 0.3356571765628749 --- Val Loss: 0.12183186238806484 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 140/1000 --- Train Loss: 0.3760284828583589 --- Val Loss: 0.1561155018024246 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 150/1000 --- Train Loss: 0.43590095276220203 --- Val Loss: 0.11848206908378321 --- Train Acc: 0.97 --- Val Acc: 0.96\n",
      "Epoch 160/1000 --- Train Loss: 0.5082708777224899 --- Val Loss: 0.18254767827655605 --- Train Acc: 0.95 --- Val Acc: 0.93\n",
      "Epoch 170/1000 --- Train Loss: 0.7184539816290564 --- Val Loss: 0.37093513231989994 --- Train Acc: 0.92 --- Val Acc: 0.91\n",
      "Epoch 180/1000 --- Train Loss: 0.3907947623150461 --- Val Loss: 0.10873674641809504 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 190/1000 --- Train Loss: 0.7809988899641771 --- Val Loss: 0.49237210770210266 --- Train Acc: 0.90 --- Val Acc: 0.87\n",
      "Epoch 200/1000 --- Train Loss: 0.4229314488922765 --- Val Loss: 0.1443126108766041 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 210/1000 --- Train Loss: 0.6928946385806081 --- Val Loss: 0.32100194271829824 --- Train Acc: 0.90 --- Val Acc: 0.88\n",
      "Epoch 220/1000 --- Train Loss: 0.5037912241053631 --- Val Loss: 0.16596017214211473 --- Train Acc: 0.96 --- Val Acc: 0.95\n",
      "Epoch 230/1000 --- Train Loss: 0.43615745791502253 --- Val Loss: 0.20796674480574998 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 240/1000 --- Train Loss: 0.4809992003422831 --- Val Loss: 0.21796996884543654 --- Train Acc: 0.95 --- Val Acc: 0.95\n",
      "Epoch 250/1000 --- Train Loss: 0.551720058230122 --- Val Loss: 0.2097325666455816 --- Train Acc: 0.95 --- Val Acc: 0.94\n",
      "Epoch 260/1000 --- Train Loss: 0.5490055930777469 --- Val Loss: 0.1853681184949362 --- Train Acc: 0.96 --- Val Acc: 0.94\n",
      "Epoch 270/1000 --- Train Loss: 0.5373756616882309 --- Val Loss: 0.17467242855004866 --- Train Acc: 0.96 --- Val Acc: 0.96\n",
      "Epoch 280/1000 --- Train Loss: 0.5337293669293727 --- Val Loss: 0.2635993795844369 --- Train Acc: 0.93 --- Val Acc: 0.94\n",
      "Epoch 290/1000 --- Train Loss: 0.7681025684130924 --- Val Loss: 0.5227153664861298 --- Train Acc: 0.92 --- Val Acc: 0.89\n",
      "Epoch 300/1000 --- Train Loss: 0.6191903755475938 --- Val Loss: 0.36625077343283197 --- Train Acc: 0.90 --- Val Acc: 0.88\n",
      "Epoch 310/1000 --- Train Loss: 0.6765229597327199 --- Val Loss: 0.42436167809021536 --- Train Acc: 0.87 --- Val Acc: 0.85\n",
      "Epoch 320/1000 --- Train Loss: 0.9319469136864598 --- Val Loss: 0.62790475070814 --- Train Acc: 0.86 --- Val Acc: 0.82\n",
      "Epoch 330/1000 --- Train Loss: 0.7964090298263744 --- Val Loss: 0.6042657710724795 --- Train Acc: 0.85 --- Val Acc: 0.80\n",
      "Epoch 340/1000 --- Train Loss: 0.7885105873215948 --- Val Loss: 0.46892197704594324 --- Train Acc: 0.89 --- Val Acc: 0.85\n",
      "Epoch 350/1000 --- Train Loss: 0.9300588813538564 --- Val Loss: 0.7764022142272188 --- Train Acc: 0.80 --- Val Acc: 0.75\n",
      "Epoch 360/1000 --- Train Loss: 0.8055469526735441 --- Val Loss: 0.4365712541155492 --- Train Acc: 0.91 --- Val Acc: 0.89\n",
      "Epoch 370/1000 --- Train Loss: 0.9109776102933831 --- Val Loss: 0.5377352137680034 --- Train Acc: 0.82 --- Val Acc: 0.82\n",
      "Epoch 380/1000 --- Train Loss: 0.8683673753889323 --- Val Loss: 0.65471401881525 --- Train Acc: 0.83 --- Val Acc: 0.78\n",
      "Epoch 390/1000 --- Train Loss: 1.0388546492940323 --- Val Loss: 0.8983295446845379 --- Train Acc: 0.75 --- Val Acc: 0.69\n",
      "Epoch 400/1000 --- Train Loss: 1.0946574614763322 --- Val Loss: 0.7983859347767717 --- Train Acc: 0.74 --- Val Acc: 0.74\n",
      "Epoch 410/1000 --- Train Loss: 1.157100632766281 --- Val Loss: 1.1303583036532567 --- Train Acc: 0.69 --- Val Acc: 0.66\n",
      "Epoch 420/1000 --- Train Loss: 1.125075712989028 --- Val Loss: 1.0356778923537582 --- Train Acc: 0.69 --- Val Acc: 0.67\n",
      "Epoch 430/1000 --- Train Loss: 1.3007249446958016 --- Val Loss: 0.9527755112192848 --- Train Acc: 0.70 --- Val Acc: 0.70\n",
      "Epoch 440/1000 --- Train Loss: 1.7432786110858347 --- Val Loss: 1.621091804524495 --- Train Acc: 0.43 --- Val Acc: 0.44\n",
      "Epoch 450/1000 --- Train Loss: 1.635618545386414 --- Val Loss: 1.6326400645143118 --- Train Acc: 0.45 --- Val Acc: 0.40\n",
      "Epoch 460/1000 --- Train Loss: 1.6405899510495587 --- Val Loss: 1.586349950716683 --- Train Acc: 0.46 --- Val Acc: 0.43\n",
      "Epoch 470/1000 --- Train Loss: 1.6401307579097206 --- Val Loss: 1.5453818297596564 --- Train Acc: 0.46 --- Val Acc: 0.43\n",
      "Epoch 480/1000 --- Train Loss: 1.7116211549923177 --- Val Loss: 1.6989943969635009 --- Train Acc: 0.39 --- Val Acc: 0.35\n",
      "Epoch 490/1000 --- Train Loss: 1.7028703680854382 --- Val Loss: 1.5703513288437865 --- Train Acc: 0.43 --- Val Acc: 0.41\n",
      "Epoch 500/1000 --- Train Loss: 1.6528969875591695 --- Val Loss: 1.6353398582249616 --- Train Acc: 0.42 --- Val Acc: 0.38\n",
      "Epoch 510/1000 --- Train Loss: 2.538472291772955 --- Val Loss: 2.310554710773526 --- Train Acc: 0.23 --- Val Acc: 0.24\n",
      "Epoch 520/1000 --- Train Loss: 1.941185165481802 --- Val Loss: 2.002313829367033 --- Train Acc: 0.28 --- Val Acc: 0.24\n",
      "Epoch 530/1000 --- Train Loss: 1.764578665652838 --- Val Loss: 1.728062675014308 --- Train Acc: 0.41 --- Val Acc: 0.36\n",
      "Epoch 540/1000 --- Train Loss: 1.6734857877787286 --- Val Loss: 1.6545597254293822 --- Train Acc: 0.42 --- Val Acc: 0.38\n",
      "Epoch 550/1000 --- Train Loss: 1.6800534976725983 --- Val Loss: 1.5727823986751839 --- Train Acc: 0.43 --- Val Acc: 0.42\n",
      "Epoch 560/1000 --- Train Loss: 1.801506400295687 --- Val Loss: 1.740373341683933 --- Train Acc: 0.38 --- Val Acc: 0.34\n",
      "Epoch 570/1000 --- Train Loss: 1.996527718847683 --- Val Loss: 1.9875886208990334 --- Train Acc: 0.26 --- Val Acc: 0.24\n",
      "Epoch 580/1000 --- Train Loss: 1.9564627119214508 --- Val Loss: 1.9211857767409493 --- Train Acc: 0.27 --- Val Acc: 0.27\n",
      "Epoch 590/1000 --- Train Loss: 2.035138271553473 --- Val Loss: 2.0226825550372842 --- Train Acc: 0.23 --- Val Acc: 0.23\n",
      "Epoch 600/1000 --- Train Loss: 2.0542354435969132 --- Val Loss: 2.0276725869881944 --- Train Acc: 0.22 --- Val Acc: 0.22\n",
      "Epoch 610/1000 --- Train Loss: 2.0522330013310546 --- Val Loss: 2.0482214167306037 --- Train Acc: 0.21 --- Val Acc: 0.22\n",
      "Epoch 620/1000 --- Train Loss: 2.066584748472134 --- Val Loss: 2.0223697350962393 --- Train Acc: 0.23 --- Val Acc: 0.23\n",
      "Epoch 630/1000 --- Train Loss: 1.9967892833553427 --- Val Loss: 1.9946410949828977 --- Train Acc: 0.26 --- Val Acc: 0.24\n",
      "Epoch 640/1000 --- Train Loss: 2.0559817931563202 --- Val Loss: 2.0416572171795737 --- Train Acc: 0.24 --- Val Acc: 0.22\n",
      "Epoch 650/1000 --- Train Loss: 2.0298131596850677 --- Val Loss: 1.9868344638903546 --- Train Acc: 0.25 --- Val Acc: 0.25\n",
      "Epoch 660/1000 --- Train Loss: 2.090302590979947 --- Val Loss: 2.061338533091242 --- Train Acc: 0.20 --- Val Acc: 0.21\n",
      "Epoch 670/1000 --- Train Loss: 2.1317246511825245 --- Val Loss: 2.1211184405765735 --- Train Acc: 0.19 --- Val Acc: 0.18\n",
      "Epoch 680/1000 --- Train Loss: 2.06135882107122 --- Val Loss: 2.081504379446488 --- Train Acc: 0.20 --- Val Acc: 0.21\n",
      "Epoch 690/1000 --- Train Loss: 2.122639408133701 --- Val Loss: 2.087377242723198 --- Train Acc: 0.21 --- Val Acc: 0.21\n",
      "Epoch 700/1000 --- Train Loss: 2.050996469244846 --- Val Loss: 2.0458837617632564 --- Train Acc: 0.23 --- Val Acc: 0.23\n",
      "Epoch 710/1000 --- Train Loss: 2.201786630546316 --- Val Loss: 2.1708040560561206 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 720/1000 --- Train Loss: 2.1361822228163674 --- Val Loss: 2.158801374835006 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 730/1000 --- Train Loss: 2.177694704396015 --- Val Loss: 2.2223694978414104 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 740/1000 --- Train Loss: 2.1534799748055318 --- Val Loss: 2.1648746612736294 --- Train Acc: 0.17 --- Val Acc: 0.17\n",
      "Epoch 750/1000 --- Train Loss: 2.159533260826651 --- Val Loss: 2.138247989386094 --- Train Acc: 0.17 --- Val Acc: 0.18\n",
      "Epoch 760/1000 --- Train Loss: 2.2065882834982333 --- Val Loss: 2.20480464647708 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 770/1000 --- Train Loss: 2.181856042423218 --- Val Loss: 2.1733301289164735 --- Train Acc: 0.15 --- Val Acc: 0.17\n",
      "Epoch 780/1000 --- Train Loss: 2.296266176888912 --- Val Loss: 2.3003978682194846 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.2758407845598128 --- Val Loss: 2.2671740148917836 --- Train Acc: 0.12 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.228841493555608 --- Val Loss: 2.2211351669013837 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 810/1000 --- Train Loss: 2.2431965669376597 --- Val Loss: 2.2229063878683135 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 820/1000 --- Train Loss: 2.1977378820695783 --- Val Loss: 2.2272014952408266 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 830/1000 --- Train Loss: 2.232760441434253 --- Val Loss: 2.228180034361519 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 840/1000 --- Train Loss: 2.211509752957108 --- Val Loss: 2.2106567848225245 --- Train Acc: 0.15 --- Val Acc: 0.15\n",
      "Epoch 850/1000 --- Train Loss: 2.181980556521886 --- Val Loss: 2.188384766962223 --- Train Acc: 0.16 --- Val Acc: 0.16\n",
      "Epoch 860/1000 --- Train Loss: 2.177925060578686 --- Val Loss: 2.159755916203634 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 870/1000 --- Train Loss: 2.23677562131176 --- Val Loss: 2.158254242821486 --- Train Acc: 0.16 --- Val Acc: 0.17\n",
      "Epoch 880/1000 --- Train Loss: 2.1493098655388256 --- Val Loss: 2.161858798655927 --- Train Acc: 0.17 --- Val Acc: 0.17\n",
      "Epoch 890/1000 --- Train Loss: 2.245482394298984 --- Val Loss: 2.223857462112733 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 900/1000 --- Train Loss: 2.2636972352377613 --- Val Loss: 2.2812743132262785 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 910/1000 --- Train Loss: 2.249243676178497 --- Val Loss: 2.2532495255627842 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 920/1000 --- Train Loss: 2.222346339034621 --- Val Loss: 2.2228789616597058 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 930/1000 --- Train Loss: 2.267231667629903 --- Val Loss: 2.2351971942878914 --- Train Acc: 0.13 --- Val Acc: 0.14\n",
      "Epoch 940/1000 --- Train Loss: 2.3011231992843677 --- Val Loss: 2.3019926296542774 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.3028401997356407 --- Val Loss: 2.300861595868511 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: 2.275318563600111 --- Val Loss: 2.2848260880523954 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Epoch 970/1000 --- Train Loss: 2.2415378834110156 --- Val Loss: 2.225542471629141 --- Train Acc: 0.14 --- Val Acc: 0.15\n",
      "Epoch 980/1000 --- Train Loss: 2.3016074851058286 --- Val Loss: 2.3008808152256695 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.28114035253534 --- Val Loss: 2.2873394142984917 --- Train Acc: 0.12 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.1111111111111111\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.3008640884666964 --- Val Loss: 2.2990172823528865 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 10/1000 --- Train Loss: 2.1980236456149305 --- Val Loss: 2.1274971339750732 --- Train Acc: 0.18 --- Val Acc: 0.18\n",
      "Epoch 20/1000 --- Train Loss: 2.338972541108154 --- Val Loss: 2.2332350417409206 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 30/1000 --- Train Loss: 2.374619708172383 --- Val Loss: 2.2990800815505787 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 40/1000 --- Train Loss: 2.174457646817561 --- Val Loss: 2.1131575699040765 --- Train Acc: 0.17 --- Val Acc: 0.20\n",
      "Epoch 50/1000 --- Train Loss: 2.337253232266273 --- Val Loss: 2.2981637780417277 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 60/1000 --- Train Loss: 2.3132931094425664 --- Val Loss: 2.299646129273068 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 70/1000 --- Train Loss: 2.325566916846529 --- Val Loss: 2.299533309971277 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.2994550024464426 --- Val Loss: 2.2998439113675198 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 90/1000 --- Train Loss: 2.2975734306323616 --- Val Loss: 2.2990921767409374 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 100/1000 --- Train Loss: 2.301312111045392 --- Val Loss: 2.2990319442417166 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.3134210837091853 --- Val Loss: 2.3001135505512464 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.299602825365229 --- Val Loss: 2.29781411664303 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: 2.301393004903096 --- Val Loss: 2.2984186247568767 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: 2.313401228317214 --- Val Loss: 2.2996975314834995 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: 2.3133900218134222 --- Val Loss: 2.299352822301085 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.3014562575567075 --- Val Loss: 2.2987513353024673 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.3013502729251165 --- Val Loss: 2.2983130011218993 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: 2.301474772030756 --- Val Loss: 2.299308433336863 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.3014481441016015 --- Val Loss: 2.2989785691506786 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 200/1000 --- Train Loss: 2.3013558659086155 --- Val Loss: 2.2990580164375176 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.3013535542953 --- Val Loss: 2.298252890792157 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 220/1000 --- Train Loss: 2.3014364115689556 --- Val Loss: 2.2980563548020814 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 230/1000 --- Train Loss: 2.301395426757264 --- Val Loss: 2.298379489807499 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 240/1000 --- Train Loss: 2.3013947703349342 --- Val Loss: 2.297887290407634 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 250/1000 --- Train Loss: 2.301417616709583 --- Val Loss: 2.298182933178529 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.301432080442499 --- Val Loss: 2.2989418284199648 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 270/1000 --- Train Loss: 2.3014898543200575 --- Val Loss: 2.297982341504733 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.301371334687823 --- Val Loss: 2.2994654347932952 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 290/1000 --- Train Loss: 2.301437908639483 --- Val Loss: 2.2992808570672456 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 300/1000 --- Train Loss: 2.3013427929483385 --- Val Loss: 2.298936227023394 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 310/1000 --- Train Loss: 2.3013471994439856 --- Val Loss: 2.29838371542186 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.3013653234869538 --- Val Loss: 2.2996029754086806 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.3014082354649994 --- Val Loss: 2.2990327828154222 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.301410515552597 --- Val Loss: 2.299126022956651 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.3134715956660985 --- Val Loss: 2.2984651808252266 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.3013863233479763 --- Val Loss: 2.2986047613737717 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.3014279500571595 --- Val Loss: 2.2999035778675285 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 380/1000 --- Train Loss: 2.3013457176722514 --- Val Loss: 2.299646775482013 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 390/1000 --- Train Loss: 2.3013325722007556 --- Val Loss: 2.2984067852614873 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 400/1000 --- Train Loss: 2.301350725440688 --- Val Loss: 2.2976745725938406 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 410/1000 --- Train Loss: 2.301354403394554 --- Val Loss: 2.2991076155778005 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 420/1000 --- Train Loss: 2.3015074310351693 --- Val Loss: 2.3003039520082083 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3014470810408425 --- Val Loss: 2.29903554424606 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 440/1000 --- Train Loss: 2.3013417815591315 --- Val Loss: 2.2986186377745197 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 450/1000 --- Train Loss: 2.3013026011278437 --- Val Loss: 2.299002663991252 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 460/1000 --- Train Loss: 2.301368821638948 --- Val Loss: 2.2990491709341 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.3013962009447924 --- Val Loss: 2.30034970033615 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.301396836522689 --- Val Loss: 2.298307753343507 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 490/1000 --- Train Loss: 2.301397892378942 --- Val Loss: 2.299205638308495 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.3014021440006243 --- Val Loss: 2.299574737832378 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 510/1000 --- Train Loss: 2.301402924351539 --- Val Loss: 2.2990995275881416 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 520/1000 --- Train Loss: 2.3015147262073934 --- Val Loss: 2.2998422695498824 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 530/1000 --- Train Loss: 2.301359347826072 --- Val Loss: 2.2982118307213364 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 540/1000 --- Train Loss: 2.30133346256808 --- Val Loss: 2.29922971697166 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 550/1000 --- Train Loss: 2.3015940144301394 --- Val Loss: 2.299731454989757 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.3013511643345237 --- Val Loss: 2.2978171831950704 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 570/1000 --- Train Loss: 2.3014762212613658 --- Val Loss: 2.298332298829784 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.301489810372168 --- Val Loss: 2.2976788562251707 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 590/1000 --- Train Loss: 2.3014147855157834 --- Val Loss: 2.2996815119490424 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.301388194268765 --- Val Loss: 2.298940972197196 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.301319018265082 --- Val Loss: 2.299041251119658 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.301366139903276 --- Val Loss: 2.2984469260729625 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.3014093351516425 --- Val Loss: 2.2996924851282814 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.3013717655716217 --- Val Loss: 2.2988930661961087 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 650/1000 --- Train Loss: 2.3013735655122742 --- Val Loss: 2.2984703676729534 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 660/1000 --- Train Loss: 2.301494768561962 --- Val Loss: 2.298518783700767 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3013934697874934 --- Val Loss: 2.2992598972325724 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.30136991218984 --- Val Loss: 2.299075090014783 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.3014037358623414 --- Val Loss: 2.2999153273785926 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 700/1000 --- Train Loss: 2.3013608207749434 --- Val Loss: 2.2991035066007384 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 710/1000 --- Train Loss: 2.3013304829733787 --- Val Loss: 2.2986600668570714 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.3013118297978026 --- Val Loss: 2.2983287083849318 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 730/1000 --- Train Loss: 2.301328476772443 --- Val Loss: 2.2993522456297923 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.301456717468893 --- Val Loss: 2.3000206577082056 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 750/1000 --- Train Loss: 2.30139801034619 --- Val Loss: 2.298408760150448 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 760/1000 --- Train Loss: 2.3014033479904485 --- Val Loss: 2.29963601615894 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 770/1000 --- Train Loss: 2.3013864902294325 --- Val Loss: 2.2991643108616935 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.301419440505187 --- Val Loss: 2.3002578301773844 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.301399640064047 --- Val Loss: 2.2984462388958193 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 800/1000 --- Train Loss: 2.3013647074132044 --- Val Loss: 2.29806313339439 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 810/1000 --- Train Loss: 2.3013102053277827 --- Val Loss: 2.2985006431220723 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.301484887364088 --- Val Loss: 2.2977669331930834 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 830/1000 --- Train Loss: 2.3013633651206304 --- Val Loss: 2.29889567396571 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 840/1000 --- Train Loss: 2.301351039028004 --- Val Loss: 2.298689288974208 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 850/1000 --- Train Loss: 2.301410919519169 --- Val Loss: 2.298794843107729 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 860/1000 --- Train Loss: 2.3134829605237246 --- Val Loss: 2.2997210858775636 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: 2.301354025079384 --- Val Loss: 2.2988596710297227 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 880/1000 --- Train Loss: 2.3013979714979067 --- Val Loss: 2.299176452077548 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 890/1000 --- Train Loss: 2.3014158540907004 --- Val Loss: 2.297496444488569 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.3014046536227055 --- Val Loss: 2.2985222885814327 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 910/1000 --- Train Loss: 2.3013231987205867 --- Val Loss: 2.298875341858419 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 920/1000 --- Train Loss: 2.3013671283883617 --- Val Loss: 2.2982430690223716 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 930/1000 --- Train Loss: 2.3014236276985605 --- Val Loss: 2.2996727748791366 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.301316745776312 --- Val Loss: 2.2988047437520645 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.301336659491684 --- Val Loss: 2.298848948656817 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 960/1000 --- Train Loss: 2.3014050222598073 --- Val Loss: 2.2988905993769007 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 970/1000 --- Train Loss: 2.301424632180924 --- Val Loss: 2.2988633241507452 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 980/1000 --- Train Loss: 2.3014190414963163 --- Val Loss: 2.2991999958504326 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Epoch 990/1000 --- Train Loss: 2.3013282510039605 --- Val Loss: 2.298816999932028 --- Train Acc: 0.11 --- Val Acc: 0.13\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.5, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.302521846662354 --- Val Loss: 2.3025002259053378 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.302005674053835 --- Val Loss: 2.3017861549245406 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.301646018982464 --- Val Loss: 2.301262569298463 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.3013974529318495 --- Val Loss: 2.300873657153683 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.3012237764052603 --- Val Loss: 2.300585373950781 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.3011040327818084 --- Val Loss: 2.3003711853116906 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: 2.3010213687486956 --- Val Loss: 2.300206936913261 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.3009638755881863 --- Val Loss: 2.3000806437337826 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 80/1000 --- Train Loss: 2.300924021830262 --- Val Loss: 2.2999859610977054 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.300896154150672 --- Val Loss: 2.2999107656748774 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.3008767336499374 --- Val Loss: 2.2998526575184415 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.3008632577317414 --- Val Loss: 2.299806327258618 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.300853832717988 --- Val Loss: 2.2997671498264873 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.3008472780116143 --- Val Loss: 2.2997370788137403 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.300842875331001 --- Val Loss: 2.2997130726683004 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.300839742793589 --- Val Loss: 2.299694520813976 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3008375625242454 --- Val Loss: 2.2996798984925895 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.3008361359414513 --- Val Loss: 2.2996687633303114 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3008349661782552 --- Val Loss: 2.299657226213892 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.3008339851832114 --- Val Loss: 2.299644987675538 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.3008334055183823 --- Val Loss: 2.2996374485707456 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.3008329749573355 --- Val Loss: 2.2996322051029487 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.3008325407236954 --- Val Loss: 2.299629467696301 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.300832233822041 --- Val Loss: 2.2996229689759313 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.3008319348155197 --- Val Loss: 2.299618210062838 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.300831687834177 --- Val Loss: 2.2996141906953493 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.30083135434064 --- Val Loss: 2.2996128236802567 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.3008311134556876 --- Val Loss: 2.2996098729233223 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.300830774300831 --- Val Loss: 2.299609416071936 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.3008304742212338 --- Val Loss: 2.29960917168403 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.3008301562477604 --- Val Loss: 2.2996057780913293 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.3008298791943727 --- Val Loss: 2.2996049978803716 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.300829536072714 --- Val Loss: 2.2996029824558417 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.3008291058435315 --- Val Loss: 2.299606278601626 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.3008287800959413 --- Val Loss: 2.2996054966019055 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.300828274604418 --- Val Loss: 2.299605981182969 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.3008276717158465 --- Val Loss: 2.2996045253061825 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3008271313155597 --- Val Loss: 2.299603182256591 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3008267024867894 --- Val Loss: 2.2996027037075644 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.300825973901538 --- Val Loss: 2.299604465505785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3008253768381097 --- Val Loss: 2.299602013646478 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.300824549457782 --- Val Loss: 2.2996008996216086 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.3008236661163464 --- Val Loss: 2.2995976009088097 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: 2.300822721507319 --- Val Loss: 2.2995965340228453 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.3008215643822854 --- Val Loss: 2.2995929425643467 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.300820816452229 --- Val Loss: 2.2995919778129323 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3008194162790363 --- Val Loss: 2.2995909305441926 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3008182588923183 --- Val Loss: 2.299589300792117 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.3008164034599585 --- Val Loss: 2.29958925888364 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.3008147115457587 --- Val Loss: 2.29958443438185 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.300812658278124 --- Val Loss: 2.2995845323662434 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.3008103179826564 --- Val Loss: 2.2995833809599664 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.3008081103326767 --- Val Loss: 2.299580190804128 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.3008061159943085 --- Val Loss: 2.2995751722110773 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3008030755326794 --- Val Loss: 2.299571164089047 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.30079951458217 --- Val Loss: 2.2995655693650088 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.300795740139352 --- Val Loss: 2.299561934880523 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3007915956702614 --- Val Loss: 2.2995586470317115 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.3007868601699637 --- Val Loss: 2.299555863563599 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.3007802021055994 --- Val Loss: 2.2995505579839612 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: 2.3007735901148547 --- Val Loss: 2.2995424755789253 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.300767459450292 --- Val Loss: 2.299536453715399 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.300758382499104 --- Val Loss: 2.2995288032084105 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.300748595903069 --- Val Loss: 2.2995191189828734 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3007370759993004 --- Val Loss: 2.2995066839628784 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.3007238935103307 --- Val Loss: 2.299492794399365 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.3007088295179203 --- Val Loss: 2.299478465623006 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.3006887200628725 --- Val Loss: 2.2994575764749734 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.3006670144634964 --- Val Loss: 2.2994356955544752 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.3006403243365865 --- Val Loss: 2.299410501366208 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.3006066495576567 --- Val Loss: 2.29937959553659 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.30056837815052 --- Val Loss: 2.2993435221594476 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.300523751133535 --- Val Loss: 2.299297964240874 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.300459376859676 --- Val Loss: 2.2992413870500545 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.300387788488025 --- Val Loss: 2.299169476101039 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.3002848055663483 --- Val Loss: 2.299076848759098 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3001599248523896 --- Val Loss: 2.2989554094971822 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.3000058084254404 --- Val Loss: 2.2987998557802714 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.299810246810282 --- Val Loss: 2.298589518986151 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.299487426341068 --- Val Loss: 2.2982997053305745 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.2991032051275058 --- Val Loss: 2.29789281312469 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.2985037367590153 --- Val Loss: 2.297292474469585 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.2976030166052346 --- Val Loss: 2.2963844413903325 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.2960665285587583 --- Val Loss: 2.294933528117111 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.293535990810202 --- Val Loss: 2.2924405435381847 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.2888819570363164 --- Val Loss: 2.2877490122457704 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: 2.2782635856451945 --- Val Loss: 2.2777193912420746 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 870/1000 --- Train Loss: 2.251580823247745 --- Val Loss: 2.2519313650099346 --- Train Acc: 0.21 --- Val Acc: 0.21\n",
      "Epoch 880/1000 --- Train Loss: 2.169900110304786 --- Val Loss: 2.171804710628492 --- Train Acc: 0.25 --- Val Acc: 0.25\n",
      "Epoch 890/1000 --- Train Loss: 2.0031558174990294 --- Val Loss: 2.009687417051185 --- Train Acc: 0.28 --- Val Acc: 0.26\n",
      "Epoch 900/1000 --- Train Loss: 1.8860568905826456 --- Val Loss: 1.8904112267423012 --- Train Acc: 0.23 --- Val Acc: 0.21\n",
      "Epoch 910/1000 --- Train Loss: 1.7931433535089043 --- Val Loss: 1.7871545613123156 --- Train Acc: 0.23 --- Val Acc: 0.22\n",
      "Epoch 920/1000 --- Train Loss: 1.7123863407738789 --- Val Loss: 1.7061625698233183 --- Train Acc: 0.27 --- Val Acc: 0.26\n",
      "Epoch 930/1000 --- Train Loss: 1.6525148625753414 --- Val Loss: 1.6422745370559841 --- Train Acc: 0.34 --- Val Acc: 0.35\n",
      "Epoch 940/1000 --- Train Loss: 1.586441438869193 --- Val Loss: 1.5788037235937382 --- Train Acc: 0.42 --- Val Acc: 0.40\n",
      "Epoch 950/1000 --- Train Loss: 1.5029106846673068 --- Val Loss: 1.4962199666877107 --- Train Acc: 0.51 --- Val Acc: 0.50\n",
      "Epoch 960/1000 --- Train Loss: 1.370281633537854 --- Val Loss: 1.3711086709749896 --- Train Acc: 0.53 --- Val Acc: 0.49\n",
      "Epoch 970/1000 --- Train Loss: 1.2139029731110196 --- Val Loss: 1.223504074659455 --- Train Acc: 0.56 --- Val Acc: 0.51\n",
      "Epoch 980/1000 --- Train Loss: 1.0793035650099785 --- Val Loss: 1.0925357098484947 --- Train Acc: 0.61 --- Val Acc: 0.56\n",
      "Epoch 990/1000 --- Train Loss: 0.967862017492908 --- Val Loss: 0.9741839419899507 --- Train Acc: 0.68 --- Val Acc: 0.64\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.7194444444444444\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.301788017662853 --- Val Loss: 2.3030298246783736 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 2.3655549153031727 --- Val Loss: 1.9707310671178275 --- Train Acc: 0.33 --- Val Acc: 0.33\n",
      "Epoch 20/1000 --- Train Loss: 2.3320703459410757 --- Val Loss: 2.3045296482186197 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.310216545451631 --- Val Loss: 2.305014497543225 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.301912480747457 --- Val Loss: 2.3043395599384477 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.3022204038963685 --- Val Loss: 2.3038481809332843 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: 2.302199434026204 --- Val Loss: 2.3052726731914674 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.3021195755064165 --- Val Loss: 2.303916842663058 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 80/1000 --- Train Loss: 2.302329077183273 --- Val Loss: 2.302074476518669 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 90/1000 --- Train Loss: 2.3023036722325667 --- Val Loss: 2.303450294119283 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.302623591697844 --- Val Loss: 2.3034264984004804 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.302157203023308 --- Val Loss: 2.3036510441326894 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.301970064885651 --- Val Loss: 2.3037052199900367 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 130/1000 --- Train Loss: 2.301908058047253 --- Val Loss: 2.3028365697537256 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 140/1000 --- Train Loss: 2.301946133542751 --- Val Loss: 2.3032980612376153 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 150/1000 --- Train Loss: 2.302083272619681 --- Val Loss: 2.305525402965486 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 160/1000 --- Train Loss: 2.3021259832238594 --- Val Loss: 2.3045207641995797 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.313927029531289 --- Val Loss: 2.3041282582014517 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.302573252170167 --- Val Loss: 2.30588473477326 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 190/1000 --- Train Loss: 2.302185730871955 --- Val Loss: 2.303532908345576 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.301901661657387 --- Val Loss: 2.3035109155940097 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.302299031368233 --- Val Loss: 2.3041727342973597 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.302401636700015 --- Val Loss: 2.3042974862547343 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.3020672233958877 --- Val Loss: 2.3023938230832632 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.302496834452252 --- Val Loss: 2.304337594955021 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.302226492052896 --- Val Loss: 2.3032741070441998 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.302174700939493 --- Val Loss: 2.3032341469036033 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.3020494662790365 --- Val Loss: 2.3044941000303822 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.3020649551037766 --- Val Loss: 2.3041647891452484 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.301943250817176 --- Val Loss: 2.304486378085328 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.302027934123197 --- Val Loss: 2.304078718919993 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.302300632938274 --- Val Loss: 2.305385156525545 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: 2.3020254871029775 --- Val Loss: 2.303877296898736 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: 2.3026971249682058 --- Val Loss: 2.3055159917898678 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: 2.3020652891231608 --- Val Loss: 2.303139261006038 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.302007335232231 --- Val Loss: 2.304113741548979 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: 2.302025239771457 --- Val Loss: 2.303476525631984 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3025136759829206 --- Val Loss: 2.3025353822910577 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3022273017251402 --- Val Loss: 2.30429975492941 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: 2.3023958340739283 --- Val Loss: 2.306126524783242 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: 2.3022148936284004 --- Val Loss: 2.3036034114209456 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: 2.3025421181754924 --- Val Loss: 2.304848848685224 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: 2.302004865810691 --- Val Loss: 2.3042774992738657 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3020436634612147 --- Val Loss: 2.303291791188269 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: 2.301896439911041 --- Val Loss: 2.304060957480344 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3024967328703267 --- Val Loss: 2.3037819903069408 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: 2.3023260473948364 --- Val Loss: 2.304077747893074 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: 2.3021453576721345 --- Val Loss: 2.3047871947144962 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: 2.301967696124895 --- Val Loss: 2.304169237989724 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: 2.3024342300670693 --- Val Loss: 2.305119779069224 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.302100570579386 --- Val Loss: 2.304662638030281 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: 2.302125008819817 --- Val Loss: 2.3053290466154785 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: 2.302246613909796 --- Val Loss: 2.3047057199973464 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: 2.302068262152128 --- Val Loss: 2.303664273907314 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: 2.3024344129741516 --- Val Loss: 2.3051789129964786 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: 2.302334256661245 --- Val Loss: 2.30318326104842 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: 2.3020800402146455 --- Val Loss: 2.302921443471033 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: 2.3023892760464126 --- Val Loss: 2.304348578077132 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: 2.302020327144351 --- Val Loss: 2.3031049084108357 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.3020840518347736 --- Val Loss: 2.3039028705515525 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.3021781201087483 --- Val Loss: 2.303966142329904 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: 2.3022623886610485 --- Val Loss: 2.3036255737827185 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: 2.30182175755306 --- Val Loss: 2.3036881534715405 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: 2.3020650571042256 --- Val Loss: 2.303248461908837 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3022022567011886 --- Val Loss: 2.3037736360990677 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: 2.302616979697632 --- Val Loss: 2.3026151626579963 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.301857997028886 --- Val Loss: 2.3033492254932018 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: 2.301963844205066 --- Val Loss: 2.303403807048996 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: 2.3019500241549076 --- Val Loss: 2.3042289718840987 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: 2.3020567233154563 --- Val Loss: 2.3029709051616973 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: 2.3019271156987053 --- Val Loss: 2.3037212257624327 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: 2.3018852569963606 --- Val Loss: 2.3037384284296625 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: 2.3022669895421286 --- Val Loss: 2.303965114900226 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: 2.3021202646291505 --- Val Loss: 2.3033115065288374 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: 2.3025186202259977 --- Val Loss: 2.301697043611088 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.302213260704198 --- Val Loss: 2.303658897457241 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.3021188367665055 --- Val Loss: 2.303835249672435 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: 2.3020545793760956 --- Val Loss: 2.304999927998912 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.3022935486002254 --- Val Loss: 2.3026515879961345 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: 2.302138828178765 --- Val Loss: 2.30283464362918 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: 2.30179653440853 --- Val Loss: 2.303321041775976 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: 2.3020636966166377 --- Val Loss: 2.3033402523567177 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3018218673492123 --- Val Loss: 2.304054251302408 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: 2.302358220437444 --- Val Loss: 2.3021486557644 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3026227973375346 --- Val Loss: 2.3043559725110976 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 850/1000 --- Train Loss: 2.302114324462581 --- Val Loss: 2.3039791030939623 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.3019018909899587 --- Val Loss: 2.302860692270632 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: 2.302291658327507 --- Val Loss: 2.3023210535357954 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.30194805507439 --- Val Loss: 2.3036358589784034 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3022693076083653 --- Val Loss: 2.3037194044177856 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: 2.301992182701857 --- Val Loss: 2.304222209443125 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: 2.3018706737415453 --- Val Loss: 2.3039030511629495 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: 2.302150940582559 --- Val Loss: 2.303835637529693 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.3019750000223875 --- Val Loss: 2.303601047817753 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: 2.3026013739432147 --- Val Loss: 2.305381244908675 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.302223318904508 --- Val Loss: 2.302658386554896 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.301990094170404 --- Val Loss: 2.3039431898579124 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: 2.30207906045133 --- Val Loss: 2.3041346789918697 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: 2.3019212242100155 --- Val Loss: 2.303835412494589 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: 2.3023864304496473 --- Val Loss: 2.3044343422780953 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 64}, Accuracy: 0.07777777777777778\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 1.4497068743300323 --- Val Loss: 1.2487999434188537 --- Train Acc: 0.69 --- Val Acc: 0.67\n",
      "Epoch 10/1000 --- Train Loss: 2.306792925065009 --- Val Loss: 2.3060994598053615 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 20/1000 --- Train Loss: 2.3065659877465388 --- Val Loss: 2.30273884663895 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 30/1000 --- Train Loss: 2.312394304719258 --- Val Loss: 2.2996054283266574 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 40/1000 --- Train Loss: 2.3103494284710866 --- Val Loss: 2.310289463270986 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 50/1000 --- Train Loss: 2.3071136518748085 --- Val Loss: 2.3023552067126256 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 60/1000 --- Train Loss: 2.3135829323323147 --- Val Loss: 2.307844144799276 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 70/1000 --- Train Loss: 2.3139540798309812 --- Val Loss: 2.3205075299791016 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 80/1000 --- Train Loss: 2.3071283561527696 --- Val Loss: 2.310141881107826 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 90/1000 --- Train Loss: 2.315621718182898 --- Val Loss: 2.310082630941194 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 100/1000 --- Train Loss: 2.3111020329787118 --- Val Loss: 2.314277438485402 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 110/1000 --- Train Loss: 2.3141552939315795 --- Val Loss: 2.301519002978562 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 120/1000 --- Train Loss: 2.3277256941335067 --- Val Loss: 2.3163629403431143 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 130/1000 --- Train Loss: 2.3299823839263083 --- Val Loss: 2.3344778266937345 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 140/1000 --- Train Loss: 2.317656152332172 --- Val Loss: 2.31418291895701 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.306488931308757 --- Val Loss: 2.3068417546432465 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 160/1000 --- Train Loss: 2.3115124857552254 --- Val Loss: 2.3082961612971875 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 170/1000 --- Train Loss: 2.313212610780953 --- Val Loss: 2.3082663175339375 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 180/1000 --- Train Loss: 2.3130150468601185 --- Val Loss: 2.3094282931301797 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.308758757106144 --- Val Loss: 2.3048417500265344 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.3110286239110835 --- Val Loss: 2.313068311715263 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 210/1000 --- Train Loss: 2.317855853373174 --- Val Loss: 2.3242837632824256 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 220/1000 --- Train Loss: 2.3090271516898673 --- Val Loss: 2.3076319309958753 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 230/1000 --- Train Loss: 2.3133008849464805 --- Val Loss: 2.311968848557253 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 240/1000 --- Train Loss: 2.3119240497544395 --- Val Loss: 2.3074964209723463 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.3104008194051486 --- Val Loss: 2.3103310216092634 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.3104748183375143 --- Val Loss: 2.316650529770003 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 270/1000 --- Train Loss: 2.306895334984875 --- Val Loss: 2.314046968199754 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 280/1000 --- Train Loss: 2.323407659742122 --- Val Loss: 2.3365199571698714 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 290/1000 --- Train Loss: 2.3130457142578726 --- Val Loss: 2.316158237345482 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.312858018340785 --- Val Loss: 2.3146403340091934 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: 2.312198732333581 --- Val Loss: 2.318458585331849 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 320/1000 --- Train Loss: 2.30806484951145 --- Val Loss: 2.3113925535666273 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 330/1000 --- Train Loss: 2.3104688618696776 --- Val Loss: 2.3198424953975687 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 340/1000 --- Train Loss: 2.3166148105113136 --- Val Loss: 2.3168514711752164 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: 2.3199710932902677 --- Val Loss: 2.3213695171230073 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.3110851397789456 --- Val Loss: 2.3096343868994347 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: 2.3137594728189623 --- Val Loss: 2.313435094922766 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: 2.3081395353638583 --- Val Loss: 2.310311612477014 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 390/1000 --- Train Loss: 2.3072725896449975 --- Val Loss: 2.3060341719501523 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 400/1000 --- Train Loss: 2.3242052426859443 --- Val Loss: 2.33773610675712 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 410/1000 --- Train Loss: 2.3046060584536137 --- Val Loss: 2.30077873544557 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.321382338612434 --- Val Loss: 2.3198128028959624 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.311196332043236 --- Val Loss: 2.3216144854747607 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 440/1000 --- Train Loss: 2.3115581082533274 --- Val Loss: 2.316896229645501 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: 2.3079091114011097 --- Val Loss: 2.3092835141908563 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.3057560089788853 --- Val Loss: 2.3038320022381353 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.305214742224144 --- Val Loss: 2.301575298868668 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 480/1000 --- Train Loss: 2.3053453515680404 --- Val Loss: 2.309506250363227 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 490/1000 --- Train Loss: 2.313528712298854 --- Val Loss: 2.3107535032667865 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: 2.3077177716308364 --- Val Loss: 2.314008586680568 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 510/1000 --- Train Loss: 2.320378574921535 --- Val Loss: 2.312749263812566 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.3110832084375024 --- Val Loss: 2.3157376018455786 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3053231608561173 --- Val Loss: 2.3051497793789433 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.310792054888597 --- Val Loss: 2.320707574105238 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.3207359270377825 --- Val Loss: 2.3320439300828806 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 560/1000 --- Train Loss: 2.313045606017145 --- Val Loss: 2.310787248423562 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.321349419324454 --- Val Loss: 2.323552244476088 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.306692090581979 --- Val Loss: 2.3050809548684614 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: 2.316225070578988 --- Val Loss: 2.3125945378132906 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.3130076847544223 --- Val Loss: 2.3162763544299603 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 610/1000 --- Train Loss: 2.3099374195843887 --- Val Loss: 2.3193724456824096 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 620/1000 --- Train Loss: 2.3195839012840342 --- Val Loss: 2.3352638430506785 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 630/1000 --- Train Loss: 2.3126719974496464 --- Val Loss: 2.308380640392604 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: 2.3117158402406393 --- Val Loss: 2.321264203973143 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 650/1000 --- Train Loss: 2.313336753449743 --- Val Loss: 2.3080293232490043 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: 2.303784462501038 --- Val Loss: 2.3023600653347414 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.3024537101270663 --- Val Loss: 2.3009658529296133 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 680/1000 --- Train Loss: 2.3187948212981824 --- Val Loss: 2.3208765839591443 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 690/1000 --- Train Loss: 2.312735168981919 --- Val Loss: 2.3074060428106984 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.3069901459901336 --- Val Loss: 2.3104825348104914 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 710/1000 --- Train Loss: 2.31021700879928 --- Val Loss: 2.3048419017441018 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.307808803837875 --- Val Loss: 2.304932886572075 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.3079587013789555 --- Val Loss: 2.3051028023716196 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.31222937804772 --- Val Loss: 2.3117647031273254 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: 2.3105673507925926 --- Val Loss: 2.308090551875144 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: 2.303604166815514 --- Val Loss: 2.3010499430688025 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.3119785385000147 --- Val Loss: 2.3120333421146033 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: 2.312279646577078 --- Val Loss: 2.308847245250404 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.310590828585045 --- Val Loss: 2.319142353165766 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 800/1000 --- Train Loss: 2.310975278351051 --- Val Loss: 2.3254535551172912 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 810/1000 --- Train Loss: 2.3091808830414307 --- Val Loss: 2.29796664718172 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: 2.3081985943563277 --- Val Loss: 2.3137836663332605 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 830/1000 --- Train Loss: 2.311592384996081 --- Val Loss: 2.3116479967141417 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: 2.3081529113473205 --- Val Loss: 2.30561544383657 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: 2.3118665246633987 --- Val Loss: 2.301166474869733 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.318643701492629 --- Val Loss: 2.321830680367234 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.3057709474388886 --- Val Loss: 2.310719927963234 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: 2.3103340676633275 --- Val Loss: 2.313195410574198 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: 2.3142953199085525 --- Val Loss: 2.3222071475656976 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 900/1000 --- Train Loss: 2.306876981478175 --- Val Loss: 2.299930637515981 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.313255572610432 --- Val Loss: 2.314275600097713 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 920/1000 --- Train Loss: 2.3084397108943695 --- Val Loss: 2.3097756654533925 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: 2.308234215625868 --- Val Loss: 2.3083095251951886 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.3176027138528754 --- Val Loss: 2.3240785615895962 --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: 2.309822555283125 --- Val Loss: 2.3103086130324857 --- Train Acc: 0.10 --- Val Acc: 0.09\n",
      "Epoch 960/1000 --- Train Loss: 2.3092605273665865 --- Val Loss: 2.313034568753469 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.310959598676995 --- Val Loss: 2.32019997870543 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 980/1000 --- Train Loss: 2.309133604612853 --- Val Loss: 2.31657488574537 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 990/1000 --- Train Loss: 2.3106317074870915 --- Val Loss: 2.319007890215823 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.30161199682105 --- Val Loss: 2.305104872730048 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 10/1000 --- Train Loss: 2.301466313301996 --- Val Loss: 2.3068518020292546 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 20/1000 --- Train Loss: 2.301470387353321 --- Val Loss: 2.3061314170683134 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 30/1000 --- Train Loss: 2.3014836576872253 --- Val Loss: 2.308816756624093 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 40/1000 --- Train Loss: 2.2996758260936647 --- Val Loss: 2.30551206422705 --- Train Acc: 0.11 --- Val Acc: 0.05\n",
      "Epoch 50/1000 --- Train Loss: 1.19497268292805 --- Val Loss: 1.180879013472165 --- Train Acc: 0.52 --- Val Acc: 0.49\n",
      "Epoch 60/1000 --- Train Loss: 0.23459492584661273 --- Val Loss: 0.14463320113687056 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 70/1000 --- Train Loss: 0.13095603403943232 --- Val Loss: 0.04463419399372403 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 80/1000 --- Train Loss: 0.06118806730818258 --- Val Loss: 0.018154888084761046 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 90/1000 --- Train Loss: 0.056271164640596213 --- Val Loss: 0.009147816267413657 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 100/1000 --- Train Loss: 0.044978476166949076 --- Val Loss: 0.009839779104147045 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 110/1000 --- Train Loss: 0.04241690700145199 --- Val Loss: 0.006110836313457272 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/1000 --- Train Loss: 0.040608227739310644 --- Val Loss: 0.002677715029728943 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/1000 --- Train Loss: 0.03228826308618316 --- Val Loss: 0.003891643310295209 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 140/1000 --- Train Loss: 0.04189582975699286 --- Val Loss: 0.002462937091430511 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 150/1000 --- Train Loss: 0.02092183548219575 --- Val Loss: 0.002124587624644725 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/1000 --- Train Loss: 0.02333607410308259 --- Val Loss: 0.0024098359051971663 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/1000 --- Train Loss: 0.03623682838946307 --- Val Loss: 0.00354395250579675 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 180/1000 --- Train Loss: 0.026944533315547785 --- Val Loss: 0.001715134926951937 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 190/1000 --- Train Loss: 0.02467184472919904 --- Val Loss: 0.0015574695711955092 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 200/1000 --- Train Loss: 0.024766998147035438 --- Val Loss: 0.0012437500791419164 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 210/1000 --- Train Loss: 0.013592772585413661 --- Val Loss: 0.0012742949010892395 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 220/1000 --- Train Loss: 0.034587177375246934 --- Val Loss: 0.0040127783254996505 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 230/1000 --- Train Loss: 0.03324395828617519 --- Val Loss: 0.0005253330830010481 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 240/1000 --- Train Loss: 0.022929490687770487 --- Val Loss: 0.0010521296822139001 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 250/1000 --- Train Loss: 0.02858484323266329 --- Val Loss: 0.0016504394351390468 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 260/1000 --- Train Loss: 0.04408278170874442 --- Val Loss: 0.000849289763081092 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 270/1000 --- Train Loss: 0.017474948129503752 --- Val Loss: 0.001042849954249349 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 280/1000 --- Train Loss: 0.03619928125420105 --- Val Loss: 0.0005113077283991385 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 290/1000 --- Train Loss: 0.029017575792062336 --- Val Loss: 0.0007585430550257646 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 300/1000 --- Train Loss: 0.015992059346316007 --- Val Loss: 0.000420106013039539 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 310/1000 --- Train Loss: 0.01478108175659996 --- Val Loss: 0.0004762780400119656 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 320/1000 --- Train Loss: 0.03818795400038459 --- Val Loss: 0.00043539627966539306 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 330/1000 --- Train Loss: 0.01905409689301147 --- Val Loss: 0.0013233715253063887 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 340/1000 --- Train Loss: 0.036896319781303796 --- Val Loss: 0.00038246618994631956 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 350/1000 --- Train Loss: 0.03174404813358519 --- Val Loss: 0.0003378486987668858 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 360/1000 --- Train Loss: 0.06825398975575557 --- Val Loss: 0.0007284961983402489 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 370/1000 --- Train Loss: 0.04580739274195546 --- Val Loss: 0.0021490768558895387 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 380/1000 --- Train Loss: 0.047803389387855194 --- Val Loss: 0.0010670123910995414 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 390/1000 --- Train Loss: 0.025790383024025657 --- Val Loss: 0.0016096794753449155 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 400/1000 --- Train Loss: 0.027927689790759352 --- Val Loss: 0.0012786373765069697 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 410/1000 --- Train Loss: 0.01936665032862945 --- Val Loss: 0.0014758137142669634 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 420/1000 --- Train Loss: 0.04695890568886691 --- Val Loss: 0.00041547314266476317 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 430/1000 --- Train Loss: 0.015889512052895363 --- Val Loss: 0.0004600134642963754 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 440/1000 --- Train Loss: 0.045337511058454995 --- Val Loss: 0.0004190113390499154 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 450/1000 --- Train Loss: 0.03328838956357434 --- Val Loss: 0.0009960975623267867 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 460/1000 --- Train Loss: 0.029784117053630082 --- Val Loss: 0.0004392160719208071 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 470/1000 --- Train Loss: 0.018341982946942652 --- Val Loss: 0.0006019584542696166 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 480/1000 --- Train Loss: 0.023244840186666203 --- Val Loss: 0.0004151880393751815 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 490/1000 --- Train Loss: 0.027592290589405377 --- Val Loss: 0.0005653638192193377 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 500/1000 --- Train Loss: 0.023068673727773666 --- Val Loss: 0.00048102406267331026 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 510/1000 --- Train Loss: 0.021581248595245198 --- Val Loss: 0.0005461263284505625 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 520/1000 --- Train Loss: 0.025424161371705035 --- Val Loss: 0.00035188864567451056 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 530/1000 --- Train Loss: 0.025438023071769428 --- Val Loss: 0.00029458248895453907 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 540/1000 --- Train Loss: 0.02268217731643813 --- Val Loss: 0.00044921318112605384 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 550/1000 --- Train Loss: 0.022533404770520758 --- Val Loss: 0.0006209372408545364 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 560/1000 --- Train Loss: 0.020451913806414878 --- Val Loss: 0.0007687559459144342 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 570/1000 --- Train Loss: 0.020783834235641646 --- Val Loss: 0.0012543000693882634 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 580/1000 --- Train Loss: 0.052128733434869 --- Val Loss: 0.003699927021213041 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 590/1000 --- Train Loss: 0.02999871425373103 --- Val Loss: 0.0009442176527985193 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 600/1000 --- Train Loss: 0.02657102635085402 --- Val Loss: 0.0007909849701137866 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 610/1000 --- Train Loss: 0.03343621349114066 --- Val Loss: 0.0005670833849505052 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 620/1000 --- Train Loss: 0.026690171397322448 --- Val Loss: 0.0005482160708480039 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 630/1000 --- Train Loss: 0.0218455123897172 --- Val Loss: 0.00048116209912335095 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 640/1000 --- Train Loss: 0.03285404991177266 --- Val Loss: 0.000496268890021337 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 650/1000 --- Train Loss: 0.0325894539412621 --- Val Loss: 0.0007064996096492591 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 660/1000 --- Train Loss: 0.020898808524922312 --- Val Loss: 0.0002828021277122627 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 670/1000 --- Train Loss: 0.02387164294269407 --- Val Loss: 0.00036586011237981365 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 680/1000 --- Train Loss: 0.017701771095773193 --- Val Loss: 0.00019908134920202416 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 690/1000 --- Train Loss: 0.017983254942392415 --- Val Loss: 0.0002303069634089362 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 700/1000 --- Train Loss: 0.03445069129439529 --- Val Loss: 0.0004127229122494173 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 710/1000 --- Train Loss: 0.02189761099148823 --- Val Loss: 0.00028561555295816013 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 720/1000 --- Train Loss: 0.012615814603265453 --- Val Loss: 0.0004187318520883453 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 730/1000 --- Train Loss: 0.02527803550466724 --- Val Loss: 0.0007687769906709911 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 740/1000 --- Train Loss: 0.01983900055479578 --- Val Loss: 0.00015850944805143835 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 750/1000 --- Train Loss: 0.02286685746098636 --- Val Loss: 0.0002931757346282521 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 760/1000 --- Train Loss: 0.02901198630937037 --- Val Loss: 0.00013220795428309847 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 770/1000 --- Train Loss: 0.03627165324396705 --- Val Loss: 0.001206744470648753 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 780/1000 --- Train Loss: 0.028493674366745787 --- Val Loss: 0.0012162721541438706 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 790/1000 --- Train Loss: 0.021950165304715642 --- Val Loss: 0.00025351752848353957 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 800/1000 --- Train Loss: 0.018154533276985827 --- Val Loss: 0.0003535860556131148 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 810/1000 --- Train Loss: 0.01164066329240698 --- Val Loss: 0.0002664860531468022 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 820/1000 --- Train Loss: 0.03981243485055903 --- Val Loss: 0.0005792309616742732 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 830/1000 --- Train Loss: 0.020745750914339195 --- Val Loss: 0.00036207325688099155 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 840/1000 --- Train Loss: 0.01465646286035242 --- Val Loss: 0.0004104596739220334 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 850/1000 --- Train Loss: 0.017189310218276178 --- Val Loss: 0.00040494568933721565 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 860/1000 --- Train Loss: 0.012700910154522455 --- Val Loss: 0.0003791019298394321 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 870/1000 --- Train Loss: 0.019494015978772913 --- Val Loss: 0.00020411763793829192 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 880/1000 --- Train Loss: 0.03392624257245883 --- Val Loss: 0.00031266074912760433 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 890/1000 --- Train Loss: 0.020235567206790935 --- Val Loss: 0.0005809190502613393 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 900/1000 --- Train Loss: 0.014801683090229435 --- Val Loss: 0.0003154546982728014 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 910/1000 --- Train Loss: 0.01848247269462155 --- Val Loss: 0.00027524492174256043 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 920/1000 --- Train Loss: 0.010490345492229453 --- Val Loss: 0.00024228171497323888 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 930/1000 --- Train Loss: 0.021636868385578465 --- Val Loss: 0.0002651846590052525 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 940/1000 --- Train Loss: 0.014033866397795771 --- Val Loss: 0.0002211576744155122 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 950/1000 --- Train Loss: 0.015374404933346507 --- Val Loss: 0.00015705174094545943 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 960/1000 --- Train Loss: 0.028868668367913793 --- Val Loss: 0.00013597460211653293 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 970/1000 --- Train Loss: 0.07162478295031985 --- Val Loss: 0.0010748904311880135 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 980/1000 --- Train Loss: 0.008582402875676388 --- Val Loss: 0.00023075846885975561 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 990/1000 --- Train Loss: 0.03405596651092032 --- Val Loss: 0.0004194449980133148 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Accuracy: 0.9638888888888889\n",
      "32\n",
      "Epoch 0/1000 --- Train Loss: 2.3021329039154295 --- Val Loss: 2.302257522806392 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/1000 --- Train Loss: 2.301932273771446 --- Val Loss: 2.3005679710521587 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 20/1000 --- Train Loss: 2.3018763812481664 --- Val Loss: 2.300997518876922 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 30/1000 --- Train Loss: 1.734791104710093 --- Val Loss: 1.7577845917913044 --- Train Acc: 0.34 --- Val Acc: 0.30\n",
      "Epoch 40/1000 --- Train Loss: 0.1563586384162459 --- Val Loss: 0.0999342815788513 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 50/1000 --- Train Loss: 0.20582035077770428 --- Val Loss: 0.06073839582308395 --- Train Acc: 0.99 --- Val Acc: 0.98\n",
      "Epoch 60/1000 --- Train Loss: 0.18973392778987477 --- Val Loss: 0.033086404216418154 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 70/1000 --- Train Loss: 0.4855357255169818 --- Val Loss: 0.1090910463731889 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 80/1000 --- Train Loss: 1.2984160061197298 --- Val Loss: 1.0878431536862982 --- Train Acc: 0.56 --- Val Acc: 0.59\n",
      "Epoch 90/1000 --- Train Loss: 1.7945785170821649 --- Val Loss: 1.7472425338567694 --- Train Acc: 0.29 --- Val Acc: 0.32\n",
      "Epoch 100/1000 --- Train Loss: 2.2974060082663885 --- Val Loss: 2.3026791265103386 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 110/1000 --- Train Loss: 2.30637589127978 --- Val Loss: 2.300330441291763 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 120/1000 --- Train Loss: 2.298047968000239 --- Val Loss: 2.3004066201590287 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 130/1000 --- Train Loss: 2.301934388002612 --- Val Loss: 2.300384162941214 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 140/1000 --- Train Loss: 2.3019411311920566 --- Val Loss: 2.3002860605106545 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 150/1000 --- Train Loss: 2.2981020589538477 --- Val Loss: 2.3007115846072765 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 160/1000 --- Train Loss: 2.296152899731651 --- Val Loss: 2.3005386486930446 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 170/1000 --- Train Loss: 2.299991434813194 --- Val Loss: 2.300973902960576 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 180/1000 --- Train Loss: 2.301927341285088 --- Val Loss: 2.3015485814121655 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 190/1000 --- Train Loss: 2.2961287038302527 --- Val Loss: 2.3009443539056846 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 200/1000 --- Train Loss: 2.2960528786882772 --- Val Loss: 2.3004200892272797 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 210/1000 --- Train Loss: 2.299973712114489 --- Val Loss: 2.30004080698091 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 220/1000 --- Train Loss: 2.295981000040844 --- Val Loss: 2.3011132399962198 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 230/1000 --- Train Loss: 2.2979621963901957 --- Val Loss: 2.3016358858043793 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 240/1000 --- Train Loss: 2.2999964306860816 --- Val Loss: 2.2991727196589684 --- Train Acc: 0.10 --- Val Acc: 0.15\n",
      "Epoch 250/1000 --- Train Loss: 2.2979017053274773 --- Val Loss: 2.300720601482702 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 260/1000 --- Train Loss: 2.29594235245063 --- Val Loss: 2.301610416511867 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 270/1000 --- Train Loss: 2.2979275795438254 --- Val Loss: 2.301008492022765 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 280/1000 --- Train Loss: 2.2938977347485516 --- Val Loss: 2.300528940849322 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 290/1000 --- Train Loss: 2.299955405628716 --- Val Loss: 2.301084571735737 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 300/1000 --- Train Loss: 2.3019614327818334 --- Val Loss: 2.3016697745957306 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 310/1000 --- Train Loss: 2.3019653774485347 --- Val Loss: 2.3002255402849667 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 320/1000 --- Train Loss: 2.297905595106579 --- Val Loss: 2.3009747299378502 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 330/1000 --- Train Loss: 2.2999275526535348 --- Val Loss: 2.301755152958748 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 340/1000 --- Train Loss: 2.2959679319093804 --- Val Loss: 2.3007661960407026 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 350/1000 --- Train Loss: 2.300054199833408 --- Val Loss: 2.30133800244873 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/1000 --- Train Loss: 2.3019282473978446 --- Val Loss: 2.3009126154974626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 370/1000 --- Train Loss: 2.298039105892337 --- Val Loss: 2.301178544243758 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 380/1000 --- Train Loss: 2.301937632531596 --- Val Loss: 2.300293243218317 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 390/1000 --- Train Loss: 2.3019819429280224 --- Val Loss: 2.3011390123836852 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/1000 --- Train Loss: 2.2941837458120857 --- Val Loss: 2.299643793526416 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/1000 --- Train Loss: 2.2980012951523956 --- Val Loss: 2.300459749327468 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 420/1000 --- Train Loss: 2.301964251800796 --- Val Loss: 2.3008434507110227 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 430/1000 --- Train Loss: 2.3000193304867813 --- Val Loss: 2.3024230373525327 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 440/1000 --- Train Loss: 2.298040788740864 --- Val Loss: 2.30034997841231 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/1000 --- Train Loss: 2.299962489575295 --- Val Loss: 2.3005487782542797 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 460/1000 --- Train Loss: 2.2999794118758703 --- Val Loss: 2.301558200414481 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 470/1000 --- Train Loss: 2.299956056098679 --- Val Loss: 2.3007085808659595 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 480/1000 --- Train Loss: 2.301949647315221 --- Val Loss: 2.301674110430276 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 490/1000 --- Train Loss: 2.299931919901342 --- Val Loss: 2.300494973360899 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 500/1000 --- Train Loss: 2.300038552419006 --- Val Loss: 2.3006759457948425 --- Train Acc: 0.10 --- Val Acc: 0.15\n",
      "Epoch 510/1000 --- Train Loss: 2.2999660691072075 --- Val Loss: 2.300128154749819 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 520/1000 --- Train Loss: 2.301962931783889 --- Val Loss: 2.301035894673419 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 530/1000 --- Train Loss: 2.3019442425995766 --- Val Loss: 2.3013474767599695 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 540/1000 --- Train Loss: 2.2959934355250917 --- Val Loss: 2.3001602996273194 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 550/1000 --- Train Loss: 2.301923309429616 --- Val Loss: 2.3009658059996996 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 560/1000 --- Train Loss: 2.2960118572775348 --- Val Loss: 2.3009237372091573 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 570/1000 --- Train Loss: 2.298089997430924 --- Val Loss: 2.2999749745734754 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 580/1000 --- Train Loss: 2.2979952472624277 --- Val Loss: 2.301558505300385 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 590/1000 --- Train Loss: 2.2979640584643195 --- Val Loss: 2.300648941462377 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 600/1000 --- Train Loss: 2.2959626006120133 --- Val Loss: 2.3014578294196277 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 610/1000 --- Train Loss: 2.2939693738687175 --- Val Loss: 2.3017972445469956 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 620/1000 --- Train Loss: 2.2979923552863837 --- Val Loss: 2.3000636291648466 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 630/1000 --- Train Loss: 2.299949167576373 --- Val Loss: 2.3000464066871253 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 640/1000 --- Train Loss: 2.2999959305466655 --- Val Loss: 2.3016322977283967 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 650/1000 --- Train Loss: 2.2999470455022615 --- Val Loss: 2.3009423622264378 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 660/1000 --- Train Loss: 2.2959873055378983 --- Val Loss: 2.3007199630262645 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 670/1000 --- Train Loss: 2.2979952069482645 --- Val Loss: 2.3006878564103013 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 680/1000 --- Train Loss: 2.297999737820553 --- Val Loss: 2.3000110904621396 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 690/1000 --- Train Loss: 2.2979363301548195 --- Val Loss: 2.3005945783132677 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 700/1000 --- Train Loss: 2.2979541431372965 --- Val Loss: 2.300866409668792 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 710/1000 --- Train Loss: 2.295929159962801 --- Val Loss: 2.3013672925524395 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 720/1000 --- Train Loss: 2.3019720839572937 --- Val Loss: 2.3018094040886448 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 730/1000 --- Train Loss: 2.2999436194700302 --- Val Loss: 2.3009993458475626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 740/1000 --- Train Loss: 2.2959851904714625 --- Val Loss: 2.300885091224687 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 750/1000 --- Train Loss: 2.3019464264340104 --- Val Loss: 2.3011173099851465 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 760/1000 --- Train Loss: 2.301910378403852 --- Val Loss: 2.3006432009667344 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 770/1000 --- Train Loss: 2.2999359187307182 --- Val Loss: 2.300991484518859 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 780/1000 --- Train Loss: 2.3019649538324796 --- Val Loss: 2.3024535979311063 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 790/1000 --- Train Loss: 2.2979343864789965 --- Val Loss: 2.301693851585626 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 800/1000 --- Train Loss: 2.297939860140971 --- Val Loss: 2.301197874559536 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 810/1000 --- Train Loss: 2.3019232626590873 --- Val Loss: 2.3008566465357965 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 820/1000 --- Train Loss: 2.2999586384605943 --- Val Loss: 2.3002554169777403 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 830/1000 --- Train Loss: 2.297962998293432 --- Val Loss: 2.3012825264678773 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 840/1000 --- Train Loss: 2.2980483732283408 --- Val Loss: 2.300885805685633 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 850/1000 --- Train Loss: 2.2940214799044423 --- Val Loss: 2.2999366377651587 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 860/1000 --- Train Loss: 2.29796143947702 --- Val Loss: 2.302782209255937 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 870/1000 --- Train Loss: 2.301958962208527 --- Val Loss: 2.3011596233055416 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 880/1000 --- Train Loss: 2.295988187071963 --- Val Loss: 2.301302703143801 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 890/1000 --- Train Loss: 2.299930125310782 --- Val Loss: 2.3009508650115094 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 900/1000 --- Train Loss: 2.297976481331959 --- Val Loss: 2.301211225508065 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 910/1000 --- Train Loss: 2.301957299201068 --- Val Loss: 2.2997444399971534 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 920/1000 --- Train Loss: 2.299932297490791 --- Val Loss: 2.3010896768724756 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 930/1000 --- Train Loss: 2.2999701893429125 --- Val Loss: 2.2998162775891484 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 940/1000 --- Train Loss: 2.299938296274145 --- Val Loss: 2.3017113974461565 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 950/1000 --- Train Loss: 2.299936470505046 --- Val Loss: 2.301235578292988 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 960/1000 --- Train Loss: 2.299991101745259 --- Val Loss: 2.2992288061254476 --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 970/1000 --- Train Loss: 2.3019412990097727 --- Val Loss: 2.3011088112157956 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 980/1000 --- Train Loss: 2.2979852682563386 --- Val Loss: 2.301494727290268 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 990/1000 --- Train Loss: 2.2999431305727764 --- Val Loss: 2.3008218091607 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 32, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09444444444444444\n",
      "128\n",
      "Epoch 0/1000 --- Train Loss: 2.301713822507412 --- Val Loss: 2.3006839692924252 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 10/1000 --- Train Loss: 0.690596343130513 --- Val Loss: 0.29505671632635355 --- Train Acc: 0.92 --- Val Acc: 0.93\n",
      "Epoch 20/1000 --- Train Loss: 1.1043500763628282 --- Val Loss: 0.9487252451475776 --- Train Acc: 0.64 --- Val Acc: 0.66\n",
      "Epoch 30/1000 --- Train Loss: 2.0702218449677177 --- Val Loss: 2.1206738594802697 --- Train Acc: 0.19 --- Val Acc: 0.20\n",
      "Epoch 40/1000 --- Train Loss: 2.1087227532339123 --- Val Loss: 2.1611327796972564 --- Train Acc: 0.17 --- Val Acc: 0.17\n",
      "Epoch 50/1000 --- Train Loss: 2.226276991007294 --- Val Loss: 2.267605998051103 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 60/1000 --- Train Loss: 2.218974908156525 --- Val Loss: 2.267332785196753 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 70/1000 --- Train Loss: 2.233554691505596 --- Val Loss: 2.2680284849592143 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 80/1000 --- Train Loss: 2.235994344868324 --- Val Loss: 2.2670322995668775 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 90/1000 --- Train Loss: 2.232043540053929 --- Val Loss: 2.267314488869678 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 100/1000 --- Train Loss: 2.2321861973963375 --- Val Loss: 2.267980206661933 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 110/1000 --- Train Loss: 2.230286715364099 --- Val Loss: 2.267080695784038 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 120/1000 --- Train Loss: 2.2303552023565207 --- Val Loss: 2.2668027322782804 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 130/1000 --- Train Loss: 2.2152452066885133 --- Val Loss: 2.2672545439232077 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 140/1000 --- Train Loss: 2.2339916599027965 --- Val Loss: 2.2673881447798805 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 150/1000 --- Train Loss: 2.227790036095555 --- Val Loss: 2.2674184515639566 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 160/1000 --- Train Loss: 2.2274183836850705 --- Val Loss: 2.267553073714612 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 170/1000 --- Train Loss: 2.2258492569637145 --- Val Loss: 2.2671406189339236 --- Train Acc: 0.13 --- Val Acc: 0.13\n",
      "Epoch 180/1000 --- Train Loss: 2.2820714575411913 --- Val Loss: 2.3017979344162054 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 190/1000 --- Train Loss: 2.2879587486789283 --- Val Loss: 2.299766265302936 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 200/1000 --- Train Loss: 2.2837539114261665 --- Val Loss: 2.3008767569627726 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 210/1000 --- Train Loss: 2.2818937465546254 --- Val Loss: 2.3014922541658187 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 220/1000 --- Train Loss: 2.287912722286474 --- Val Loss: 2.300529787973397 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 230/1000 --- Train Loss: 2.2879097147711698 --- Val Loss: 2.3007205240264827 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 240/1000 --- Train Loss: 2.2817831576276286 --- Val Loss: 2.3001650413380172 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 250/1000 --- Train Loss: 2.2799775952601844 --- Val Loss: 2.3001790881309034 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 260/1000 --- Train Loss: 2.2817409742574113 --- Val Loss: 2.3004359463728186 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 270/1000 --- Train Loss: 2.2779694864330415 --- Val Loss: 2.3006135379850035 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 280/1000 --- Train Loss: 2.2879040229065954 --- Val Loss: 2.30014561500481 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 290/1000 --- Train Loss: 2.2877984212062814 --- Val Loss: 2.300165122848655 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 300/1000 --- Train Loss: 2.281969254599007 --- Val Loss: 2.2999371098441728 --- Train Acc: 0.11 --- Val Acc: 0.11\n",
      "Epoch 310/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 320/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 330/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.11\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.0, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 16}, Accuracy: 0.09166666666666666\n",
      "64\n",
      "Epoch 0/1000 --- Train Loss: 2.302105493080125 --- Val Loss: 2.3007438586553213 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 10/1000 --- Train Loss: 0.4212251148460197 --- Val Loss: 0.30395667027368134 --- Train Acc: 0.91 --- Val Acc: 0.91\n",
      "Epoch 20/1000 --- Train Loss: 0.2832231623891077 --- Val Loss: 0.06677217250107843 --- Train Acc: 0.97 --- Val Acc: 0.97\n",
      "Epoch 30/1000 --- Train Loss: 1.0773952815562042 --- Val Loss: 0.15966625323770348 --- Train Acc: 0.96 --- Val Acc: 0.97\n",
      "Epoch 40/1000 --- Train Loss: 0.8530339133864181 --- Val Loss: 0.08501603654490844 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 50/1000 --- Train Loss: 0.6683470205896 --- Val Loss: 0.03967080914651209 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 60/1000 --- Train Loss: 0.6335575524918655 --- Val Loss: 0.08593943747443168 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 70/1000 --- Train Loss: 0.577897211865744 --- Val Loss: 0.05616071167581789 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 80/1000 --- Train Loss: 1.0091331568501358 --- Val Loss: 0.39578986181213666 --- Train Acc: 0.98 --- Val Acc: 0.97\n",
      "Epoch 90/1000 --- Train Loss: 0.5466137752064174 --- Val Loss: 0.05616071167581789 --- Train Acc: 0.99 --- Val Acc: 1.00\n",
      "Epoch 100/1000 --- Train Loss: 0.91314881418347 --- Val Loss: 0.7862486634613866 --- Train Acc: 0.97 --- Val Acc: 0.95\n",
      "Epoch 110/1000 --- Train Loss: 0.5278930617853166 --- Val Loss: 0.0737307252732873 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 120/1000 --- Train Loss: 0.7724908551526284 --- Val Loss: 0.34467850687733614 --- Train Acc: 0.91 --- Val Acc: 0.90\n",
      "Epoch 130/1000 --- Train Loss: 0.8384147757934735 --- Val Loss: 0.52987569919615 --- Train Acc: 0.82 --- Val Acc: 0.80\n",
      "Epoch 140/1000 --- Train Loss: 1.0491021966381098 --- Val Loss: 0.9047116906525589 --- Train Acc: 0.65 --- Val Acc: 0.63\n",
      "Epoch 150/1000 --- Train Loss: 1.1968843502514046 --- Val Loss: 1.0516947808967612 --- Train Acc: 0.60 --- Val Acc: 0.57\n",
      "Epoch 160/1000 --- Train Loss: 1.4634831288047514 --- Val Loss: 1.5116979845817906 --- Train Acc: 0.46 --- Val Acc: 0.44\n",
      "Epoch 170/1000 --- Train Loss: 1.6002043379255821 --- Val Loss: 1.6287475499610096 --- Train Acc: 0.38 --- Val Acc: 0.34\n",
      "Epoch 180/1000 --- Train Loss: 1.7149181402880973 --- Val Loss: 1.6910580882739694 --- Train Acc: 0.34 --- Val Acc: 0.31\n",
      "Epoch 190/1000 --- Train Loss: 1.7086798945969053 --- Val Loss: 1.7792480452919635 --- Train Acc: 0.32 --- Val Acc: 0.32\n",
      "Epoch 200/1000 --- Train Loss: 2.043947895122416 --- Val Loss: 1.9467528910070986 --- Train Acc: 0.24 --- Val Acc: 0.25\n",
      "Epoch 210/1000 --- Train Loss: 2.417923467258195 --- Val Loss: 2.2981020649816744 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 220/1000 --- Train Loss: 2.382979630359472 --- Val Loss: 2.2882989899070574 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 230/1000 --- Train Loss: 2.300725077678438 --- Val Loss: 2.283644881081322 --- Train Acc: 0.11 --- Val Acc: 0.15\n",
      "Epoch 240/1000 --- Train Loss: 2.297223355727699 --- Val Loss: 2.2927406786381006 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 250/1000 --- Train Loss: 2.3130413416356737 --- Val Loss: 2.2927868914508034 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 260/1000 --- Train Loss: 2.3011054396902124 --- Val Loss: 2.2929119588404796 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 270/1000 --- Train Loss: 2.3131791032572324 --- Val Loss: 2.292886700264983 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 280/1000 --- Train Loss: 2.301106604529784 --- Val Loss: 2.292936229064347 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 290/1000 --- Train Loss: 2.301105461747478 --- Val Loss: 2.2928964284800006 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 300/1000 --- Train Loss: 2.313041896876179 --- Val Loss: 2.293118105794877 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 310/1000 --- Train Loss: 2.3011056986166847 --- Val Loss: 2.2928572261584335 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 320/1000 --- Train Loss: 2.301105641620848 --- Val Loss: 2.2928477039557666 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 330/1000 --- Train Loss: 2.3011055199230754 --- Val Loss: 2.2928062622951066 --- Train Acc: 0.11 --- Val Acc: 0.14\n",
      "Epoch 340/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 350/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 360/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 370/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 380/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 390/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 400/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 410/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 420/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 430/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 440/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 450/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 460/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 470/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 480/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 490/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 500/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 510/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 520/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 530/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 540/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 550/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 560/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 570/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 580/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 590/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 600/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 610/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 620/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 630/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 640/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 650/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 660/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 670/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 680/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 690/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 700/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 710/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 720/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 730/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 740/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 750/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 760/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 770/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 780/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 790/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 800/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 810/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 820/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 830/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 840/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 850/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 860/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 870/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 880/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 890/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 900/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 910/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 920/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 930/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 940/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 950/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 960/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 970/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 980/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Epoch 990/1000 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.07\n",
      "Params: {'layer_configs': {'layer1_nodes': 64, 'layer1_l1': 0.01, 'layer1_l2': 0.0, 'layer2_nodes': 32, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 1000, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "32\n",
      "Epoch 0/500 --- Train Loss: 2.301701272511386 --- Val Loss: 2.3012945756091603 --- Train Acc: 0.11 --- Val Acc: 0.10\n",
      "Epoch 10/500 --- Train Loss: 2.88981262229377 --- Val Loss: 1.485148121086365 --- Train Acc: 0.85 --- Val Acc: 0.85\n",
      "Epoch 20/500 --- Train Loss: 3.0326209649554765 --- Val Loss: 2.3832754254558366 --- Train Acc: 0.23 --- Val Acc: 0.20\n",
      "Epoch 30/500 --- Train Loss: 2.4782482062410085 --- Val Loss: 2.3032760676182553 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 40/500 --- Train Loss: 2.38367648954987 --- Val Loss: 2.3004093235965977 --- Train Acc: 0.11 --- Val Acc: 0.09\n",
      "Epoch 50/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 60/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 70/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 80/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 90/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 100/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 110/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 120/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 130/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 140/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 150/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 160/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 170/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 180/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 190/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 200/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 210/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 220/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 230/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 240/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 250/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 260/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 270/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 280/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 290/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 300/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 310/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 320/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.12\n",
      "Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.01}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.09166666666666666\n",
      "128\n",
      "Epoch 0/100 --- Train Loss: 2.3023541763841524 --- Val Loss: 2.3026361174651466 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 10/100 --- Train Loss: 2.3010794821147877 --- Val Loss: 2.303070014316812 --- Train Acc: 0.11 --- Val Acc: 0.12\n",
      "Epoch 20/100 --- Train Loss: 2.197078179569675 --- Val Loss: 2.198867948458093 --- Train Acc: 0.37 --- Val Acc: 0.39\n",
      "Epoch 30/100 --- Train Loss: 0.5880155079388647 --- Val Loss: 0.5859640110480608 --- Train Acc: 0.87 --- Val Acc: 0.84\n",
      "Epoch 40/100 --- Train Loss: 0.25735892315916414 --- Val Loss: 0.202526136397763 --- Train Acc: 0.96 --- Val Acc: 0.93\n",
      "Epoch 50/100 --- Train Loss: 0.17346903134945899 --- Val Loss: 0.08604576889916202 --- Train Acc: 0.98 --- Val Acc: 0.98\n",
      "Epoch 60/100 --- Train Loss: 0.15293097129326988 --- Val Loss: 0.07133483943872294 --- Train Acc: 0.98 --- Val Acc: 0.96\n",
      "Epoch 70/100 --- Train Loss: 0.13143525061619707 --- Val Loss: 0.0223123603394711 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 80/100 --- Train Loss: 0.10029910023878782 --- Val Loss: 0.0135726798153636 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 90/100 --- Train Loss: 0.1036278238241681 --- Val Loss: 0.014250544756018061 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.01, 'layer1_l2': 0.01, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 100, 'optimizer': 'Momentum', 'momentum': 0.5, 'batch_size': 32}, Accuracy: 0.9472222222222222\n",
      "128\n",
      "Epoch 0/500 --- Train Loss: 2.3021434663781 --- Val Loss: 2.3031382835710077 --- Train Acc: 0.11 --- Val Acc: 0.08\n",
      "Epoch 10/500 --- Train Loss: 0.791075627668695 --- Val Loss: 0.8266446937727798 --- Train Acc: 0.79 --- Val Acc: 0.76\n",
      "Epoch 20/500 --- Train Loss: 0.052013220056369915 --- Val Loss: 0.0387660230634307 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 30/500 --- Train Loss: 0.021833022321773338 --- Val Loss: 0.0028899911624959643 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 40/500 --- Train Loss: 0.006615243308748119 --- Val Loss: 0.0005601419470940336 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 50/500 --- Train Loss: 0.001883915773852635 --- Val Loss: 1.3203742613887913e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 60/500 --- Train Loss: 0.006511566223293424 --- Val Loss: 1.051550767289171e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 70/500 --- Train Loss: 0.2924485907706491 --- Val Loss: 0.03781018992527771 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 80/500 --- Train Loss: 0.32474173006648244 --- Val Loss: 0.14517944803854663 --- Train Acc: 0.98 --- Val Acc: 0.99\n",
      "Epoch 90/500 --- Train Loss: 0.25772363975555 --- Val Loss: 0.22464270998051253 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 100/500 --- Train Loss: 0.3229154752845081 --- Val Loss: 0.1684819350274438 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 110/500 --- Train Loss: 0.26682662154954495 --- Val Loss: 1.3458475325830695e-05 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 120/500 --- Train Loss: 0.13679326381027743 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 130/500 --- Train Loss: 0.350101675307008 --- Val Loss: 0.2246425467032567 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 140/500 --- Train Loss: 0.37842495206598287 --- Val Loss: 0.11232132335163082 --- Train Acc: 0.99 --- Val Acc: 0.99\n",
      "Epoch 150/500 --- Train Loss: 0.19775244559546404 --- Val Loss: 0.05616071167581789 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 160/500 --- Train Loss: 0.16818892314043954 --- Val Loss: 1.0000000494736472e-07 --- Train Acc: 1.00 --- Val Acc: 1.00\n",
      "Epoch 170/500 --- Train Loss: 1.943003342914421 --- Val Loss: 1.4203517623000455 --- Train Acc: 0.87 --- Val Acc: 0.85\n",
      "Epoch 180/500 --- Train Loss: 0.605551662458843 --- Val Loss: 0.3247151839908681 --- Train Acc: 0.93 --- Val Acc: 0.93\n",
      "Epoch 190/500 --- Train Loss: 0.6149017134173522 --- Val Loss: 0.4593529232413616 --- Train Acc: 0.85 --- Val Acc: 0.87\n",
      "Epoch 200/500 --- Train Loss: 0.5619357802811124 --- Val Loss: 0.4010854511550374 --- Train Acc: 0.84 --- Val Acc: 0.87\n",
      "Epoch 210/500 --- Train Loss: 0.677990858475995 --- Val Loss: 0.5528607889282163 --- Train Acc: 0.79 --- Val Acc: 0.80\n",
      "Epoch 220/500 --- Train Loss: 0.7047769600278496 --- Val Loss: 0.5218184253486248 --- Train Acc: 0.79 --- Val Acc: 0.82\n",
      "Epoch 230/500 --- Train Loss: 0.6269663430001301 --- Val Loss: 0.5883916422685246 --- Train Acc: 0.80 --- Val Acc: 0.80\n",
      "Epoch 240/500 --- Train Loss: 1.9816471223950276 --- Val Loss: 2.22206983890649 --- Train Acc: 0.64 --- Val Acc: 0.62\n",
      "Epoch 250/500 --- Train Loss: 1.6110805514323008 --- Val Loss: 1.5146450411377221 --- Train Acc: 0.45 --- Val Acc: 0.46\n",
      "Epoch 260/500 --- Train Loss: 1.7108833665342589 --- Val Loss: 1.728445444372434 --- Train Acc: 0.32 --- Val Acc: 0.34\n",
      "Epoch 270/500 --- Train Loss: 1.6859007525893221 --- Val Loss: 1.7131292339675894 --- Train Acc: 0.33 --- Val Acc: 0.31\n",
      "Epoch 280/500 --- Train Loss: 1.6917548749269984 --- Val Loss: 1.621660361348612 --- Train Acc: 0.37 --- Val Acc: 0.35\n",
      "Epoch 290/500 --- Train Loss: 1.7724664368000687 --- Val Loss: 1.7832231199500526 --- Train Acc: 0.29 --- Val Acc: 0.29\n",
      "Epoch 300/500 --- Train Loss: 1.8265713522731928 --- Val Loss: 1.7837081147095528 --- Train Acc: 0.29 --- Val Acc: 0.29\n",
      "Epoch 310/500 --- Train Loss: 1.7676322777874394 --- Val Loss: 1.7836597024077394 --- Train Acc: 0.29 --- Val Acc: 0.29\n",
      "Epoch 320/500 --- Train Loss: 1.7824507907294933 --- Val Loss: 1.8113358082221982 --- Train Acc: 0.28 --- Val Acc: 0.28\n",
      "Epoch 330/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 340/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 350/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 360/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 370/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 380/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 390/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 400/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 410/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 420/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 430/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 440/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 450/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 460/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 470/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 480/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Epoch 490/500 --- Train Loss: nan --- Val Loss: nan --- Train Acc: 0.10 --- Val Acc: 0.10\n",
      "Params: {'layer_configs': {'layer1_nodes': 128, 'layer1_l1': 0.0, 'layer1_l2': 0.01, 'layer2_nodes': 64, 'layer2_l1': 0.01, 'layer2_l2': 0.0}, 'learning_rate': 0.1, 'epochs': 500, 'optimizer': 'GD', 'momentum': 0.5, 'batch_size': 64}, Accuracy: 0.09166666666666666\n",
      "Best Params: {'layer_configs': {'layer1_nodes': 32, 'layer1_l1': 0.0, 'layer1_l2': 0.0, 'layer2_nodes': 16, 'layer2_l1': 0.0, 'layer2_l2': 0.0}, 'learning_rate': 0.01, 'epochs': 1000, 'optimizer': 'Momentum', 'momentum': 0.9, 'batch_size': 16}, Best Accuracy: 0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "random_search = RandomSearch(NeuralNetwork, param_grid, n_iter=20)\n",
    "best_params, best_accuracy = random_search.search(X, y)\n",
    "print(f\"\\nBest Params: {best_params}, Best Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
