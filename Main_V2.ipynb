{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "236dfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ReLU layer class\n",
    "class ReLU:\n",
    "    '''\n",
    "    A class representing the Rectified Linear Unit (reLu) activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.input = None # placeholder for storing the input to the layer\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data # store the input to use it in the backward pass\n",
    "        return np.maximum(0, input_data) # apply the relu function: if x is negative, max(0, x) will be 0; otherwise, will be x\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Compute the backward pass through the reLu activation function.\n",
    "\n",
    "        The method calculates the gradient of the reLu function with respect \n",
    "        to its input 'x', given the gradient of the loss function with respect \n",
    "        to the output of the relu layer ('gradient_values').\n",
    "\n",
    "        Parameters:\n",
    "        - gradient_values (numpy.ndarray): The gradient of the loss function with respect \n",
    "                                           to the output of the relu layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the \n",
    "                         input of the relu layer.\n",
    "        '''\n",
    "        # apply the derivative of the relu function: if the input is negative, the derivative is 0; otherwise, the derivative is 1\n",
    "        return output_gradient * (self.input > 0)\n",
    "        #return output_gradient * np.where(self.input > 0, 1.0, 0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1d0cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid layer class\n",
    "class Sigmoid:\n",
    "    '''\n",
    "    A class representing the Sigmoid activation function.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.output = None # placeholder for storing the output of the forward pass\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.output = 1 / (1 + np.exp(-input_data)) # apply the sigmoid function: f(x) = 1 / (1 + exp(-x))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, output_gradient):\n",
    "        '''\n",
    "        Computes the backward pass of the Sigmoid activation function.\n",
    "\n",
    "        Given the gradient of the loss function with respect to the output of the\n",
    "        Sigmoid layer ('output_gradient'), this method calculates the gradient with respect\n",
    "        to the Sigmoid input.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient (numpy.ndarray): The gradient of the loss function with respect\n",
    "                                           to the output of the Sigmoid layer.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The gradient of the loss function with respect to the\n",
    "                         input of the Sigmoid layer.\n",
    "        '''\n",
    "        return output_gradient * (self.output * (1 - self.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "94c275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer class\n",
    "class Softmax:\n",
    "    '''\n",
    "    A class representing the Softmax activation function.\n",
    "    '''\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Computes the forward pass of the Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): A numpy array containing the input data to which the Softmax\n",
    "                             function should be applied.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The result of applying the Softmax function to 'input_data', with the\n",
    "                         same shape as 'input_data'.\n",
    "        ''' \n",
    "        exp_values = np.exp(input_data - np.max(input_data, axis=1, keepdims=True)) # Shift the input data to avoid numerical instability in exponential calculations\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, dvalues):\n",
    "        # The gradient of loss with respect to the input logits \n",
    "        # directly passed through in case of softmax + categorical cross-entropy\n",
    "        return dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3bb61882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        \n",
    "    def forward_pass(self, input_data):\n",
    "        self.mask = np.random.binomial(1, 1-self.probability, size=input_data.shape) / (1-self.probability)\n",
    "        return input_data * self.mask\n",
    "    \n",
    "    def backward_pass(self, output_gradient):\n",
    "        return output_gradient * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3a283b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = 0.01 * np.random.normal(0, 1/np.sqrt(input_size), (input_size, output_size)) # Normal distribution initialisation\n",
    "        self.biases = np.full((1, output_size), 0.001) # Initialise biases with a small positive value\n",
    "        self.input = None\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate):\n",
    "        '''\n",
    "        Computes the backward pass of the Dense layer.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the output of the layer.\n",
    "\n",
    "        - learning_rate: A hyperparameter that controls how much the weights and biases are updated during training.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: the gradient of the loss with respect to the layer's inputs (which will be passed back to the previous layer in the network).\n",
    "        ''' \n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        biases_gradient = np.sum(output_gradient, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights += learning_rate * weights_gradient\n",
    "        self.biases += learning_rate * biases_gradient\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7cc0ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network wrapper class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [] # placeholder for storing the layers of the network so we can propagate the infomation in a sequential order\n",
    "        self.loss_history = [] # placeholder to store the (train) loss for plotting\n",
    "        self.val_loss_history = [] #placeholder to store the loss function calculated on the validation set for plotting\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "        Add the layer to the network\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network. \n",
    "        It sequentially passes the input data through each layer, transforming it according to each layer's operation.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "    \n",
    "    def prediction(self, input_data):\n",
    "        '''\n",
    "        Performs a forward pass through the network ignoring the dropout.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            if not isinstance(layer, Dropout):\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def backward_pass(self, output_gradient, learning_rate):\n",
    "        '''\n",
    "        Performs the backward pass (backpropagation) for training. \n",
    "        It propagates the gradient of the loss function backward through the network, updating weights in the process if the layer is a dense one.\n",
    "\n",
    "        Parameters:\n",
    "        - output_gradient: The gradient of the loss function with respect to the network's output.\n",
    "\n",
    "        - learning_rate: The step size for weight updates.\n",
    "        '''\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                output_gradient = layer.backward_pass(output_gradient, learning_rate)\n",
    "            else:\n",
    "                output_gradient = layer.backward_pass(output_gradient)\n",
    "    \n",
    "    def compute_categorical_cross_entropy_loss(self, y_pred, y_true):\n",
    "        '''\n",
    "        Computes the categorical cross entropy loss\n",
    "        '''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # Clip predictions to prevent log(0)\n",
    "\n",
    "        # Calculate the negative log of the probabilities of the correct class\n",
    "        # Multiply with the one-hot encoded true labels and sum across classes\n",
    "        loss = np.sum(y_true * -np.log(y_pred_clipped), axis=1)\n",
    "\n",
    "        # Average loss over all samples\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def compute_categorical_cross_entropy_gradient(self, y_pred, y_true):\n",
    "        '''\n",
    "        Calculates the gradient of the categorical cross entropy loss with respect to the network's output, assuming that the output layer is the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - y_pred: Output of the softmax activation function.\n",
    "\n",
    "        - y_true: One-hot encoded label array.\n",
    "        '''\n",
    "        # Assuming y_true is one-hot encoded and y_pred is the output of softmax\n",
    "        y_pred_gradient = (y_pred - y_true) / len(y_pred)\n",
    "        return y_pred_gradient\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, learning_rate=0.001, batch_size=32, validation_split = 0.2, verbose = 1):\n",
    "        '''\n",
    "        Conducts the training process over a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input features of the training data.\n",
    "\n",
    "        - y_train: The target output (labels) of the training data.\n",
    "\n",
    "        - epochs: The number of times the entire training dataset is passed forward and backward through the neural network.\n",
    "\n",
    "        - learning_rate: The step size at each iteration while moving toward a minimum of the loss function.\n",
    "\n",
    "        - batch_size: The number of training examples used in one iteration.\n",
    "\n",
    "        - validation_split: Fraction of the training data to be used as validation data.\n",
    "\n",
    "        - verbose: The mode of verbosity (0 = silent, 1 = update every 10 epochs, 2 = update every epoch).\n",
    "\n",
    "        '''\n",
    "        val_sample_size = int(len(X_train) * validation_split) # calculate validation sample size based on validation split parameter\n",
    "\n",
    "        # Shuffles the indices of the training data to ensure random distribution\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices) \n",
    "        X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "        X_train, y_train = X_train[val_sample_size:], y_train[val_sample_size:] # splits the data into new training set.\n",
    "        X_val, y_val = X_train[:val_sample_size], y_train[:val_sample_size] # splits the data into new validation set.\n",
    "\n",
    "        n_samples = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffles the indices of the training data at the beginning of each epoch to improve generalisation\n",
    "            indices = np.arange(n_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Processing of the training data in batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_x = X_train[start_idx:end_idx]\n",
    "                batch_y = y_train[start_idx:end_idx]\n",
    "\n",
    "                output = self.forward_pass(batch_x) # forward pass to get the output predictions\n",
    "                loss_gradient = self.compute_categorical_cross_entropy_gradient(batch_y, output)\n",
    "                self.backward_pass(loss_gradient, learning_rate) # backward pass to update the network's weights\n",
    "\n",
    "            # Calculate training loss for the epoch\n",
    "            output = self.forward_pass(X_train)\n",
    "            train_loss = self.compute_categorical_cross_entropy_loss(output, y_train)\n",
    "            self.loss_history.append(train_loss)\n",
    "\n",
    "            # Calculate validation loss for the epoch\n",
    "            val_output = self.prediction(X_val)  # ensure dropout is not applied\n",
    "            val_loss = self.compute_categorical_cross_entropy_loss(val_output, y_val)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "\n",
    "            # Printing\n",
    "            if verbose == 1:\n",
    "                if epoch % 10 == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss}\")\n",
    "            elif verbose == 2:\n",
    "                print(f\"Epoch {epoch}/{epochs} --- Train Loss: {train_loss} --- Val Loss: {val_loss}\")\n",
    "            epoch += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Uses the trained network to make predictions on new data (X_test).\n",
    "        '''\n",
    "        output = self.prediction(X_test) # use prediction method to avoid dropout\n",
    "\n",
    "        predictions = np.argmax(output, axis=1) # convert probabilities to class predictions\n",
    "        return predictions\n",
    "\n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss history stored in self.loss_history over the epochs.\n",
    "        '''\n",
    "        plt.plot(self.loss_history, label = 'Train Loss')\n",
    "        plt.plot(self.val_loss_history, label = 'Val Loss')\n",
    "        plt.title(\"Loss over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed3504a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(X):\n",
    "    # Calculate the mean and standard deviation for each feature\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "\n",
    "    # Avoid division by zero in case of a constant feature\n",
    "    stds[stds == 0] = 1\n",
    "\n",
    "    # Standardize each feature\n",
    "    X_standardized = (X - means) / stds\n",
    "    return X_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "32d43306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500 --- Train Loss: 2.2999575989069676 --- Val Loss: 2.2995314619860343\n",
      "Epoch 10/500 --- Train Loss: 0.21153275332057692 --- Val Loss: 0.15715140661740112\n",
      "Epoch 20/500 --- Train Loss: 0.09219177613475542 --- Val Loss: 0.05716786162698407\n",
      "Epoch 30/500 --- Train Loss: 0.0627524026525128 --- Val Loss: 0.03193237930468554\n",
      "Epoch 40/500 --- Train Loss: 0.039617865218786436 --- Val Loss: 0.018660426303570045\n",
      "Epoch 50/500 --- Train Loss: 0.029800081493823526 --- Val Loss: 0.013599835852640036\n",
      "Epoch 60/500 --- Train Loss: 0.022148668150504418 --- Val Loss: 0.009228723718344819\n",
      "Epoch 70/500 --- Train Loss: 0.02237034973808061 --- Val Loss: 0.006830245837173013\n",
      "Epoch 80/500 --- Train Loss: 0.022222041167296785 --- Val Loss: 0.005136403856388931\n",
      "Epoch 90/500 --- Train Loss: 0.014947633409149395 --- Val Loss: 0.004144556608346813\n",
      "Epoch 100/500 --- Train Loss: 0.014861894507340476 --- Val Loss: 0.003644608248583674\n",
      "Epoch 110/500 --- Train Loss: 0.013282622835634965 --- Val Loss: 0.0033186827179840016\n",
      "Epoch 120/500 --- Train Loss: 0.01003435217867799 --- Val Loss: 0.002981753529834174\n",
      "Epoch 130/500 --- Train Loss: 0.013058966296834975 --- Val Loss: 0.0024453019552862013\n",
      "Epoch 140/500 --- Train Loss: 0.006801289475246531 --- Val Loss: 0.002216389139978582\n",
      "Epoch 150/500 --- Train Loss: 0.009873475424665068 --- Val Loss: 0.0017926459537571876\n",
      "Epoch 160/500 --- Train Loss: 0.00815102335390366 --- Val Loss: 0.001778618002442109\n",
      "Epoch 170/500 --- Train Loss: 0.008040220538696307 --- Val Loss: 0.0015994044670266838\n",
      "Epoch 180/500 --- Train Loss: 0.010580394352105576 --- Val Loss: 0.0014129615508905067\n",
      "Epoch 190/500 --- Train Loss: 0.00654003357296896 --- Val Loss: 0.0013745088178924961\n",
      "Epoch 200/500 --- Train Loss: 0.004706822142624939 --- Val Loss: 0.0011464692357829422\n",
      "Epoch 210/500 --- Train Loss: 0.005634296704692005 --- Val Loss: 0.001127087995582705\n",
      "Epoch 220/500 --- Train Loss: 0.005152251966240826 --- Val Loss: 0.001023677686319413\n",
      "Epoch 230/500 --- Train Loss: 0.00635586687671835 --- Val Loss: 0.0009228251626522613\n",
      "Epoch 240/500 --- Train Loss: 0.004798325704806459 --- Val Loss: 0.000973182446962985\n",
      "Epoch 250/500 --- Train Loss: 0.006582450538143669 --- Val Loss: 0.0009025153661740521\n",
      "Epoch 260/500 --- Train Loss: 0.0030747375045790384 --- Val Loss: 0.0007964635153152952\n",
      "Epoch 270/500 --- Train Loss: 0.00439682134175331 --- Val Loss: 0.0007352230542692302\n",
      "Epoch 280/500 --- Train Loss: 0.0032825793445821266 --- Val Loss: 0.0006860205158461538\n",
      "Epoch 290/500 --- Train Loss: 0.0029045149336469197 --- Val Loss: 0.0006325550773435696\n",
      "Epoch 300/500 --- Train Loss: 0.004743312176158272 --- Val Loss: 0.0006408213493944604\n",
      "Epoch 310/500 --- Train Loss: 0.0025266454898374476 --- Val Loss: 0.0005460725637297124\n",
      "Epoch 320/500 --- Train Loss: 0.0029651919318294693 --- Val Loss: 0.0005382292436625577\n",
      "Epoch 330/500 --- Train Loss: 0.003217378222765244 --- Val Loss: 0.00048239882883654317\n",
      "Epoch 340/500 --- Train Loss: 0.002185251850192475 --- Val Loss: 0.00046772991971374587\n",
      "Epoch 350/500 --- Train Loss: 0.0036116753953080584 --- Val Loss: 0.00046017339480423214\n",
      "Epoch 360/500 --- Train Loss: 0.0026578999062781368 --- Val Loss: 0.00042000137717209535\n",
      "Epoch 370/500 --- Train Loss: 0.004020905346239343 --- Val Loss: 0.0004021898474511201\n",
      "Epoch 380/500 --- Train Loss: 0.002834405633355756 --- Val Loss: 0.0004064739439460837\n",
      "Epoch 390/500 --- Train Loss: 0.0025451055781008348 --- Val Loss: 0.0003968066131392837\n",
      "Epoch 400/500 --- Train Loss: 0.00210673643664575 --- Val Loss: 0.0003538984294405045\n",
      "Epoch 410/500 --- Train Loss: 0.0031007434325654197 --- Val Loss: 0.0003674236861722803\n",
      "Epoch 420/500 --- Train Loss: 0.0032466076725833452 --- Val Loss: 0.0003484804678888249\n",
      "Epoch 430/500 --- Train Loss: 0.002230722255577102 --- Val Loss: 0.0003732758976404424\n",
      "Epoch 440/500 --- Train Loss: 0.0026711647182701926 --- Val Loss: 0.0003305072328871941\n",
      "Epoch 450/500 --- Train Loss: 0.0018764002275211548 --- Val Loss: 0.00029975045458790795\n",
      "Epoch 460/500 --- Train Loss: 0.0019381659013081227 --- Val Loss: 0.0002855948653436492\n",
      "Epoch 470/500 --- Train Loss: 0.004048250333039196 --- Val Loss: 0.00028959397976570443\n",
      "Epoch 480/500 --- Train Loss: 0.0018803287731628349 --- Val Loss: 0.00027079931490762673\n",
      "Epoch 490/500 --- Train Loss: 0.0030098226970883073 --- Val Loss: 0.0002633135772684663\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(64, 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Dropout(0.25))\n",
    "network.add_layer(Layer(128, 10))  # 10 classes\n",
    "network.add_layer(Softmax())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=500, learning_rate=0.01, batch_size=4)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1) # transoform back the One-Hot encoded array of the labels\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72fcf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3000 --- Train Loss: 0.6919487781833515 --- Val Loss: 0.6921793301261909\n",
      "Epoch 10/3000 --- Train Loss: 0.685922924066591 --- Val Loss: 0.6883382352327392\n",
      "Epoch 20/3000 --- Train Loss: 0.6809172865652192 --- Val Loss: 0.685100552877359\n",
      "Epoch 30/3000 --- Train Loss: 0.6751442337392647 --- Val Loss: 0.6804655767397297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aless\\miniconda3\\envs\\IntroductionToAI\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/3000 --- Train Loss: 0.6635661110555428 --- Val Loss: 0.6683815529277327\n",
      "Epoch 50/3000 --- Train Loss: 0.6310974675191099 --- Val Loss: 0.6307730932760698\n",
      "Epoch 60/3000 --- Train Loss: 0.5574459018031742 --- Val Loss: 0.5465970451515085\n",
      "Epoch 70/3000 --- Train Loss: 0.4637015113016679 --- Val Loss: 0.44669365888755214\n",
      "Epoch 80/3000 --- Train Loss: 0.377842509775937 --- Val Loss: 0.36060354877154965\n",
      "Epoch 90/3000 --- Train Loss: 0.3081474068847135 --- Val Loss: 0.292635981697731\n",
      "Epoch 100/3000 --- Train Loss: 0.25739299340117694 --- Val Loss: 0.24375643050595674\n",
      "Epoch 110/3000 --- Train Loss: 0.22137771474891738 --- Val Loss: 0.2096243418633342\n",
      "Epoch 120/3000 --- Train Loss: 0.1952195993242167 --- Val Loss: 0.18501823330578565\n",
      "Epoch 130/3000 --- Train Loss: 0.17538744194759157 --- Val Loss: 0.16642264395208492\n",
      "Epoch 140/3000 --- Train Loss: 0.15971486956140088 --- Val Loss: 0.15176505232337006\n",
      "Epoch 150/3000 --- Train Loss: 0.14708315093556681 --- Val Loss: 0.14000929198740059\n",
      "Epoch 160/3000 --- Train Loss: 0.13659867482786633 --- Val Loss: 0.13031238547381307\n",
      "Epoch 170/3000 --- Train Loss: 0.12768474408241373 --- Val Loss: 0.12220810862005872\n",
      "Epoch 180/3000 --- Train Loss: 0.11992981296580382 --- Val Loss: 0.11530657699530479\n",
      "Epoch 190/3000 --- Train Loss: 0.11316910940807937 --- Val Loss: 0.10936801429370112\n",
      "Epoch 200/3000 --- Train Loss: 0.10723916933017405 --- Val Loss: 0.10420542595085877\n",
      "Epoch 210/3000 --- Train Loss: 0.10202639901236915 --- Val Loss: 0.09968610949987744\n",
      "Epoch 220/3000 --- Train Loss: 0.09739754090939119 --- Val Loss: 0.09569677705337899\n",
      "Epoch 230/3000 --- Train Loss: 0.09325602731525169 --- Val Loss: 0.09214517731091307\n",
      "Epoch 240/3000 --- Train Loss: 0.08954187802209962 --- Val Loss: 0.08898737484461428\n",
      "Epoch 250/3000 --- Train Loss: 0.08617979692548484 --- Val Loss: 0.08611465150450388\n",
      "Epoch 260/3000 --- Train Loss: 0.08312506226630084 --- Val Loss: 0.08349762838558515\n",
      "Epoch 270/3000 --- Train Loss: 0.08031414432635624 --- Val Loss: 0.08106787649467033\n",
      "Epoch 280/3000 --- Train Loss: 0.0777421613857692 --- Val Loss: 0.07882348141985715\n",
      "Epoch 290/3000 --- Train Loss: 0.07536989247868439 --- Val Loss: 0.07674111369816405\n",
      "Epoch 300/3000 --- Train Loss: 0.0731735209233804 --- Val Loss: 0.0748076521107217\n",
      "Epoch 310/3000 --- Train Loss: 0.07113439396309829 --- Val Loss: 0.07298241430085234\n",
      "Epoch 320/3000 --- Train Loss: 0.06922826827938573 --- Val Loss: 0.07127404704153197\n",
      "Epoch 330/3000 --- Train Loss: 0.06744991192751947 --- Val Loss: 0.0696729912964653\n",
      "Epoch 340/3000 --- Train Loss: 0.06578254995624919 --- Val Loss: 0.06815030583497889\n",
      "Epoch 350/3000 --- Train Loss: 0.06421093114876347 --- Val Loss: 0.06671696460079242\n",
      "Epoch 360/3000 --- Train Loss: 0.06273831204223868 --- Val Loss: 0.06535205262030974\n",
      "Epoch 370/3000 --- Train Loss: 0.061342344148756625 --- Val Loss: 0.06406336903809264\n",
      "Epoch 380/3000 --- Train Loss: 0.06002730714552629 --- Val Loss: 0.06280936937911645\n",
      "Epoch 390/3000 --- Train Loss: 0.058770995773746934 --- Val Loss: 0.061628953962956354\n",
      "Epoch 400/3000 --- Train Loss: 0.057576142893311096 --- Val Loss: 0.06050690603708261\n",
      "Epoch 410/3000 --- Train Loss: 0.05644786212644879 --- Val Loss: 0.05944362733044763\n",
      "Epoch 420/3000 --- Train Loss: 0.055383104656503986 --- Val Loss: 0.05840805441327506\n",
      "Epoch 430/3000 --- Train Loss: 0.054355370128859874 --- Val Loss: 0.05743522869941434\n",
      "Epoch 440/3000 --- Train Loss: 0.05337710703905197 --- Val Loss: 0.056497985681927335\n",
      "Epoch 450/3000 --- Train Loss: 0.052447808961260875 --- Val Loss: 0.05558683319772384\n",
      "Epoch 460/3000 --- Train Loss: 0.051552310394958334 --- Val Loss: 0.05472331216667298\n",
      "Epoch 470/3000 --- Train Loss: 0.05070070348217946 --- Val Loss: 0.05387490991279039\n",
      "Epoch 480/3000 --- Train Loss: 0.04987652731297692 --- Val Loss: 0.053072245946773504\n",
      "Epoch 490/3000 --- Train Loss: 0.04908573919608534 --- Val Loss: 0.05227160554825467\n",
      "Epoch 500/3000 --- Train Loss: 0.04833212867861969 --- Val Loss: 0.05151570628338769\n",
      "Epoch 510/3000 --- Train Loss: 0.04760140767304514 --- Val Loss: 0.05078256349308474\n",
      "Epoch 520/3000 --- Train Loss: 0.046906111787207015 --- Val Loss: 0.050069713647595465\n",
      "Epoch 530/3000 --- Train Loss: 0.0462247324093134 --- Val Loss: 0.04938599179655182\n",
      "Epoch 540/3000 --- Train Loss: 0.04557594724529636 --- Val Loss: 0.04870450038101012\n",
      "Epoch 550/3000 --- Train Loss: 0.04494917865828802 --- Val Loss: 0.04804593469525517\n",
      "Epoch 560/3000 --- Train Loss: 0.04434177403033582 --- Val Loss: 0.04739852614811002\n",
      "Epoch 570/3000 --- Train Loss: 0.043749785210827286 --- Val Loss: 0.04678767231042987\n",
      "Epoch 580/3000 --- Train Loss: 0.04318171747518606 --- Val Loss: 0.046190083007873924\n",
      "Epoch 590/3000 --- Train Loss: 0.04263237425117333 --- Val Loss: 0.04560487296792156\n",
      "Epoch 600/3000 --- Train Loss: 0.04209540576215671 --- Val Loss: 0.04504259482328066\n",
      "Epoch 610/3000 --- Train Loss: 0.04157644075425768 --- Val Loss: 0.044487822228128866\n",
      "Epoch 620/3000 --- Train Loss: 0.0410708832507396 --- Val Loss: 0.043952834085103455\n",
      "Epoch 630/3000 --- Train Loss: 0.04058641974946994 --- Val Loss: 0.043423531537675045\n",
      "Epoch 640/3000 --- Train Loss: 0.04011432922090592 --- Val Loss: 0.04291064330349285\n",
      "Epoch 650/3000 --- Train Loss: 0.03965464252919239 --- Val Loss: 0.042401493685296276\n",
      "Epoch 660/3000 --- Train Loss: 0.039202041236531836 --- Val Loss: 0.04192208042454965\n",
      "Epoch 670/3000 --- Train Loss: 0.03876858658867907 --- Val Loss: 0.04144568717564865\n",
      "Epoch 680/3000 --- Train Loss: 0.03834687521767259 --- Val Loss: 0.04097277484968753\n",
      "Epoch 690/3000 --- Train Loss: 0.03793645981176988 --- Val Loss: 0.04049203650728207\n",
      "Epoch 700/3000 --- Train Loss: 0.037533990911064484 --- Val Loss: 0.040045573069992124\n",
      "Epoch 710/3000 --- Train Loss: 0.0371437098289661 --- Val Loss: 0.03960084637276828\n",
      "Epoch 720/3000 --- Train Loss: 0.03676460221864937 --- Val Loss: 0.0391653168025292\n",
      "Epoch 730/3000 --- Train Loss: 0.03639590279707203 --- Val Loss: 0.038729194179483485\n",
      "Epoch 740/3000 --- Train Loss: 0.03603663872439013 --- Val Loss: 0.038307699852730644\n",
      "Epoch 750/3000 --- Train Loss: 0.03567605288789537 --- Val Loss: 0.03791991822562647\n",
      "Epoch 760/3000 --- Train Loss: 0.03533198496441122 --- Val Loss: 0.037523106870049325\n",
      "Epoch 770/3000 --- Train Loss: 0.03499338071403259 --- Val Loss: 0.037135917217393545\n",
      "Epoch 780/3000 --- Train Loss: 0.03466208103994429 --- Val Loss: 0.03675611054927435\n",
      "Epoch 790/3000 --- Train Loss: 0.034340730082941515 --- Val Loss: 0.03639246790806802\n",
      "Epoch 800/3000 --- Train Loss: 0.03401940653156932 --- Val Loss: 0.03604287425550461\n",
      "Epoch 810/3000 --- Train Loss: 0.03371493202228489 --- Val Loss: 0.035684981017502185\n",
      "Epoch 820/3000 --- Train Loss: 0.03341105772670558 --- Val Loss: 0.03534326257180562\n",
      "Epoch 830/3000 --- Train Loss: 0.033118566175212 --- Val Loss: 0.03499605637173901\n",
      "Epoch 840/3000 --- Train Loss: 0.03282867702358532 --- Val Loss: 0.03465945543933873\n",
      "Epoch 850/3000 --- Train Loss: 0.03254454749784096 --- Val Loss: 0.0343311838035924\n",
      "Epoch 860/3000 --- Train Loss: 0.0322657110540207 --- Val Loss: 0.03400888540296914\n",
      "Epoch 870/3000 --- Train Loss: 0.03199331828780079 --- Val Loss: 0.03369587323663267\n",
      "Epoch 880/3000 --- Train Loss: 0.03172646309595789 --- Val Loss: 0.033388923127030894\n",
      "Epoch 890/3000 --- Train Loss: 0.03146509471279352 --- Val Loss: 0.03308084111479285\n",
      "Epoch 900/3000 --- Train Loss: 0.031207762631359246 --- Val Loss: 0.032780883030059556\n",
      "Epoch 910/3000 --- Train Loss: 0.03095600946494595 --- Val Loss: 0.03248725613063762\n",
      "Epoch 920/3000 --- Train Loss: 0.030708535173940336 --- Val Loss: 0.032197959064253386\n",
      "Epoch 930/3000 --- Train Loss: 0.030467561111374657 --- Val Loss: 0.03191823884187816\n",
      "Epoch 940/3000 --- Train Loss: 0.030228969913181407 --- Val Loss: 0.031643545579705785\n",
      "Epoch 950/3000 --- Train Loss: 0.029992182360979287 --- Val Loss: 0.031377951911150184\n",
      "Epoch 960/3000 --- Train Loss: 0.029763536094862684 --- Val Loss: 0.03110965274109811\n",
      "Epoch 970/3000 --- Train Loss: 0.029541378984786128 --- Val Loss: 0.030831725062163848\n",
      "Epoch 980/3000 --- Train Loss: 0.02931861778149814 --- Val Loss: 0.030581628380329094\n",
      "Epoch 990/3000 --- Train Loss: 0.029099189280237484 --- Val Loss: 0.03034013600995138\n",
      "Epoch 1000/3000 --- Train Loss: 0.0288874498458894 --- Val Loss: 0.030077943692451543\n",
      "Epoch 1010/3000 --- Train Loss: 0.028675014831753203 --- Val Loss: 0.029838139828868477\n",
      "Epoch 1020/3000 --- Train Loss: 0.028465101471600147 --- Val Loss: 0.02960533962161975\n",
      "Epoch 1030/3000 --- Train Loss: 0.028262675420390817 --- Val Loss: 0.02937513430988234\n",
      "Epoch 1040/3000 --- Train Loss: 0.02806227590063654 --- Val Loss: 0.029133450235415827\n",
      "Epoch 1050/3000 --- Train Loss: 0.02786489585628106 --- Val Loss: 0.02891483427955035\n",
      "Epoch 1060/3000 --- Train Loss: 0.02767134427741587 --- Val Loss: 0.02867553504539395\n",
      "Epoch 1070/3000 --- Train Loss: 0.027477231509608915 --- Val Loss: 0.028461411067936516\n",
      "Epoch 1080/3000 --- Train Loss: 0.027290400282542697 --- Val Loss: 0.028241145998034484\n",
      "Epoch 1090/3000 --- Train Loss: 0.027103605875627802 --- Val Loss: 0.02802724661522622\n",
      "Epoch 1100/3000 --- Train Loss: 0.026922715632490744 --- Val Loss: 0.027817124311259676\n",
      "Epoch 1110/3000 --- Train Loss: 0.026742271629215653 --- Val Loss: 0.027609535819472758\n",
      "Epoch 1120/3000 --- Train Loss: 0.02656590310089161 --- Val Loss: 0.02739578620953496\n",
      "Epoch 1130/3000 --- Train Loss: 0.026391342004115997 --- Val Loss: 0.02719493673493227\n",
      "Epoch 1140/3000 --- Train Loss: 0.026215477311407148 --- Val Loss: 0.027012171911367858\n",
      "Epoch 1150/3000 --- Train Loss: 0.026047642091239106 --- Val Loss: 0.026807104962466705\n",
      "Epoch 1160/3000 --- Train Loss: 0.02587770695389994 --- Val Loss: 0.02661682201046113\n",
      "Epoch 1170/3000 --- Train Loss: 0.025712476200413805 --- Val Loss: 0.026427732566948137\n",
      "Epoch 1180/3000 --- Train Loss: 0.025549987499693803 --- Val Loss: 0.026236755311297717\n",
      "Epoch 1190/3000 --- Train Loss: 0.025388667359765207 --- Val Loss: 0.026058358705592564\n",
      "Epoch 1200/3000 --- Train Loss: 0.02523200800754428 --- Val Loss: 0.025871148812670643\n",
      "Epoch 1210/3000 --- Train Loss: 0.02507445298824829 --- Val Loss: 0.02569818212578889\n",
      "Epoch 1220/3000 --- Train Loss: 0.02491969349988787 --- Val Loss: 0.02552833595611136\n",
      "Epoch 1230/3000 --- Train Loss: 0.02476789217666973 --- Val Loss: 0.025348889482186207\n",
      "Epoch 1240/3000 --- Train Loss: 0.024617140131599147 --- Val Loss: 0.02517482622938768\n",
      "Epoch 1250/3000 --- Train Loss: 0.024467108649266444 --- Val Loss: 0.025011747205954765\n",
      "Epoch 1260/3000 --- Train Loss: 0.024320404789640957 --- Val Loss: 0.02484208227936748\n",
      "Epoch 1270/3000 --- Train Loss: 0.024176364113596697 --- Val Loss: 0.024678291165858526\n",
      "Epoch 1280/3000 --- Train Loss: 0.024032868791713076 --- Val Loss: 0.02451720492460185\n",
      "Epoch 1290/3000 --- Train Loss: 0.023892917434889718 --- Val Loss: 0.024355943096752073\n",
      "Epoch 1300/3000 --- Train Loss: 0.023752881286519218 --- Val Loss: 0.024205090136570476\n",
      "Epoch 1310/3000 --- Train Loss: 0.023613851767952648 --- Val Loss: 0.024050805295139616\n",
      "Epoch 1320/3000 --- Train Loss: 0.02347796298088715 --- Val Loss: 0.023901802505773076\n",
      "Epoch 1330/3000 --- Train Loss: 0.02334453084399064 --- Val Loss: 0.02374629546472053\n",
      "Epoch 1340/3000 --- Train Loss: 0.023210851196507606 --- Val Loss: 0.02359695785095269\n",
      "Epoch 1350/3000 --- Train Loss: 0.02308255778668473 --- Val Loss: 0.02344069123161823\n",
      "Epoch 1360/3000 --- Train Loss: 0.022951182296692577 --- Val Loss: 0.023300969595946085\n",
      "Epoch 1370/3000 --- Train Loss: 0.022822230455488916 --- Val Loss: 0.023154709780591965\n",
      "Epoch 1380/3000 --- Train Loss: 0.02269399461295095 --- Val Loss: 0.023013796314970338\n",
      "Epoch 1390/3000 --- Train Loss: 0.02256866224447448 --- Val Loss: 0.022878699869616725\n",
      "Epoch 1400/3000 --- Train Loss: 0.022442274696860134 --- Val Loss: 0.0227439693978248\n",
      "Epoch 1410/3000 --- Train Loss: 0.02232117046128977 --- Val Loss: 0.022603217173371857\n",
      "Epoch 1420/3000 --- Train Loss: 0.02219788839951308 --- Val Loss: 0.022469852398665745\n",
      "Epoch 1430/3000 --- Train Loss: 0.022076050003226214 --- Val Loss: 0.022339385038095225\n",
      "Epoch 1440/3000 --- Train Loss: 0.021956299417565637 --- Val Loss: 0.02220549313629819\n",
      "Epoch 1450/3000 --- Train Loss: 0.02183794889994556 --- Val Loss: 0.022076897416636716\n",
      "Epoch 1460/3000 --- Train Loss: 0.021720847613947367 --- Val Loss: 0.021950792083278867\n",
      "Epoch 1470/3000 --- Train Loss: 0.02160470781161877 --- Val Loss: 0.021824324106978724\n",
      "Epoch 1480/3000 --- Train Loss: 0.021490605040628524 --- Val Loss: 0.02169841969323262\n",
      "Epoch 1490/3000 --- Train Loss: 0.021377648696692744 --- Val Loss: 0.021574663368114654\n",
      "Epoch 1500/3000 --- Train Loss: 0.021264916083925254 --- Val Loss: 0.021454204924893208\n",
      "Epoch 1510/3000 --- Train Loss: 0.02115283903866759 --- Val Loss: 0.021336615931511392\n",
      "Epoch 1520/3000 --- Train Loss: 0.021042524879061088 --- Val Loss: 0.021216475680902815\n",
      "Epoch 1530/3000 --- Train Loss: 0.020934502972090853 --- Val Loss: 0.021095327195269256\n",
      "Epoch 1540/3000 --- Train Loss: 0.020826356349703628 --- Val Loss: 0.02098834560793413\n",
      "Epoch 1550/3000 --- Train Loss: 0.020720956441651107 --- Val Loss: 0.02086404914778276\n",
      "Epoch 1560/3000 --- Train Loss: 0.020613045483801076 --- Val Loss: 0.02076443061453701\n",
      "Epoch 1570/3000 --- Train Loss: 0.02050849800317659 --- Val Loss: 0.02064986016155937\n",
      "Epoch 1580/3000 --- Train Loss: 0.020404876827044165 --- Val Loss: 0.020538609169418652\n",
      "Epoch 1590/3000 --- Train Loss: 0.020301627768696043 --- Val Loss: 0.02042045750359442\n",
      "Epoch 1600/3000 --- Train Loss: 0.02019918254092408 --- Val Loss: 0.02031763007020235\n",
      "Epoch 1610/3000 --- Train Loss: 0.020098230192626115 --- Val Loss: 0.02021035727731996\n",
      "Epoch 1620/3000 --- Train Loss: 0.019998537448777536 --- Val Loss: 0.02010360715899422\n",
      "Epoch 1630/3000 --- Train Loss: 0.019898677916735323 --- Val Loss: 0.019998566318971987\n",
      "Epoch 1640/3000 --- Train Loss: 0.01980103307013279 --- Val Loss: 0.0198862199573765\n",
      "Epoch 1650/3000 --- Train Loss: 0.019703503557156225 --- Val Loss: 0.01978299407893476\n",
      "Epoch 1660/3000 --- Train Loss: 0.019607450127122335 --- Val Loss: 0.01967842406517393\n",
      "Epoch 1670/3000 --- Train Loss: 0.019509851440508163 --- Val Loss: 0.019577199815978868\n",
      "Epoch 1680/3000 --- Train Loss: 0.01941388384286234 --- Val Loss: 0.019479832494819178\n",
      "Epoch 1690/3000 --- Train Loss: 0.01931860199874127 --- Val Loss: 0.019380898493606774\n",
      "Epoch 1700/3000 --- Train Loss: 0.01922502066708629 --- Val Loss: 0.019280884944336208\n",
      "Epoch 1710/3000 --- Train Loss: 0.019131647002154202 --- Val Loss: 0.019183726822003446\n",
      "Epoch 1720/3000 --- Train Loss: 0.019039508955426207 --- Val Loss: 0.019085537491867175\n",
      "Epoch 1730/3000 --- Train Loss: 0.018947583109027374 --- Val Loss: 0.018995997434586222\n",
      "Epoch 1740/3000 --- Train Loss: 0.018856401796747607 --- Val Loss: 0.01890160227013538\n",
      "Epoch 1750/3000 --- Train Loss: 0.018765647224153265 --- Val Loss: 0.018813491845083175\n",
      "Epoch 1760/3000 --- Train Loss: 0.01867793183631695 --- Val Loss: 0.018710431847952985\n",
      "Epoch 1770/3000 --- Train Loss: 0.018588736046590126 --- Val Loss: 0.018622481406850116\n",
      "Epoch 1780/3000 --- Train Loss: 0.018500929586132118 --- Val Loss: 0.018528009545458825\n",
      "Epoch 1790/3000 --- Train Loss: 0.018414542598309155 --- Val Loss: 0.018438083850695116\n",
      "Epoch 1800/3000 --- Train Loss: 0.018325905525957482 --- Val Loss: 0.01835339339059392\n",
      "Epoch 1810/3000 --- Train Loss: 0.018241845270166927 --- Val Loss: 0.018261775244784305\n",
      "Epoch 1820/3000 --- Train Loss: 0.018155840243092507 --- Val Loss: 0.018172792898508233\n",
      "Epoch 1830/3000 --- Train Loss: 0.01806974450977505 --- Val Loss: 0.01808746718076526\n",
      "Epoch 1840/3000 --- Train Loss: 0.017984985985106252 --- Val Loss: 0.017996802674665538\n",
      "Epoch 1850/3000 --- Train Loss: 0.01790031652267053 --- Val Loss: 0.01791482569654441\n",
      "Epoch 1860/3000 --- Train Loss: 0.017816759625643334 --- Val Loss: 0.01782625308278745\n",
      "Epoch 1870/3000 --- Train Loss: 0.017733097976738003 --- Val Loss: 0.017739193733482712\n",
      "Epoch 1880/3000 --- Train Loss: 0.01764998073762246 --- Val Loss: 0.017656500492941835\n",
      "Epoch 1890/3000 --- Train Loss: 0.01756806399606769 --- Val Loss: 0.017572766411984637\n",
      "Epoch 1900/3000 --- Train Loss: 0.01748761319273126 --- Val Loss: 0.017490836903370284\n",
      "Epoch 1910/3000 --- Train Loss: 0.017406083234499988 --- Val Loss: 0.017408991499571338\n",
      "Epoch 1920/3000 --- Train Loss: 0.01732583097758086 --- Val Loss: 0.017326202932820438\n",
      "Epoch 1930/3000 --- Train Loss: 0.01724670566689246 --- Val Loss: 0.01724414273682884\n",
      "Epoch 1940/3000 --- Train Loss: 0.017168599973040523 --- Val Loss: 0.017163735622040918\n",
      "Epoch 1950/3000 --- Train Loss: 0.017090089839339333 --- Val Loss: 0.01708107843942132\n",
      "Epoch 1960/3000 --- Train Loss: 0.017011424116426317 --- Val Loss: 0.017003610258967447\n",
      "Epoch 1970/3000 --- Train Loss: 0.0169335398608487 --- Val Loss: 0.016926506842685034\n",
      "Epoch 1980/3000 --- Train Loss: 0.016857440440362068 --- Val Loss: 0.016845385101428353\n",
      "Epoch 1990/3000 --- Train Loss: 0.016781290857064805 --- Val Loss: 0.016764866987987063\n",
      "Epoch 2000/3000 --- Train Loss: 0.016704869474016396 --- Val Loss: 0.016687825061165347\n",
      "Epoch 2010/3000 --- Train Loss: 0.016628727946997508 --- Val Loss: 0.016610646206503314\n",
      "Epoch 2020/3000 --- Train Loss: 0.016553714353814468 --- Val Loss: 0.016533743286994314\n",
      "Epoch 2030/3000 --- Train Loss: 0.016480327506276753 --- Val Loss: 0.016455535920054134\n",
      "Epoch 2040/3000 --- Train Loss: 0.016406965536544586 --- Val Loss: 0.016383137904328264\n",
      "Epoch 2050/3000 --- Train Loss: 0.016332234375900926 --- Val Loss: 0.016313809960488913\n",
      "Epoch 2060/3000 --- Train Loss: 0.01625854591515752 --- Val Loss: 0.016239592536339664\n",
      "Epoch 2070/3000 --- Train Loss: 0.01618716216888493 --- Val Loss: 0.016165904262307365\n",
      "Epoch 2080/3000 --- Train Loss: 0.01611489675289625 --- Val Loss: 0.016095107466802643\n",
      "Epoch 2090/3000 --- Train Loss: 0.016042310358181575 --- Val Loss: 0.016022274407883518\n",
      "Epoch 2100/3000 --- Train Loss: 0.01597190500199273 --- Val Loss: 0.01594749197719376\n",
      "Epoch 2110/3000 --- Train Loss: 0.015900550788695452 --- Val Loss: 0.015876893841404262\n",
      "Epoch 2120/3000 --- Train Loss: 0.015828988410707897 --- Val Loss: 0.015803961802130416\n",
      "Epoch 2130/3000 --- Train Loss: 0.0157583703998369 --- Val Loss: 0.01573692179642914\n",
      "Epoch 2140/3000 --- Train Loss: 0.01568912975710994 --- Val Loss: 0.015666910044559976\n",
      "Epoch 2150/3000 --- Train Loss: 0.015620362870261306 --- Val Loss: 0.015599497664433744\n",
      "Epoch 2160/3000 --- Train Loss: 0.015552434135003583 --- Val Loss: 0.015533715609043952\n",
      "Epoch 2170/3000 --- Train Loss: 0.01548436449383255 --- Val Loss: 0.015464619841917372\n",
      "Epoch 2180/3000 --- Train Loss: 0.015416139878346774 --- Val Loss: 0.015395762879023712\n",
      "Epoch 2190/3000 --- Train Loss: 0.015349475619325412 --- Val Loss: 0.015330312600261374\n",
      "Epoch 2200/3000 --- Train Loss: 0.01528195093875594 --- Val Loss: 0.015262588228582384\n",
      "Epoch 2210/3000 --- Train Loss: 0.015214279157860557 --- Val Loss: 0.01519882634526804\n",
      "Epoch 2220/3000 --- Train Loss: 0.015149165999242249 --- Val Loss: 0.015134365064321031\n",
      "Epoch 2230/3000 --- Train Loss: 0.015084007596144009 --- Val Loss: 0.015074071429971199\n",
      "Epoch 2240/3000 --- Train Loss: 0.015017791020314029 --- Val Loss: 0.015013530331102472\n",
      "Epoch 2250/3000 --- Train Loss: 0.014954072934309707 --- Val Loss: 0.01494925652699565\n",
      "Epoch 2260/3000 --- Train Loss: 0.014889243362556193 --- Val Loss: 0.014883122198654012\n",
      "Epoch 2270/3000 --- Train Loss: 0.01482619578089791 --- Val Loss: 0.014822916293861337\n",
      "Epoch 2280/3000 --- Train Loss: 0.014762706442510067 --- Val Loss: 0.014762062853565615\n",
      "Epoch 2290/3000 --- Train Loss: 0.014700123434564819 --- Val Loss: 0.01470223556406217\n",
      "Epoch 2300/3000 --- Train Loss: 0.014637602023055725 --- Val Loss: 0.01463918169372933\n",
      "Epoch 2310/3000 --- Train Loss: 0.014574794927889989 --- Val Loss: 0.014582070818300203\n",
      "Epoch 2320/3000 --- Train Loss: 0.014513808324036339 --- Val Loss: 0.014522700555584534\n",
      "Epoch 2330/3000 --- Train Loss: 0.014452323035826544 --- Val Loss: 0.014466041047182158\n",
      "Epoch 2340/3000 --- Train Loss: 0.01439172126175063 --- Val Loss: 0.014407840425380361\n",
      "Epoch 2350/3000 --- Train Loss: 0.014330596305491222 --- Val Loss: 0.014348983975711669\n",
      "Epoch 2360/3000 --- Train Loss: 0.01426970801896126 --- Val Loss: 0.014293113688959084\n",
      "Epoch 2370/3000 --- Train Loss: 0.014210190502495732 --- Val Loss: 0.014238898943135826\n",
      "Epoch 2380/3000 --- Train Loss: 0.014151448834784641 --- Val Loss: 0.014181832582247929\n",
      "Epoch 2390/3000 --- Train Loss: 0.014092040777970367 --- Val Loss: 0.014126033546129615\n",
      "Epoch 2400/3000 --- Train Loss: 0.01403300592590314 --- Val Loss: 0.014070039527881733\n",
      "Epoch 2410/3000 --- Train Loss: 0.013974690319803487 --- Val Loss: 0.01401652125123618\n",
      "Epoch 2420/3000 --- Train Loss: 0.013916846419550044 --- Val Loss: 0.013960913111303843\n",
      "Epoch 2430/3000 --- Train Loss: 0.013859506300274415 --- Val Loss: 0.013906701256999865\n",
      "Epoch 2440/3000 --- Train Loss: 0.01380270851237581 --- Val Loss: 0.01385292578268651\n",
      "Epoch 2450/3000 --- Train Loss: 0.013746456810589175 --- Val Loss: 0.01379922241500746\n",
      "Epoch 2460/3000 --- Train Loss: 0.013690719028553556 --- Val Loss: 0.013744819460815507\n",
      "Epoch 2470/3000 --- Train Loss: 0.013634112143664364 --- Val Loss: 0.01369194939546446\n",
      "Epoch 2480/3000 --- Train Loss: 0.01357935735611535 --- Val Loss: 0.013638941575307019\n",
      "Epoch 2490/3000 --- Train Loss: 0.013523872402489154 --- Val Loss: 0.01358829310459079\n",
      "Epoch 2500/3000 --- Train Loss: 0.013469878470072387 --- Val Loss: 0.013537286118501992\n",
      "Epoch 2510/3000 --- Train Loss: 0.01341615559050185 --- Val Loss: 0.013488331079213088\n",
      "Epoch 2520/3000 --- Train Loss: 0.013361909473441581 --- Val Loss: 0.013437213386678195\n",
      "Epoch 2530/3000 --- Train Loss: 0.013308454257875389 --- Val Loss: 0.013389036730919271\n",
      "Epoch 2540/3000 --- Train Loss: 0.013256746812486548 --- Val Loss: 0.013340380945641337\n",
      "Epoch 2550/3000 --- Train Loss: 0.013203227079081966 --- Val Loss: 0.013289226331605569\n",
      "Epoch 2560/3000 --- Train Loss: 0.013150945877739396 --- Val Loss: 0.013241104419010127\n",
      "Epoch 2570/3000 --- Train Loss: 0.01309847120863409 --- Val Loss: 0.013190163032704637\n",
      "Epoch 2580/3000 --- Train Loss: 0.013047167663926306 --- Val Loss: 0.013143303418692149\n",
      "Epoch 2590/3000 --- Train Loss: 0.01299623288244521 --- Val Loss: 0.013096265205329652\n",
      "Epoch 2600/3000 --- Train Loss: 0.01294458333202546 --- Val Loss: 0.013048208413395733\n",
      "Epoch 2610/3000 --- Train Loss: 0.012894814540800902 --- Val Loss: 0.01300203117406129\n",
      "Epoch 2620/3000 --- Train Loss: 0.01284408641823878 --- Val Loss: 0.012954387586582017\n",
      "Epoch 2630/3000 --- Train Loss: 0.01279464280317187 --- Val Loss: 0.012903714459689755\n",
      "Epoch 2640/3000 --- Train Loss: 0.012745626942114886 --- Val Loss: 0.012859419242309945\n",
      "Epoch 2650/3000 --- Train Loss: 0.012697370588256743 --- Val Loss: 0.012816472827464849\n",
      "Epoch 2660/3000 --- Train Loss: 0.012648710173740056 --- Val Loss: 0.012768723641318924\n",
      "Epoch 2670/3000 --- Train Loss: 0.012600989194764677 --- Val Loss: 0.012725657053639157\n",
      "Epoch 2680/3000 --- Train Loss: 0.012553605142746364 --- Val Loss: 0.012679646089249158\n",
      "Epoch 2690/3000 --- Train Loss: 0.012505235642056113 --- Val Loss: 0.012635212148687068\n",
      "Epoch 2700/3000 --- Train Loss: 0.012457483089570629 --- Val Loss: 0.012590646287920674\n",
      "Epoch 2710/3000 --- Train Loss: 0.012411079894480448 --- Val Loss: 0.012545041992458782\n",
      "Epoch 2720/3000 --- Train Loss: 0.01236404705800475 --- Val Loss: 0.012501614115307617\n",
      "Epoch 2730/3000 --- Train Loss: 0.012317715954238622 --- Val Loss: 0.012455862122548457\n",
      "Epoch 2740/3000 --- Train Loss: 0.012271317945915607 --- Val Loss: 0.01241678129134673\n",
      "Epoch 2750/3000 --- Train Loss: 0.012225589800574102 --- Val Loss: 0.012371433163009316\n",
      "Epoch 2760/3000 --- Train Loss: 0.012181064321438761 --- Val Loss: 0.012329660103388521\n",
      "Epoch 2770/3000 --- Train Loss: 0.012135743851487649 --- Val Loss: 0.012288511180170702\n",
      "Epoch 2780/3000 --- Train Loss: 0.01209123065302459 --- Val Loss: 0.01224565655053327\n",
      "Epoch 2790/3000 --- Train Loss: 0.012047316209004292 --- Val Loss: 0.01220313649934102\n",
      "Epoch 2800/3000 --- Train Loss: 0.012003330251756131 --- Val Loss: 0.012160533724715285\n",
      "Epoch 2810/3000 --- Train Loss: 0.011960587163538003 --- Val Loss: 0.01212419660784778\n",
      "Epoch 2820/3000 --- Train Loss: 0.011917678447492015 --- Val Loss: 0.012079440270971683\n",
      "Epoch 2830/3000 --- Train Loss: 0.011874882936878557 --- Val Loss: 0.012038430995768913\n",
      "Epoch 2840/3000 --- Train Loss: 0.011832897417579029 --- Val Loss: 0.011997345910941303\n",
      "Epoch 2850/3000 --- Train Loss: 0.011791309281940101 --- Val Loss: 0.011957719954576788\n",
      "Epoch 2860/3000 --- Train Loss: 0.011749213772445885 --- Val Loss: 0.011915982914875745\n",
      "Epoch 2870/3000 --- Train Loss: 0.01170774786474109 --- Val Loss: 0.011880533347637195\n",
      "Epoch 2880/3000 --- Train Loss: 0.011666909126110264 --- Val Loss: 0.011837541319263421\n",
      "Epoch 2890/3000 --- Train Loss: 0.011626216263821846 --- Val Loss: 0.01179836093303058\n",
      "Epoch 2900/3000 --- Train Loss: 0.011585486275006853 --- Val Loss: 0.011761546926924278\n",
      "Epoch 2910/3000 --- Train Loss: 0.011545030387442418 --- Val Loss: 0.011721327421326595\n",
      "Epoch 2920/3000 --- Train Loss: 0.01150596270210842 --- Val Loss: 0.011683662303339658\n",
      "Epoch 2930/3000 --- Train Loss: 0.01146613390225879 --- Val Loss: 0.011645202333506504\n",
      "Epoch 2940/3000 --- Train Loss: 0.01142708499446035 --- Val Loss: 0.01160627283887534\n",
      "Epoch 2950/3000 --- Train Loss: 0.011388201810269246 --- Val Loss: 0.0115685020072963\n",
      "Epoch 2960/3000 --- Train Loss: 0.011349873014107437 --- Val Loss: 0.011530994446083461\n",
      "Epoch 2970/3000 --- Train Loss: 0.011312031167531265 --- Val Loss: 0.011496113571012377\n",
      "Epoch 2980/3000 --- Train Loss: 0.011274000955366237 --- Val Loss: 0.01145596076454233\n",
      "Epoch 2990/3000 --- Train Loss: 0.011236286117456269 --- Val Loss: 0.011418156595886924\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9912280701754386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Standardize the features\n",
    "X = standardize_data(X)\n",
    "\n",
    "# One-hot encode the labels\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "y = one_hot_encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the neural network model\n",
    "network = NeuralNetwork()\n",
    "network.add_layer(Layer(X_train.shape[1], 128))  # 64 inputs (8x8 images)\n",
    "network.add_layer(ReLU())\n",
    "network.add_layer(Layer(128, 2))  # 1 classes\n",
    "network.add_layer(Sigmoid())\n",
    "\n",
    "# Train the network\n",
    "network.train(X_train, y_train, epochs=3000, learning_rate=0.01, batch_size=16)\n",
    "\n",
    "network.plot_loss()\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "y_pred = network.predict(X_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f\"\\nAccuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "* L1 and L2 regularizers\n",
    "\n",
    "* Gradient with momentum\n",
    "\n",
    "* optimization of hyperparameters (random search and grid search function?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
